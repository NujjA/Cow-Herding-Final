{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output produced from agent teams running\n",
    "\n",
    "## Feel free to change the number of episodes and steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CHModel\n",
    "import csv, datetime\n",
    "\n",
    "# Change these values to see what happens\n",
    "# number of episodes\n",
    "episodes = 10\n",
    "# number of steps per episode\n",
    "steps = 100\n",
    "\n",
    "# number of agents - Q table was trained using teams of 2 agents. Changing these values may not produce great results.\n",
    "random_agents = 2\n",
    "cow_agents = 4\n",
    "plan_agents = 2\n",
    "trained_mc_agents = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating random agent\n",
      "creating random agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "Random Episode  0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 1  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 2  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 3  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "0.0 0 4  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 5  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 6  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 7  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 8  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 9  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 10  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 11  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 12  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 13  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 14  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 15  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 16  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 17  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 18  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 19  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 20  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 21  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 22  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 23  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 24  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 25  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 26  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 27  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 28  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 29  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 30  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 31  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 32  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 33  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "2.0 2 34  Episode:  0\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "random agent step\n",
      "random agent step\n",
      "4.0 2 35  Episode:  0\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "6.0 2 36  Episode:  0\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "8.0 2 37  Episode:  0\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "random agent step\n",
      "11.0 3 38  Episode:  0\n",
      "New cow in the goal: 3\n",
      "cows in goal:  3 , previous_cow_count:  2  reward:  50\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "15.0 4 39  Episode:  0\n",
      "New cow in the goal: 4\n",
      "cows in goal:  4 , previous_cow_count:  3  reward:  50\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "19.0 4 40  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "23.0 4 41  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "27.0 4 42  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "31.0 4 43  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "35.0 4 44  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "39.0 4 45  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "43.0 4 46  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "47.0 4 47  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "51.0 4 48  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "55.0 4 49  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "59.0 4 50  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "63.0 4 51  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "67.0 4 52  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "71.0 4 53  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "75.0 4 54  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "79.0 4 55  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "83.0 4 56  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "87.0 4 57  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "91.0 4 58  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "95.0 4 59  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "99.0 4 60  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "103.0 4 61  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "107.0 4 62  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "111.0 4 63  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "115.0 4 64  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "119.0 4 65  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "123.0 4 66  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "127.0 4 67  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "131.0 4 68  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "135.0 4 69  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "139.0 4 70  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "143.0 4 71  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random agent step\n",
      "random agent step\n",
      "147.0 4 72  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "151.0 4 73  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "155.0 4 74  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "159.0 4 75  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "163.0 4 76  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "167.0 4 77  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "171.0 4 78  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "175.0 4 79  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "179.0 4 80  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "183.0 4 81  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "187.0 4 82  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "191.0 4 83  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "195.0 4 84  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "199.0 4 85  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "203.0 4 86  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "207.0 4 87  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "211.0 4 88  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "215.0 4 89  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "219.0 4 90  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "223.0 4 91  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "227.0 4 92  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "231.0 4 93  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "235.0 4 94  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "239.0 4 95  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "243.0 4 96  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "247.0 4 97  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "251.0 4 98  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "255.0 4 99  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "259.0 4 100  Episode:  0\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "creating random agent\n",
      "creating random agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "Random Episode  1\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 1  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 2  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 3  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 4  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 5  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 6  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 7  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 8  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 9  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 10  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 11  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 12  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 13  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 14  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 15  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 16  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 17  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 18  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 19  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 20  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 21  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 22  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 23  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 24  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 25  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 26  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 27  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 28  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 29  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 30  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 31  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 32  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 33  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 34  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 35  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 36  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 37  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 38  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 39  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 40  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 41  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 42  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "0.0 0 43  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 44  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 45  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 46  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 47  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 48  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "2.0 2 49  Episode:  1\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "random agent step\n",
      "5.0 3 50  Episode:  1\n",
      "New cow in the goal: 3\n",
      "cows in goal:  3 , previous_cow_count:  2  reward:  50\n",
      "random agent step\n",
      "random agent step\n",
      "8.0 3 51  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "11.0 3 52  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "14.0 3 53  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "17.0 3 54  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "20.0 3 55  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "23.0 3 56  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "26.0 3 57  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "29.0 3 58  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "32.0 3 59  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "35.0 3 60  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "38.0 3 61  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "41.0 3 62  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "44.0 3 63  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "47.0 3 64  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "50.0 3 65  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "53.0 3 66  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "56.0 3 67  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "59.0 3 68  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "62.0 3 69  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "65.0 3 70  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "68.0 3 71  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "71.0 3 72  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "74.0 3 73  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "77.0 3 74  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "80.0 3 75  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "83.0 3 76  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "86.0 3 77  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "89.0 3 78  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "92.0 3 79  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "95.0 3 80  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "98.0 3 81  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "101.0 3 82  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "104.0 3 83  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "107.0 3 84  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "110.0 3 85  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "113.0 3 86  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "116.0 3 87  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "119.0 3 88  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "122.0 3 89  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "125.0 3 90  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "128.0 3 91  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "131.0 3 92  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "134.0 3 93  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "137.0 3 94  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "140.0 3 95  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "143.0 3 96  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "146.0 3 97  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "149.0 3 98  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "152.0 3 99  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "155.0 3 100  Episode:  1\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "creating random agent\n",
      "creating random agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "Random Episode  2\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "2.0 2 1  Episode:  2\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0.0  reward:  50\n",
      "random agent step\n",
      "random agent step\n",
      "4.0 2 2  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "random agent step\n",
      "7.0 3 3  Episode:  2\n",
      "New cow in the goal: 3\n",
      "cows in goal:  3 , previous_cow_count:  2  reward:  50\n",
      "random agent step\n",
      "random agent step\n",
      "10.0 3 4  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "13.0 3 5  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "16.0 3 6  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "19.0 3 7  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "22.0 3 8  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "25.0 3 9  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "28.0 3 10  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "31.0 3 11  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "34.0 3 12  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "37.0 3 13  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "40.0 3 14  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "43.0 3 15  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "46.0 3 16  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "49.0 3 17  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "52.0 3 18  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "55.0 3 19  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "58.0 3 20  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.0 3 21  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "64.0 3 22  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "67.0 3 23  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "70.0 3 24  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "73.0 3 25  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "76.0 3 26  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "79.0 3 27  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "82.0 3 28  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "85.0 3 29  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "88.0 3 30  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "91.0 3 31  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "94.0 3 32  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "97.0 3 33  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "100.0 3 34  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "103.0 3 35  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "106.0 3 36  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "109.0 3 37  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "112.0 3 38  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "115.0 3 39  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "118.0 3 40  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "121.0 3 41  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "124.0 3 42  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "127.0 3 43  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "130.0 3 44  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "133.0 3 45  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "136.0 3 46  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "139.0 3 47  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "142.0 3 48  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "145.0 3 49  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "148.0 3 50  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "151.0 3 51  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "154.0 3 52  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "157.0 3 53  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "160.0 3 54  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "163.0 3 55  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "166.0 3 56  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "169.0 3 57  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "172.0 3 58  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "175.0 3 59  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "178.0 3 60  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "181.0 3 61  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "184.0 3 62  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "187.0 3 63  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "190.0 3 64  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "193.0 3 65  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "196.0 3 66  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "199.0 3 67  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "202.0 3 68  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "205.0 3 69  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "208.0 3 70  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "211.0 3 71  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "214.0 3 72  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "217.0 3 73  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "220.0 3 74  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "223.0 3 75  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "226.0 3 76  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "229.0 3 77  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "232.0 3 78  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "235.0 3 79  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "238.0 3 80  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "241.0 3 81  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "244.0 3 82  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "247.0 3 83  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "250.0 3 84  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "253.0 3 85  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "256.0 3 86  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "259.0 3 87  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "262.0 3 88  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "265.0 3 89  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "268.0 3 90  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "271.0 3 91  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "274.0 3 92  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "277.0 3 93  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "280.0 3 94  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "283.0 3 95  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "286.0 3 96  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "289.0 3 97  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "292.0 3 98  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "295.0 3 99  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "298.0 3 100  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "creating random agent\n",
      "creating random agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "Random Episode  3\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 1  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 2  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 3  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 4  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 5  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 6  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 7  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 8  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random agent step\n",
      "0.0 0 9  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 10  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "0.0 0 11  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 12  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 13  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 14  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 15  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 16  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 17  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 18  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 19  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 20  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 21  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 22  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 23  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 24  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 25  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 26  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 27  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 28  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 29  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 30  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 31  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 32  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 33  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 34  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 35  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 36  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 37  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 38  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 39  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 40  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 41  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 42  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 43  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 44  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 45  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 46  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 47  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 48  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 49  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 50  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 51  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 52  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 53  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 54  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 55  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 56  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 57  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 58  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "2.0 2 59  Episode:  3\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "random agent step\n",
      "random agent step\n",
      "4.0 2 60  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "6.0 2 61  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "8.0 2 62  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "10.0 2 63  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "12.0 2 64  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "14.0 2 65  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "16.0 2 66  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "18.0 2 67  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "20.0 2 68  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "22.0 2 69  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "24.0 2 70  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "26.0 2 71  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "28.0 2 72  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "30.0 2 73  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "32.0 2 74  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "34.0 2 75  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "36.0 2 76  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "38.0 2 77  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "40.0 2 78  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "42.0 2 79  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "44.0 2 80  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "46.0 2 81  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "48.0 2 82  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "50.0 2 83  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random agent step\n",
      "random agent step\n",
      "52.0 2 84  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "54.0 2 85  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "56.0 2 86  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "58.0 2 87  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "60.0 2 88  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "62.0 2 89  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "64.0 2 90  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "66.0 2 91  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "68.0 2 92  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "70.0 2 93  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "72.0 2 94  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "74.0 2 95  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "76.0 2 96  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "78.0 2 97  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "80.0 2 98  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "82.0 2 99  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "84.0 2 100  Episode:  3\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "creating random agent\n",
      "creating random agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "Random Episode  4\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 1  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 2  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 3  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 4  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 5  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 6  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 7  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 8  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 9  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 10  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 11  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 12  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 13  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 14  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 15  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 16  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 17  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 18  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 19  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 20  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 21  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 22  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 23  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 24  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 25  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 26  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 27  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "0.0 0 28  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 29  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 30  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 31  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 32  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 33  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 34  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 35  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 36  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 37  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 38  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 39  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 40  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 41  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 42  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 43  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 44  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 45  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 46  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 47  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 48  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 49  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 50  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 51  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 52  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 53  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 54  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 55  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 56  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 57  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 58  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 59  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 60  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 61  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 62  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 63  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 64  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 65  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 66  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 67  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 68  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 69  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 70  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 71  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 72  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 73  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 74  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 75  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 76  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 77  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 78  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 79  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 80  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 81  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 82  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 83  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random agent step\n",
      "random agent step\n",
      "0.0 0 84  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 85  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 86  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "random agent step\n",
      "2.0 2 87  Episode:  4\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "random agent step\n",
      "5.0 3 88  Episode:  4\n",
      "New cow in the goal: 3\n",
      "cows in goal:  3 , previous_cow_count:  2  reward:  50\n",
      "random agent step\n",
      "random agent step\n",
      "8.0 3 89  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "11.0 3 90  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "14.0 3 91  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "17.0 3 92  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "20.0 3 93  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "23.0 3 94  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "26.0 3 95  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "29.0 3 96  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "32.0 3 97  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "35.0 3 98  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "38.0 3 99  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "41.0 3 100  Episode:  4\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "creating random agent\n",
      "creating random agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "Random Episode  5\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 1  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 2  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 3  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 4  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 5  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 6  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 7  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 8  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 9  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 10  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 11  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 12  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 13  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 14  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 15  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 16  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 17  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 18  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 19  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 20  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 21  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 22  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 23  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 24  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 25  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 26  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 27  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 28  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 29  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 30  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 31  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 32  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 33  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 34  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 35  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 36  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 37  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 38  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 39  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 40  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 41  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 42  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 43  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 44  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 45  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 46  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 47  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 48  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 49  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 50  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 51  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 52  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 53  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 54  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 55  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 56  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 57  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 58  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 59  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 60  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 61  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 62  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 63  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 64  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 65  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 66  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 67  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 68  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 69  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 70  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 71  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 72  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 73  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 74  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 75  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 76  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 77  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 78  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 79  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 80  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 81  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 82  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 83  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 84  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 85  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "random agent step\n",
      "1.0 1 86  Episode:  5\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0  reward:  50\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 87  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 88  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 89  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 90  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 91  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 92  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 93  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 94  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 95  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 96  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 97  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 98  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 99  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "1.0 0 100  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating random agent\n",
      "creating random agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "Random Episode  6\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 1  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 2  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 3  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 4  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 5  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 6  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 7  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 8  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 9  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 10  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 11  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 12  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 13  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 14  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 15  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 16  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 17  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 18  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 19  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 20  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 21  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 22  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 23  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 24  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 25  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 26  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 27  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 28  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 29  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 30  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 31  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 32  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 33  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 34  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 35  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 36  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 37  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 38  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 39  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 40  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 41  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random agent step\n",
      "0.0 0 42  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 43  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 44  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 45  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 46  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 47  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 48  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 49  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 50  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 51  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 52  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 53  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 54  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 55  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 56  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 57  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 58  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 59  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 60  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 61  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 62  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 63  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 64  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 65  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 66  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 67  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 68  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 69  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 70  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 71  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 72  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 73  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 74  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 75  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 76  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 77  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 78  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 79  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 80  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 81  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 82  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 83  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 84  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 85  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 86  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 87  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 88  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 89  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 90  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 91  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 92  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 93  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 94  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 95  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 96  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 97  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 98  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 99  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 100  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating random agent\n",
      "creating random agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "Random Episode  7\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 1  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 2  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 3  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 4  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 5  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 6  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 7  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 8  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 9  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "0.0 0 10  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 11  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 12  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 13  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "random agent step\n",
      "2.0 2 14  Episode:  7\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "5.0 3 15  Episode:  7\n",
      "New cow in the goal: 3\n",
      "cows in goal:  3 , previous_cow_count:  2  reward:  50\n",
      "random agent step\n",
      "random agent step\n",
      "8.0 3 16  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "11.0 3 17  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "14.0 3 18  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "17.0 3 19  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "20.0 3 20  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "23.0 3 21  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "26.0 3 22  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "29.0 3 23  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "32.0 3 24  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "35.0 3 25  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "38.0 3 26  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "41.0 3 27  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "44.0 3 28  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "47.0 3 29  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "50.0 3 30  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "53.0 3 31  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "56.0 3 32  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "59.0 3 33  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "62.0 3 34  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "65.0 3 35  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "68.0 3 36  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "71.0 3 37  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "74.0 3 38  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "77.0 3 39  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "80.0 3 40  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "83.0 3 41  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "86.0 3 42  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "89.0 3 43  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "92.0 3 44  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "95.0 3 45  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "98.0 3 46  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "101.0 3 47  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "104.0 3 48  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "107.0 3 49  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "110.0 3 50  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "113.0 3 51  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "116.0 3 52  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "119.0 3 53  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random agent step\n",
      "random agent step\n",
      "122.0 3 54  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "125.0 3 55  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "128.0 3 56  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "131.0 3 57  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "134.0 3 58  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "137.0 3 59  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "140.0 3 60  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "143.0 3 61  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "146.0 3 62  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "149.0 3 63  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "152.0 3 64  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "155.0 3 65  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "158.0 3 66  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "161.0 3 67  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "164.0 3 68  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "167.0 3 69  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "170.0 3 70  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "173.0 3 71  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "176.0 3 72  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "179.0 3 73  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "182.0 3 74  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "185.0 3 75  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "188.0 3 76  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "191.0 3 77  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "194.0 3 78  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "197.0 3 79  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "200.0 3 80  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "203.0 3 81  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "206.0 3 82  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "209.0 3 83  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "212.0 3 84  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "215.0 3 85  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "218.0 3 86  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "221.0 3 87  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "224.0 3 88  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "227.0 3 89  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "230.0 3 90  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "233.0 3 91  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "236.0 3 92  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "239.0 3 93  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "242.0 3 94  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "245.0 3 95  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "248.0 3 96  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "251.0 3 97  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "254.0 3 98  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "257.0 3 99  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "260.0 3 100  Episode:  7\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "creating random agent\n",
      "creating random agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "Random Episode  8\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 1  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 2  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 3  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 4  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 5  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 6  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 7  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 8  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 9  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 10  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 11  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 12  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 13  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 14  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 15  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 16  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 17  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 18  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 19  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 20  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 21  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 22  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 23  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 24  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 25  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random agent step\n",
      "random agent step\n",
      "0.0 0 26  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 27  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 28  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 29  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 30  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 31  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 32  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 33  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 34  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 35  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 36  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 37  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 38  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 39  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 40  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 41  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 42  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 43  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 44  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 45  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 46  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 47  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 48  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 49  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 50  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 51  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 52  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 53  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 54  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 55  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 56  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 57  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 58  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 59  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 60  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 61  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 62  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 63  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 64  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 65  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 66  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 67  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 68  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 69  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 70  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 71  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 72  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 73  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 74  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 75  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 76  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 77  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 78  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 79  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 80  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 81  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 82  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 83  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 84  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 85  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 86  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 87  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 88  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 89  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 90  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 91  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 92  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 93  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 94  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 95  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 96  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 97  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 98  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 99  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 100  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating random agent\n",
      "creating random agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "Random Episode  9\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 1  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 2  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 3  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 4  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 5  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 6  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 7  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 8  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 9  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 10  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 11  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 12  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 13  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 14  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 15  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 16  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 17  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 18  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 19  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 20  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 21  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 22  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 23  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 24  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 25  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 26  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 27  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 28  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 29  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 30  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 31  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 32  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 33  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "random agent step\n",
      "0.0 0 34  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "2.0 2 35  Episode:  9\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "random agent step\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "random agent step\n",
      "6.0 4 36  Episode:  9\n",
      "New cow in the goal: 4\n",
      "cows in goal:  4 , previous_cow_count:  2  reward:  50\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "10.0 4 37  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "14.0 4 38  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "18.0 4 39  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "22.0 4 40  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "26.0 4 41  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "30.0 4 42  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "34.0 4 43  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "38.0 4 44  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "42.0 4 45  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "46.0 4 46  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "50.0 4 47  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "54.0 4 48  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "58.0 4 49  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "62.0 4 50  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "66.0 4 51  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "70.0 4 52  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "74.0 4 53  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "78.0 4 54  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "82.0 4 55  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "86.0 4 56  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "90.0 4 57  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "94.0 4 58  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "98.0 4 59  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "102.0 4 60  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "106.0 4 61  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "110.0 4 62  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "114.0 4 63  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "118.0 4 64  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "122.0 4 65  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "126.0 4 66  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "130.0 4 67  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "134.0 4 68  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "138.0 4 69  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "142.0 4 70  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "146.0 4 71  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "150.0 4 72  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "154.0 4 73  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "158.0 4 74  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "162.0 4 75  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "166.0 4 76  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "170.0 4 77  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "174.0 4 78  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "178.0 4 79  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "182.0 4 80  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "186.0 4 81  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "190.0 4 82  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "194.0 4 83  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "198.0 4 84  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "202.0 4 85  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "206.0 4 86  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "210.0 4 87  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "214.0 4 88  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "218.0 4 89  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "222.0 4 90  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "226.0 4 91  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "230.0 4 92  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "234.0 4 93  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "238.0 4 94  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "242.0 4 95  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "246.0 4 96  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "250.0 4 97  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "254.0 4 98  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "258.0 4 99  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "random agent step\n",
      "random agent step\n",
      "262.0 4 100  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# Collect times for random_agents\n",
    "final_random_scores = []\n",
    "final_random_times = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    model = CHModel(10, 10, cow_n = cow_agents, random_n = random_agents, episode_number = episode)\n",
    "\n",
    "    print(\"Random Episode \", episode)\n",
    "    for i in range(steps):\n",
    "        model.step()\n",
    "        # if the agents are able to herd the cows in the given number of timesteps, save the time finished\n",
    "        if(model.done):\n",
    "            final_random_times.append(i)\n",
    "            #save the final score\n",
    "            final_random_scores.append(model.score)\n",
    "    # if the agents were not able to herd the cows in the given number of timesteps, save the maximum time allowed and the end score\n",
    "    if (not(model.done)):\n",
    "        final_random_times.append(steps)\n",
    "        final_random_scores.append(model.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 100, 100, 100, 100, 100, 100, 100, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "print(final_random_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38461538461538464, 0.475, 0.5609756097560976, 0.6428571428571429, 0.7209302325581395, 0.7954545454545454, 0.8666666666666667, 0.9347826086956522, 1.0, 1.0625, 1.1224489795918366, 1.18, 1.2352941176470589, 1.2884615384615385, 1.3396226415094339, 1.3888888888888888, 1.4363636363636363, 1.4821428571428572, 1.5263157894736843, 1.5689655172413792, 1.6101694915254237, 1.65, 1.6885245901639345, 1.7258064516129032, 1.7619047619047619, 1.796875, 1.8307692307692307, 1.8636363636363635, 1.8955223880597014, 1.9264705882352942, 1.9565217391304348, 1.9857142857142858, 2.0140845070422535, 2.0416666666666665, 2.0684931506849313, 2.0945945945945947, 2.12, 2.1447368421052633, 2.168831168831169, 2.1923076923076925, 2.2151898734177213, 2.2375, 2.259259259259259, 2.2804878048780486, 2.3012048192771086, 2.3214285714285716, 2.3411764705882354, 2.36046511627907, 2.3793103448275863, 2.397727272727273, 2.4157303370786516, 2.433333333333333, 2.4505494505494507, 2.467391304347826, 2.4838709677419355, 2.5, 2.5157894736842104, 2.53125, 2.5463917525773194, 2.561224489795918, 2.5757575757575757, 2.59, 1.55, 2.98, 0.84, 0.41, 0.01, 0.0, 2.6, 0.0, 0.16666666666666666, 0.2702702702702703, 0.3684210526315789, 0.46153846153846156, 0.55, 0.6341463414634146, 0.7142857142857143, 0.7906976744186046, 0.8636363636363636, 0.9333333333333333, 1.0, 1.0638297872340425, 1.125, 1.183673469387755, 1.24, 1.2941176470588236, 1.3461538461538463, 1.3962264150943395, 1.4444444444444444, 1.490909090909091, 1.5357142857142858, 1.5789473684210527, 1.6206896551724137, 1.6610169491525424, 1.7, 1.7377049180327868, 1.7741935483870968, 1.8095238095238095, 1.84375, 1.876923076923077, 1.9090909090909092, 1.9402985074626866, 1.9705882352941178, 2.0, 2.0285714285714285, 2.056338028169014, 2.0833333333333335, 2.1095890410958904, 2.135135135135135, 2.16, 2.1842105263157894, 2.207792207792208, 2.230769230769231, 2.2531645569620253, 2.275, 2.2962962962962963, 2.317073170731707, 2.3373493975903616, 2.357142857142857, 2.376470588235294, 2.395348837209302, 2.413793103448276, 2.4318181818181817, 2.449438202247191, 2.466666666666667, 2.4835164835164836, 2.5, 2.5161290322580645, 2.5319148936170213, 2.5473684210526315, 2.5625, 2.577319587628866, 2.5918367346938775, 2.606060606060606, 2.62]\n"
     ]
    }
   ],
   "source": [
    "print(final_random_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating plan agent\n",
      "creating plan agent\n",
      "Plan Episode  0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "0.0 0 1  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 2  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 3  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 4  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 5  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 6  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "0.0 0 7  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 8  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "0.0 0 9  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 1)\n",
      "0.0 0 10  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 11  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 12  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "0.0 0 13  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 14  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "0.0 0 15  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 16  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "0.0 0 17  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "0.0 0 18  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 2)\n",
      "0.0 0 19  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 20  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 2)\n",
      "0.0 0 21  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 2)\n",
      "0.0 0 22  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 23  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 24  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 25  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 26  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 27  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 28  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 29  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 30  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 31  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 32  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 33  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 34  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 35  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 36  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "0.0 0 37  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 7)\n",
      "0.0 0 38  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 7)\n",
      "0.0 0 39  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 5)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "0.0 0 40  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 3)\n",
      "0.0 0 41  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "0.0 0 42  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "0.0 0 43  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "0.0 0 44  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 45  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "0.0 0 46  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "0.0 0 47  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "0.0 0 48  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 5)\n",
      "0.0 0 49  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 50  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 51  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 52  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 53  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 54  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 55  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 56  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 57  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "0.0 0 58  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 1)\n",
      "0.0 0 59  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 60  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 61  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "0.0 0 62  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 63  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 64  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 65  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 66  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 67  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 68  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 69  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 70  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 71  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "0.0 0 72  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "0.0 0 73  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 74  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 75  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 76  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 77  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 78  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "0.0 0 79  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 80  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 81  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 82  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 83  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 84  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 85  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 86  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 87  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 88  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 89  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 90  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 91  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 92  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 93  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 8)\n",
      "0.0 0 94  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 95  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "0.0 0 96  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 97  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 98  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 99  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 100  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating plan agent\n",
      "creating plan agent\n",
      "Plan Episode  1\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 1  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 2  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "0.0 0 3  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 4  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 5  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 6  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 7  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 8  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 9  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 10  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 9)\n",
      "0.0 0 11  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 12  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 13  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 14  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 15  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 16  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 17  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 18  Episode:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 19  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 20  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 21  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 22  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 23  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 24  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 25  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 26  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 27  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 28  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 29  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 30  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 31  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 32  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 33  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 34  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 35  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 36  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 37  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 38  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "0.0 0 39  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 40  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 41  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 42  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 43  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 44  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 45  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 46  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 47  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 48  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "0.0 0 49  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 5)\n",
      "0.0 0 50  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 7)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 7)\n",
      "0.0 0 51  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 8)\n",
      "0.0 0 52  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 53  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 54  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 55  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 56  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 57  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 58  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 59  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 60  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 61  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 62  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 63  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 64  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 65  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 66  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 67  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 68  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 69  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 70  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "0.0 0 71  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "0.0 0 72  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "0.0 0 73  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 74  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "0.0 0 75  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 76  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "0.0 0 77  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 3)\n",
      "2.0 2 78  Episode:  1\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "4.0 2 79  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 3)\n",
      "6.0 2 80  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 3)\n",
      "8.0 2 81  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "10.0 2 82  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "12.0 2 83  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "14.0 2 84  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "16.0 2 85  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "18.0 2 86  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "20.0 2 87  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "22.0 2 88  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "24.0 2 89  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "26.0 2 90  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "28.0 2 91  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "30.0 2 92  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "32.0 2 93  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "34.0 2 94  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 2)\n",
      "36.0 2 95  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "38.0 2 96  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "40.0 2 97  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "42.0 2 98  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "44.0 2 99  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "46.0 2 100  Episode:  1\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating plan agent\n",
      "creating plan agent\n",
      "Plan Episode  2\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "0.0 0 1  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 2  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 3  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 4  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 2)\n",
      "0.0 0 5  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 6  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 7  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 8  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 9  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 10  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 11  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 12  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 13  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 14  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 15  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 16  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 17  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 18  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 19  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 20  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 21  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 22  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "0.0 0 23  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 24  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 25  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "0.0 0 26  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 1)\n",
      "0.0 0 27  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 2)\n",
      "0.0 0 28  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 2)\n",
      "0.0 0 29  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "0.0 0 30  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "0.0 0 31  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 3)\n",
      "0.0 0 32  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 3)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "0.0 0 33  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 34  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 35  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 36  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 37  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 38  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 39  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 40  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 41  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 42  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 43  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 44  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 45  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 46  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 47  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "1.0 1 48  Episode:  2\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0  reward:  50\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "1.0 0 49  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "1.0 0 50  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "1.0 0 51  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "2.0 1 52  Episode:  2\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0  reward:  50\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "2.0 0 53  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "2.0 0 54  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "2.0 0 55  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "2.0 0 56  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "2.0 0 57  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "2.0 0 58  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "2.0 0 59  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "2.0 0 60  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "2.0 0 61  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "2.0 0 62  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 2)\n",
      "2.0 0 63  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 2)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "2.0 0 64  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "2.0 0 65  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "4.0 2 66  Episode:  2\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "6.0 2 67  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 3)\n",
      "9.0 3 68  Episode:  2\n",
      "New cow in the goal: 3\n",
      "cows in goal:  3 , previous_cow_count:  2  reward:  50\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 7)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 7)\n",
      "12.0 3 69  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 7)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 7)\n",
      "15.0 3 70  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "18.0 3 71  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "21.0 3 72  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "24.0 3 73  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "27.0 3 74  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "30.0 3 75  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "33.0 3 76  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "36.0 3 77  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "39.0 3 78  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "42.0 3 79  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 5)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 5)\n",
      "45.0 3 80  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 7)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 7)\n",
      "48.0 3 81  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 8)\n",
      "51.0 3 82  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "54.0 3 83  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "57.0 3 84  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "60.0 3 85  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "63.0 3 86  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "66.0 3 87  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 9)\n",
      "69.0 3 88  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "72.0 3 89  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "75.0 3 90  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "78.0 3 91  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "81.0 3 92  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "84.0 3 93  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "87.0 3 94  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "90.0 3 95  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "93.0 3 96  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "96.0 3 97  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "99.0 3 98  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "102.0 3 99  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "105.0 3 100  Episode:  2\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating plan agent\n",
      "creating plan agent\n",
      "Plan Episode  3\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 1  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 2  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 5)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 3  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 7)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 4  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 5  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 6  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 7  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 8  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 9  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 10  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 11  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 12  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 13  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 14  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 15  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 16  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "0.0 0 17  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 18  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 19  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 20  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 21  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 22  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 23  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 24  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 25  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 26  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 27  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 28  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 29  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 30  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 31  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 32  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 33  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 34  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 35  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 36  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 37  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 38  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 39  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 40  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 41  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 42  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 43  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 44  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 45  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 46  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 47  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 48  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 49  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 50  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 51  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 52  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "0.0 0 53  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "0.0 0 54  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 3)\n",
      "0.0 0 55  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 7)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 7)\n",
      "0.0 0 56  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "0.0 0 57  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 58  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 59  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 60  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 61  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 62  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 63  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 64  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 65  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 66  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 67  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 68  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 69  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 70  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 71  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 72  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 73  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 74  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 75  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 76  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 77  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 78  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 79  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 80  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 81  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 82  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 83  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 84  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 85  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 86  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 87  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 88  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 89  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 90  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 91  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 92  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 93  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 94  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 95  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 96  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 97  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 98  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 99  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 100  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating plan agent\n",
      "creating plan agent\n",
      "Plan Episode  4\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 1  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 2  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 3  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "0.0 0 4  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 5  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 5)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 6  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 7)\n",
      "0.0 0 7  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 7)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 8  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 8)\n",
      "0.0 0 9  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 10  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "0.0 0 11  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 12  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 13  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "0.0 0 14  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 15  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 16  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 17  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 18  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 19  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 20  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 21  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 8)\n",
      "0.0 0 22  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 23  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 24  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 25  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 26  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 27  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 28  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 29  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 30  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 31  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 32  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 33  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 34  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 35  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 3)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 36  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "2.0 2 37  Episode:  4\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "4.0 2 38  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "6.0 2 39  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 2 40  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "10.0 2 41  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "12.0 2 42  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "14.0 2 43  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "16.0 2 44  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "18.0 2 45  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "20.0 2 46  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "22.0 2 47  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "24.0 2 48  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "26.0 2 49  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "28.0 2 50  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "30.0 2 51  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "32.0 2 52  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "34.0 2 53  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "36.0 2 54  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 9)\n",
      "38.0 2 55  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "40.0 2 56  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "42.0 2 57  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "44.0 2 58  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "46.0 2 59  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "48.0 2 60  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "50.0 2 61  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "52.0 2 62  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "54.0 2 63  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "56.0 2 64  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "58.0 2 65  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "60.0 2 66  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "62.0 2 67  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "64.0 2 68  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "66.0 2 69  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "68.0 2 70  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "70.0 2 71  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "72.0 2 72  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "74.0 2 73  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "76.0 2 74  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "78.0 2 75  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "80.0 2 76  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.0 2 77  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "84.0 2 78  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "86.0 2 79  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "88.0 2 80  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "90.0 2 81  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "92.0 2 82  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "94.0 2 83  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "96.0 2 84  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "98.0 2 85  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "100.0 2 86  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "102.0 2 87  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "104.0 2 88  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "106.0 2 89  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "108.0 2 90  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "110.0 2 91  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "112.0 2 92  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "114.0 2 93  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "116.0 2 94  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "118.0 2 95  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "120.0 2 96  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "122.0 2 97  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "124.0 2 98  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "126.0 2 99  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "128.0 2 100  Episode:  4\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating plan agent\n",
      "creating plan agent\n",
      "Plan Episode  5\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "1.0 1 1  Episode:  5\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0.0  reward:  50\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "2.0 1 2  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "3.0 1 3  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "4.0 1 4  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "5.0 1 5  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "6.0 1 6  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "7.0 1 7  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "8.0 1 8  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 9  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "8.0 0 10  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 11  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 12  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 13  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "8.0 0 14  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 15  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 16  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 17  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 18  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "8.0 0 19  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 20  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 21  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 22  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 2)\n",
      "8.0 0 23  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 2)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 24  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 2)\n",
      "8.0 0 25  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 2)\n",
      "8.0 0 26  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 3)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 27  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 2)\n",
      "8.0 0 28  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "8.0 0 29  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "8.0 0 30  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 31  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 32  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 33  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "8.0 0 34  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 3)\n",
      "8.0 0 35  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 3)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 0 36  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0 0 37  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 7)\n",
      "9.0 1 38  Episode:  5\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0  reward:  50\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 8)\n",
      "10.0 1 39  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "11.0 1 40  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "12.0 1 41  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "13.0 1 42  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "14.0 1 43  Episode:  5\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "14.0 0 44  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "14.0 0 45  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "14.0 0 46  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "14.0 0 47  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "14.0 0 48  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "14.0 0 49  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "14.0 0 50  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "14.0 0 51  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "14.0 0 52  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "14.0 0 53  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "14.0 0 54  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "14.0 0 55  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "14.0 0 56  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "14.0 0 57  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "14.0 0 58  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "14.0 0 59  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "14.0 0 60  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "14.0 0 61  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "14.0 0 62  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 7)\n",
      "14.0 0 63  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 7)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "14.0 0 64  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 5)\n",
      "14.0 0 65  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "14.0 0 66  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "14.0 0 67  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "14.0 0 68  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "14.0 0 69  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "14.0 0 70  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "14.0 0 71  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "14.0 0 72  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "14.0 0 73  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "14.0 0 74  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "14.0 0 75  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "14.0 0 76  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "14.0 0 77  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "14.0 0 78  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "14.0 0 79  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "14.0 0 80  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "14.0 0 81  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "14.0 0 82  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "14.0 0 83  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0 0 84  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 2)\n",
      "14.0 0 85  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "14.0 0 86  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "14.0 0 87  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "14.0 0 88  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "14.0 0 89  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "14.0 0 90  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "14.0 0 91  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "14.0 0 92  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "14.0 0 93  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "14.0 0 94  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "14.0 0 95  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "14.0 0 96  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "14.0 0 97  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "14.0 0 98  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "14.0 0 99  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "14.0 0 100  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating plan agent\n",
      "creating plan agent\n",
      "Plan Episode  6\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "0.0 0 1  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 2  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 2)\n",
      "0.0 0 3  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 2)\n",
      "0.0 0 4  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 2)\n",
      "0.0 0 5  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "0.0 0 6  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 7  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 8  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 9  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 10  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 11  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 12  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "0.0 0 13  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 14  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 15  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 16  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 17  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 18  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 19  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 20  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 21  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 22  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 23  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 24  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 25  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 26  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "0.0 0 27  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 7)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 28  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 7)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 29  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 3)\n",
      "0.0 0 30  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 31  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 32  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 33  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 34  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 35  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 36  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 37  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 38  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 39  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 40  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 41  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "0.0 0 42  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 43  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "0.0 0 44  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "0.0 0 45  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "0.0 0 46  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 47  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 48  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 49  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 50  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 51  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 52  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 53  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "0.0 0 54  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 55  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "0.0 0 56  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "0.0 0 57  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "0.0 0 58  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 59  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 9)\n",
      "0.0 0 60  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 9)\n",
      "0.0 0 61  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 62  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "0.0 0 63  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 2)\n",
      "0.0 0 64  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 3)\n",
      "2.0 2 65  Episode:  6\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 3)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "5.0 3 66  Episode:  6\n",
      "New cow in the goal: 3\n",
      "cows in goal:  3 , previous_cow_count:  2  reward:  50\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 5)\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "8.0 3 67  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 7)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "11.0 3 68  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 7)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "14.0 3 69  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "17.0 3 70  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "20.0 3 71  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "23.0 3 72  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "26.0 3 73  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "29.0 3 74  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "32.0 3 75  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "35.0 3 76  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "38.0 3 77  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "41.0 3 78  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "44.0 3 79  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "47.0 3 80  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "50.0 3 81  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "53.0 3 82  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "56.0 3 83  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "59.0 3 84  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "62.0 3 85  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "65.0 3 86  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "68.0 3 87  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "71.0 3 88  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "74.0 3 89  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "77.0 3 90  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "80.0 3 91  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "83.0 3 92  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "86.0 3 93  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "89.0 3 94  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "92.0 3 95  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "95.0 3 96  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "98.0 3 97  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "101.0 3 98  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "104.0 3 99  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "107.0 3 100  Episode:  6\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating plan agent\n",
      "creating plan agent\n",
      "Plan Episode  7\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "0.0 0 1  Episode:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 2)\n",
      "0.0 0 2  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 1)\n",
      "0.0 0 3  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 4  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 5  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "0.0 0 6  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 7  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 8  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 9  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 10  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "0.0 0 11  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 12  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 13  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 14  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 15  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "0.0 0 16  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "0.0 0 17  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "0.0 0 18  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "0.0 0 19  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "0.0 0 20  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 21  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 22  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 23  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "0.0 0 24  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "0.0 0 25  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "0.0 0 26  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 27  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 28  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n",
      "0.0 0 29  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 30  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 31  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 32  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 33  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 34  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 35  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 36  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 37  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 38  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 39  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 40  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 41  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 42  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 43  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 44  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "0.0 0 45  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 46  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 47  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 8)\n",
      "0.0 0 48  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 49  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 50  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 51  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 52  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "0.0 0 53  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 54  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 55  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 56  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 57  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 58  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 59  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 60  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 61  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 62  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 63  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 64  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 65  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 66  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 67  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 68  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 69  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 70  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "0.0 0 71  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "0.0 0 72  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 73  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 1)\n",
      "0.0 0 74  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "0.0 0 75  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 3)\n",
      "0.0 0 76  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 77  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "0.0 0 78  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 79  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "0.0 0 80  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 81  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 82  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 83  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 84  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 85  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 86  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 87  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "0.0 0 88  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 89  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 90  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 91  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 92  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "0.0 0 93  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 3)\n",
      "0.0 0 94  Episode:  7\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 7)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 3)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "2.0 2 95  Episode:  7\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 8)\n",
      "4.0 2 96  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "6.0 2 97  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "8.0 2 98  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "10.0 2 99  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "12.0 2 100  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating plan agent\n",
      "creating plan agent\n",
      "Plan Episode  8\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 1  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 2  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 3  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 4  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 5  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 6  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 7  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 8  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "0.0 0 9  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 10  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 0)\n",
      "0.0 0 11  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 12  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 13  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 0)\n",
      "0.0 0 14  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 15  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "0.0 0 16  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 17  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 18  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 19  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 20  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "0.0 0 21  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 22  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "0.0 0 23  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 9)\n",
      "0.0 0 24  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 25  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 26  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 27  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 28  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 29  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "0.0 0 30  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 31  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "0.0 0 32  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "0.0 0 33  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 9)\n",
      "0.0 0 34  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 9)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "0.0 0 35  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 9)\n",
      "0.0 0 36  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 8)\n",
      "0.0 0 37  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "0.0 0 38  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 8)\n",
      "0.0 0 39  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "0.0 0 40  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "0.0 0 41  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "0.0 0 42  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 8)\n",
      "0.0 0 43  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 44  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 8)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 45  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 46  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 47  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 48  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 49  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 50  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 0)\n",
      "0.0 0 51  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 52  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "0.0 0 53  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "0.0 0 54  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 55  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (8, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 56  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 9)\n",
      "0.0 0 57  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 9)\n",
      "0.0 0 58  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (7, 0)\n",
      "0.0 0 59  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 9)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "0.0 0 60  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 0)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 61  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 1)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 0)\n",
      "0.0 0 62  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 2)\n",
      "0.0 0 63  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 2)\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (4, 3)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "0.0 0 64  Episode:  8\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 3)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "2.0 2 65  Episode:  8\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "4.0 2 66  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "6.0 2 67  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "8.0 2 68  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 2 69  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "12.0 2 70  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "14.0 2 71  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "16.0 2 72  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "18.0 2 73  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "20.0 2 74  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "22.0 2 75  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 3)\n",
      "24.0 2 76  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (6, 3)\n",
      "26.0 2 77  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (5, 2)\n",
      "28.0 2 78  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 2)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "30.0 2 79  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (3, 2)\n",
      "32.0 2 80  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "34.0 2 81  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "36.0 2 82  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "38.0 2 83  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "40.0 2 84  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "42.0 2 85  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 2)\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "44.0 2 86  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "46.0 2 87  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 2)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "48.0 2 88  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "50.0 2 89  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 3)\n",
      "52.0 2 90  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "54.0 2 91  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "56.0 2 92  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "58.0 2 93  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (2, 2)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "60.0 2 94  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "62.0 2 95  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "64.0 2 96  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (9, 0)\n",
      "66.0 2 97  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (0, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "68.0 2 98  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "70.0 2 99  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "plan agent step\n",
      "herding cow\n",
      "moving towards  (1, 1)\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "72.0 2 100  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating plan agent\n",
      "creating plan agent\n",
      "Plan Episode  9\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "3.0 3 1  Episode:  9\n",
      "New cow in the goal: 3\n",
      "cows in goal:  3 , previous_cow_count:  0.0  reward:  50\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "6.0 3 2  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "9.0 3 3  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "12.0 3 4  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "15.0 3 5  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "18.0 3 6  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "21.0 3 7  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "24.0 3 8  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "27.0 3 9  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "30.0 3 10  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "33.0 3 11  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no cow here, keep looking\n",
      "36.0 3 12  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "39.0 3 13  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "42.0 3 14  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "45.0 3 15  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "48.0 3 16  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "51.0 3 17  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "54.0 3 18  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "57.0 3 19  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "60.0 3 20  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "63.0 3 21  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "66.0 3 22  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "69.0 3 23  Episode:  9\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "73.0 4 24  Episode:  9\n",
      "New cow in the goal: 4\n",
      "cows in goal:  4 , previous_cow_count:  3  reward:  50\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "77.0 4 25  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "81.0 4 26  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "85.0 4 27  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "89.0 4 28  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "93.0 4 29  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "97.0 4 30  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "101.0 4 31  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "105.0 4 32  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "109.0 4 33  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "113.0 4 34  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "117.0 4 35  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "121.0 4 36  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "125.0 4 37  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "129.0 4 38  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "133.0 4 39  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "137.0 4 40  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "141.0 4 41  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "145.0 4 42  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "149.0 4 43  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "153.0 4 44  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "157.0 4 45  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "161.0 4 46  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "165.0 4 47  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "169.0 4 48  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "173.0 4 49  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "177.0 4 50  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "181.0 4 51  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "185.0 4 52  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "189.0 4 53  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "193.0 4 54  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "197.0 4 55  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "201.0 4 56  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "205.0 4 57  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "209.0 4 58  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "213.0 4 59  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "217.0 4 60  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "221.0 4 61  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "225.0 4 62  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "229.0 4 63  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "233.0 4 64  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "237.0 4 65  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "241.0 4 66  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "245.0 4 67  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "249.0 4 68  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "253.0 4 69  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "257.0 4 70  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "261.0 4 71  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "265.0 4 72  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "269.0 4 73  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "273.0 4 74  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "277.0 4 75  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "281.0 4 76  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "285.0 4 77  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "289.0 4 78  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "293.0 4 79  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "297.0 4 80  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "301.0 4 81  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "305.0 4 82  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "309.0 4 83  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "313.0 4 84  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "317.0 4 85  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "321.0 4 86  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "325.0 4 87  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "329.0 4 88  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "333.0 4 89  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "337.0 4 90  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "cow is in the goal - resetting\n",
      "341.0 4 91  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "345.0 4 92  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "349.0 4 93  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "353.0 4 94  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "357.0 4 95  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "361.0 4 96  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "365.0 4 97  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "369.0 4 98  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "373.0 4 99  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "found cow\n",
      "plan agent step\n",
      "finding cow to follow\n",
      "no cow here, keep looking\n",
      "377.0 4 100  Episode:  9\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# Collect times for plan_agents\n",
    "final_plan_scores = []\n",
    "final_plan_times = []\n",
    "for episode in range(episodes):\n",
    "    model = CHModel(10, 10, cow_n = cow_agents, plan_n = plan_agents, episode_number = episode)\n",
    "\n",
    "    print(\"Plan Episode \", episode)\n",
    "    for i in range(steps):\n",
    "        model.step()\n",
    "        # if the agents are able to herd the cows in the given number of timesteps, save the time finished\n",
    "        if(model.done):\n",
    "            final_plan_times.append(i)\n",
    "            #save the final score\n",
    "            final_plan_scores.append(model.score)\n",
    "    # if the agents were not able to herd the cows in the given number of timesteps, save the maximum time allowed\n",
    "    if (not(model.done)):\n",
    "        final_plan_times.append(steps)\n",
    "        final_plan_scores.append(model.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "print(final_plan_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.46, 1.05, 0.0, 1.28, 0.14, 1.07, 0.12, 0.72, 3.0416666666666665, 3.08, 3.1153846153846154, 3.1481481481481484, 3.1785714285714284, 3.206896551724138, 3.2333333333333334, 3.2580645161290325, 3.28125, 3.303030303030303, 3.323529411764706, 3.342857142857143, 3.361111111111111, 3.3783783783783785, 3.3947368421052633, 3.41025641025641, 3.425, 3.4390243902439024, 3.4523809523809526, 3.4651162790697674, 3.477272727272727, 3.488888888888889, 3.5, 3.5106382978723403, 3.5208333333333335, 3.5306122448979593, 3.54, 3.549019607843137, 3.5576923076923075, 3.5660377358490565, 3.574074074074074, 3.581818181818182, 3.5892857142857144, 3.5964912280701755, 3.603448275862069, 3.610169491525424, 3.6166666666666667, 3.622950819672131, 3.629032258064516, 3.634920634920635, 3.640625, 3.646153846153846, 3.6515151515151514, 3.656716417910448, 3.661764705882353, 3.6666666666666665, 3.6714285714285713, 3.676056338028169, 3.6805555555555554, 3.684931506849315, 3.689189189189189, 3.6933333333333334, 3.6973684210526314, 3.7012987012987013, 3.7051282051282053, 3.7088607594936707, 3.7125, 3.7160493827160495, 3.7195121951219514, 3.7228915662650603, 3.7261904761904763, 3.7294117647058824, 3.7325581395348837, 3.735632183908046, 3.7386363636363638, 3.741573033707865, 3.7444444444444445, 3.7472527472527473, 3.75, 3.752688172043011, 3.75531914893617, 3.7578947368421054, 3.7604166666666665, 3.7628865979381443, 3.7653061224489797, 3.7676767676767677, 3.77]\n"
     ]
    }
   ],
   "source": [
    "print(final_plan_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating trained mc agent with vision range  2\n",
      "creating trained mc agent with vision range  2\n",
      "Monte Carlo Episode  0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -2.49282953   0.          -5.10580674  16.41220304 -19.35421611\n",
      "   7.70304236   0.           6.33952232   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 7)  new position  (8, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.82087061 -36.97603648 -40.37821211 -63.24831962 -35.21881822\n",
      " -46.65772308 -36.22761687 -27.25788312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 4)  new position  (2, 4)\n",
      "0.0 0 1  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-19.25776765 -16.71693457   1.14421871  -7.78298141 -19.78017513\n",
      "   2.83826722 -12.45137718   6.02343751   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 6)  new position  (9, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 4)  new position  (3, 3)\n",
      "0.0 0 2  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 5)  new position  (8, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 3)  new position  (3, 2)\n",
      "0.0 0 3  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 2)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 4)  new position  (9, 5)\n",
      "0.0 0 4  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 5)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 3)  new position  (4, 4)\n",
      "0.0 0 5  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 4)  new position  (4, 5)\n",
      "0.0 0 6  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 5)  new position  (5, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 6)  new position  (9, 5)\n",
      "0.0 0 7  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 4)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 5)  new position  (8, 4)\n",
      "0.0 0 8  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 4)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.         -11.4191486   -7.83123137  12.95638711  -4.2215702\n",
      "   0.         -10.08055401 -12.32662184   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 5)  new position  (4, 5)\n",
      "0.0 0 9  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 3.6359424   4.66838662  3.61232695  0.69975406 -6.82693662 -1.95821441\n",
      " 21.32860793  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 4)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-28.8171411   -6.49845616  -1.71252237 -30.61858531 -11.67747602\n",
      "   8.45500699 -23.41197973 -21.33612572   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 5)  new position  (4, 4)\n",
      "0.0 0 10  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-22.52836948   6.24446617  -3.21944451   9.88502417   1.33041872\n",
      "  -0.14330997   0.37054135  33.47834245   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 3)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 4)  new position  (3, 3)\n",
      "0.0 0 11  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 3)  new position  (7, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 3)  new position  (4, 3)\n",
      "0.0 0 12  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 2)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 3)  new position  (3, 3)\n",
      "0.0 0 13  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 3)  new position  (8, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 3)  new position  (2, 4)\n",
      "0.0 0 14  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 2)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 4)  new position  (3, 4)\n",
      "0.0 0 15  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 3)  new position  (8, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.82087061 -36.97603648 -40.37821211 -63.24831962 -35.21881822\n",
      " -46.65772308 -36.22761687 -27.25788312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 4)  new position  (4, 4)\n",
      "0.0 0 16  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 4)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 4)  new position  (8, 3)\n",
      "0.0 0 17  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 3)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 3)  new position  (2, 2)\n",
      "0.0 0 18  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 4)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 2)  new position  (3, 1)\n",
      "0.0 0 19  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 1)  new position  (3, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 3)  new position  (0, 3)\n",
      "0.0 0 20  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 31.284968    -4.82813909 -10.67881149  -1.93536291   4.62396222\n",
      "   6.19813269  -5.04263151  -6.92183961   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 0)  new position  (3, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 3)  new position  (9, 3)\n",
      "0.0 0 21  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 3)  new position  (8, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 1)  new position  (4, 2)\n",
      "0.0 0 22  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 2)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 4)  new position  (7, 3)\n",
      "0.0 0 23  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 3)  new position  (3, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 3)  new position  (6, 4)\n",
      "0.0 0 24  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 4)  new position  (7, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 2)  new position  (2, 2)\n",
      "0.0 0 25  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.52939684   0.         -16.17025784 -20.99596012 -43.77061943\n",
      " -17.78290425  10.17861875   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 5)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 2)  new position  (3, 2)\n",
      "0.0 0 26  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 2)  new position  (3, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 4)  new position  (5, 4)\n",
      "0.0 0 27  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 1)  new position  (2, 0)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 4)  new position  (6, 4)\n",
      "0.0 0 28  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.13376359   0.55244619   0.82926859   6.67461715  20.98817125\n",
      "  15.01263039  -1.1080356  -12.02177202   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 0)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 4)  new position  (5, 3)\n",
      "0.0 0 29  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.00746759 -11.1412527  -19.62385086  -1.01596757  -1.60346104\n",
      "  15.51659352 -10.01204177  -5.37875616   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 3)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 1)  new position  (1, 2)\n",
      "0.0 0 30  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.66282809  -4.26618692   4.0766115  -11.41362679  10.69348545\n",
      "  11.03964625   1.67209256  -9.43224976   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 2)  new position  (2, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -8.39859752  15.12856558 -21.03434615 -17.15122997  -8.00620146\n",
      " -15.72213677 -25.221612    -9.01008301   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 4)  new position  (4, 3)\n",
      "0.0 0 31  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-9.17851785  8.41308486 -9.88693012 21.39110837  0.92572845  7.20546874\n",
      " -9.90345904 -4.13516439  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 2)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 2.70475348  0.         -3.8738046   6.0630691  -1.48127289 11.18783186\n",
      " 18.1542991   1.98371708  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 3)  new position  (5, 3)\n",
      "0.0 0 32  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -0.69903417  10.97847514 -16.97540658   6.91061407 -15.12392938\n",
      "  14.15022956   9.9191946    1.97872133   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 3)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 3)  new position  (4, 4)\n",
      "0.0 0 33  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-18.02926715 -14.57349108 -11.76226996  -7.52284214 -18.98018788\n",
      " -16.21122549 -15.0349483   -4.51504588   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 4)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 4)  new position  (5, 3)\n",
      "0.0 0 34  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -0.69903417  10.97847514 -16.97540658   6.91061407 -15.12392938\n",
      "  14.15022956   9.9191946    1.97872133   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 3)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.         -10.50612962 -10.18128422  19.64073906   5.43184527\n",
      " -14.49799963  -0.5561602  -17.54645991   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 3)  new position  (4, 4)\n",
      "0.0 0 35  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-5.7139981  -6.25202065  2.18488209  0.         -2.25440048 20.34023795\n",
      " 13.79532742 14.38220477  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 3)  new position  (4, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-17.88147765   0.880185   -10.19239195 -29.73459655  -3.68318344\n",
      " -30.57896604 -14.99183505  -4.38387667   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 4)  new position  (3, 3)\n",
      "0.0 0 36  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.04602745   4.27966472  -1.12515927 -16.31865737   0.\n",
      "  20.35416974  -7.81925255  14.28440339   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 3)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  6.64694514  12.41742719 -14.08772312   0.          10.37060915\n",
      " -11.364505     3.74701868  -9.81673696   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 2)  new position  (3, 3)\n",
      "0.0 0 37  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 2.70475348  0.         -3.8738046   6.0630691  -1.48127289 11.18783186\n",
      " 18.1542991   1.98371708  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 3)  new position  (4, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.04602745   4.27966472  -1.12515927 -16.31865737   0.\n",
      "  20.35416974  -7.81925255  14.28440339   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 3)  new position  (4, 3)\n",
      "0.0 0 38  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  6.64694514  12.41742719 -14.08772312   0.          10.37060915\n",
      " -11.364505     3.74701868  -9.81673696   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 2)  new position  (4, 1)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-2.94035331 -6.86174902  2.50325758  1.39144159 -9.35797826 -0.09943269\n",
      " -5.62522288 18.56927906  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 3)  new position  (5, 3)\n",
      "0.0 0 39  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -6.21009091   1.54393842   7.26232499  -8.38200276   9.72958481\n",
      " -18.30100282   5.08529938  -8.68271308   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 1)  new position  (3, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -0.33183915   7.27716333  -9.83167073  -9.64704583  12.7041955\n",
      "   1.4552324  -17.46778849  27.01952637   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 3)  new position  (5, 4)\n",
      "0.0 0 40  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  4.78283262   9.7341236  -10.1199061   12.04763172  17.06204777\n",
      "  -8.2639297    3.07575223  -8.60477836   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 2)  new position  (3, 1)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 4)  new position  (4, 5)\n",
      "0.0 0 41  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 1)  new position  (2, 1)\n",
      "0.0 0 42  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 1)  new position  (3, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 6)  new position  (5, 6)\n",
      "0.0 0 43  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 1)  new position  (4, 0)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         10.6286564  18.21014817  0.          0.          0.\n",
      " 10.65163663  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 6)  new position  (4, 6)\n",
      "0.0 0 44  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 6)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.78690545  6.15444352  3.85395187 -0.15385486 -5.38010634 15.5076787\n",
      " 14.73589801 13.31404824  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 0)  new position  (5, 1)\n",
      "0.0 0 45  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 5)  new position  (3, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 1)  new position  (4, 2)\n",
      "0.0 0 46  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 2)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.90260846 -56.34286608 -32.56947316   0.         -36.81589887\n",
      "   0.         -46.41929819   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 6)  new position  (4, 5)\n",
      "0.0 0 47  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-36.41997407 -41.33370537 -23.09020636 -21.29708967 -28.78506316\n",
      " -29.17297297 -20.23437463 -16.56574828   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 3)  new position  (3, 2)\n",
      "0.0 0 48  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 2)  new position  (2, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 6)  new position  (4, 5)\n",
      "0.0 0 49  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  8.09781387   0.83212954 -17.68106013   3.0704939    0.62757579\n",
      "  -2.66026302  18.20214052 -11.81420935   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 3)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 5)  new position  (4, 6)\n",
      "0.0 0 50  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 6)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.46722491 -1.77699393 -4.893463    0.         20.07332682 40.25356017\n",
      " 21.03019205  7.96800248  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 4)  new position  (0, 4)\n",
      "0.0 0 51  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 4)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 5)  new position  (5, 6)\n",
      "0.0 0 52  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 4)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         10.6286564  18.21014817  0.          0.          0.\n",
      " 10.65163663  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 6)  new position  (4, 5)\n",
      "0.0 0 53  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 5)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 5)  new position  (3, 6)\n",
      "0.0 0 54  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -8.80937056 -22.2289585  -15.17276949   0.         -20.87199837\n",
      "   0.         -19.1627146    0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 6)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.46722491 -1.77699393 -4.893463    0.         20.07332682 40.25356017\n",
      " 21.03019205  7.96800248  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (1, 4)  new position  (1, 3)\n",
      "0.0 0 55  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 3)  new position  (0, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 5)  new position  (3, 5)\n",
      "0.0 0 56  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 5)  new position  (2, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 3)  new position  (1, 3)\n",
      "0.0 0 57  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 3)  new position  (2, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-0.4139219   0.          9.4992501   0.          7.46749503  0.\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 6)  new position  (3, 5)\n",
      "0.0 0 58  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 5)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 2)  new position  (1, 1)\n",
      "0.0 0 59  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 1)  new position  (2, 2)\n",
      "0.0 0 60  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 2)  new position  (3, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 6)  new position  (3, 6)\n",
      "0.0 0 61  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.90260846 -56.34286608 -32.56947316   0.         -36.81589887\n",
      "   0.         -46.41929819   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 6)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 2)  new position  (2, 3)\n",
      "0.0 0 62  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-29.00835659 -19.30451589 -18.82800739 -19.90379154 -17.92926811\n",
      " -17.29620314 -17.91343555 -19.69922061   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 5)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 10.00485352  -9.50064392  15.91464858   2.30950735   4.99570472\n",
      "  13.66669149 -15.06031416  18.82974988   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 3)  new position  (1, 2)\n",
      "0.0 0 63  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.66282809  -4.26618692   4.0766115  -11.41362679  10.69348545\n",
      "  11.03964625   1.67209256  -9.43224976   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 2)  new position  (2, 2)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 5)  new position  (2, 5)\n",
      "0.0 0 64  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-11.5691717    0.          18.83737322  12.54358916   7.61177462\n",
      " -18.91666582  -3.0443907    0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 5)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 2)  new position  (3, 2)\n",
      "0.0 0 65  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 2)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-26.63683087  -4.37760655 -27.73906957 -13.96902274 -21.19874428\n",
      " -39.66345661 -17.97086088 -26.81846491   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 5)  new position  (4, 4)\n",
      "0.0 0 66  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 14.68095424 -11.51213106   1.14267632 -10.96198781  -0.24203141\n",
      "   0.           3.44289796  14.02084727   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 3)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-27.53328613   0.         -19.0389761  -24.88554245 -20.94715922\n",
      " -19.3283059  -20.02868438 -23.70559386   0.        ]\n",
      "max index list:  [1, 8]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 4)  new position  (4, 5)\n",
      "0.0 0 67  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-10.89371283 -26.70875646 -27.78262978 -30.03181479 -16.68090742\n",
      " -14.16506571   0.         -19.50882962   0.        ]\n",
      "max index list:  [6, 8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-24.66951487 -15.24824514 -16.50934522  -9.13355097 -14.84115793\n",
      " -17.25228754 -17.00789199 -14.15282963   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 4)  new position  (4, 3)\n",
      "0.0 0 68  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 3)  new position  (4, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  1\n",
      "index max:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 6)  new position  (3, 6)\n",
      "0.0 0 69  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 2)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.90260846 -56.34286608 -32.56947316   0.         -36.81589887\n",
      "   0.         -46.41929819   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 6)  new position  (2, 6)\n",
      "0.0 0 70  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-0.4139219   0.          9.4992501   0.          7.46749503  0.\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  2\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 6)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -6.81920507   7.84411698  13.76058375 -15.08423836   0.57149775\n",
      " -14.54107445  -7.01343821  -4.54320962   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 3)  new position  (3, 4)\n",
      "0.0 0 71  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-28.48672238  -9.48628113 -19.88386132   0.         -40.01108001\n",
      " -23.89973148 -24.39427121 -13.89609574   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 4)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [  0.         -12.3811205   -9.40370633 -15.19235861  -4.62930503\n",
      " -27.87064383 -24.74406087 -24.28939604   0.        ]\n",
      "max index list:  [0, 8]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 5)  new position  (3, 6)\n",
      "0.0 0 72  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-14.15652035 -30.43809667  -7.74817115  -7.1790338  -15.0608865\n",
      "  -2.84107744 -10.75968598   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 5)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-34.14321467 -34.79930598   0.           0.         -18.83467664\n",
      "   0.         -22.28653701   0.           0.        ]\n",
      "max index list:  [2, 3, 5, 7, 8]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 6)  new position  (4, 6)\n",
      "0.0 0 73  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-32.7115933  -24.04123785 -11.44405302   0.         -37.50508119\n",
      "   0.           0.           0.           0.        ]\n",
      "max index list:  [3, 5, 6, 7, 8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 6)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [  0.         -12.3811205   -9.40370633 -15.19235861  -4.62930503\n",
      " -27.87064383 -24.74406087 -24.28939604   0.        ]\n",
      "max index list:  [0, 8]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 5)  new position  (2, 4)\n",
      "0.0 0 74  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-14.09313416 -19.26464498 -32.01291441 -10.28308141 -13.94422128\n",
      " -21.00181987 -23.03858832 -21.70207853   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 5)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -7.07803166 -3.04000843 28.11194832 -2.61605162  3.03559342\n",
      " 15.67012642  0.          0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 4)  new position  (1, 4)\n",
      "0.0 0 75  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [38.92430394  2.57727315 22.26500343  0.         18.563745   16.90336628\n",
      " 10.84758939  9.73024159  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 4)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 10.9874225   -0.17279798   0.81836645 -10.81641669 -19.69499441\n",
      "  -4.05652437 -21.03486858   0.           0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 4)  new position  (4, 5)\n",
      "0.0 0 76  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [  3.49026178   0.          -3.66001252 -20.84501149  13.70157179\n",
      "   8.53924215  -7.82215195   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 5)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-14.09313416 -19.26464498 -32.01291441 -10.28308141 -13.94422128\n",
      " -21.00181987 -23.03858832 -21.70207853   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 5)  new position  (4, 6)\n",
      "0.0 0 77  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -0.02120569  -8.776017   -12.65271074   0.         -12.78418366\n",
      "   0.         -17.09944422   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 6)  new position  (5, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 4)  new position  (3, 5)\n",
      "0.0 0 78  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           1.98564879 -22.90383733   0.           0.\n",
      "   0.          14.62621974   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 6)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [ -3.57958327  -7.95767713 -10.42785984  -8.48034222 -24.64851681\n",
      "   0.          -9.32871276 -17.8616374    0.        ]\n",
      "max index list:  [5, 8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 5)  new position  (4, 4)\n",
      "0.0 0 79  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-10.66468384 -23.43285918 -37.65857397   0.         -20.88683993\n",
      "   0.         -25.29700878   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 6)  new position  (5, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-10.32831726  -3.87896847 -12.77090245 -11.71303438  -6.90648059\n",
      "   4.08226144  -8.59052546  -8.77122859   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 4)  new position  (3, 4)\n",
      "0.0 0 80  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.         -13.90377565  -2.61367181   0.           0.\n",
      "   0.          -1.69106057   0.           0.        ]\n",
      "max index list:  [0, 3, 4, 5, 7, 8]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 6)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-24.66951487 -15.24824514 -16.50934522  -9.13355097 -14.84115793\n",
      " -17.25228754 -17.00789199 -14.15282963   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 4)  new position  (4, 5)\n",
      "0.0 0 81  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-28.1902841  -29.07668056   0.           0.         -15.13735052\n",
      "   0.         -27.60369597   0.           0.        ]\n",
      "max index list:  [2, 3, 5, 7, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 6)  new position  (3, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-14.15652035 -30.43809667  -7.74817115  -7.1790338  -15.0608865\n",
      "  -2.84107744 -10.75968598   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 5)  new position  (5, 5)\n",
      "0.0 0 82  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-23.9811644  -17.27244738 -12.53586039   0.         -32.11027945\n",
      "   0.         -15.42454112   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 6)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.          -3.33208553   4.54153888 -10.32647348 -11.97515439\n",
      "   0.          -9.68363387   0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 5)  new position  (4, 5)\n",
      "0.0 0 83  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-28.1902841  -29.07668056   0.           0.         -15.13735052\n",
      "   0.         -27.60369597   0.           0.        ]\n",
      "max index list:  [2, 3, 5, 7, 8]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 6)  new position  (5, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-14.52166828  -6.63556013  -4.76831629  -5.32955296  -6.27319405\n",
      "   0.          -7.56718355 -13.83162317   0.        ]\n",
      "max index list:  [5, 8]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 5)  new position  (4, 4)\n",
      "0.0 0 84  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-10.32831726  -3.87896847 -12.77090245 -11.71303438  -6.90648059\n",
      "   4.08226144  -8.59052546  -8.77122859   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 4)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         10.6286564  18.21014817  0.          0.          0.\n",
      " 10.65163663  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action I choose is  DOWN\n",
      "Old location  (5, 6)  new position  (4, 6)\n",
      "0.0 0 85  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 3)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-10.66468384 -23.43285918 -37.65857397   0.         -20.88683993\n",
      "   0.         -25.29700878   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 6)  new position  (4, 5)\n",
      "0.0 0 86  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-23.81214423 -26.3749104  -11.32253052   0.         -21.80942505\n",
      " -20.49631602 -16.95509938 -38.87111619   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 4)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [  0.          -6.31002489 -41.32765204 -22.99963205   1.83844232\n",
      " -13.85641976 -12.76173789 -17.01945918   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 5)  new position  (3, 6)\n",
      "0.0 0 87  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.          -7.97299405 -25.99499445  -4.25308313  -4.23851327\n",
      "   0.          -0.32504008  -4.52489332   0.        ]\n",
      "max index list:  [0, 5, 8]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 5)  new position  (5, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -7.97527942 -14.77943106  -1.90444502   0.          -8.46306274\n",
      "   0.          -0.74936342   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 6)  new position  (3, 5)\n",
      "0.0 0 88  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           1.98564879 -22.90383733   0.           0.\n",
      "   0.          14.62621974   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 6)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-29.97737159 -13.9275342  -12.67140677 -11.69837811 -22.29887088\n",
      " -21.82892052  -7.67597392  -1.31970687   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 5)  new position  (4, 5)\n",
      "0.0 0 89  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [  0.          -6.31002489 -41.32765204 -22.99963205   1.83844232\n",
      " -13.85641976 -12.76173789 -17.01945918   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 5)  new position  (3, 6)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.          -7.97299405 -25.99499445  -4.25308313  -4.23851327\n",
      "   0.          -0.32504008  -4.52489332   0.        ]\n",
      "max index list:  [0, 5, 8]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 5)  new position  (4, 4)\n",
      "0.0 0 90  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-18.03792444 -17.45438209 -31.15009638   0.         -16.46353855\n",
      "   0.         -22.46702313   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 6)  new position  (2, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -7.48242786  -7.7347298   -9.10976005   2.22670331 -10.21170146\n",
      " -18.33625213   1.59988107 -15.92580455   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 4)  new position  (3, 5)\n",
      "0.0 0 91  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-10.11049032  -0.34296771 -15.97229256  -6.16064078 -19.20695972\n",
      "  -6.39264541 -10.49234787   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 5)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-0.57924412  0.          0.          0.          0.17767572  0.\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 6)  new position  (3, 5)\n",
      "0.0 0 92  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.         -10.89968313   5.22817095   3.55044255\n",
      "  17.77057289  -8.96065386   0.           0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  0\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 5)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-14.19090447  -2.23876911 -11.282227   -10.01623987 -29.37494326\n",
      " -25.49260197   0.           2.03844889   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 5)  new position  (2, 5)\n",
      "0.0 0 93  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 1.1844314   0.          0.         14.54319377  2.60921642 23.17764467\n",
      " 19.60673339  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 5)  new position  (3, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 12.85965655  11.42023013   5.17323832  -0.23985413  -2.25423476\n",
      "  -8.84665217 -23.30340499   0.           0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 4)  new position  (2, 5)\n",
      "0.0 0 94  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-16.11323606  -1.20729987  -3.24015816   0.         -10.32446796\n",
      "   0.           0.           0.           0.        ]\n",
      "max index list:  [3, 5, 6, 7, 8]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 6)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [  3.49026178   0.          -3.66001252 -20.84501149  13.70157179\n",
      "   8.53924215  -7.82215195   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 5)  new position  (2, 6)\n",
      "0.0 0 95  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -0.81497065 -19.01452906  -5.24851376 -17.349091    -1.75359146\n",
      " -15.49101212  -7.36107726  -2.30725071   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 5)  new position  (3, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 0.          0.          1.89751681  0.         -3.85694006  0.\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 6)  new position  (3, 5)\n",
      "0.0 0 96  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-26.59097842 -10.54374999  -8.85508476   0.         -27.46232785\n",
      " -11.9005627  -16.53958363 -11.68561266   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [  0.         -10.38334268 -30.20918853   0.         -21.50198894\n",
      "   0.          -2.09644203   0.           0.        ]\n",
      "max index list:  [0, 3, 5, 7, 8]\n",
      "max_action:  6\n",
      "index max:  3\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 6)  new position  (2, 5)\n",
      "0.0 0 97  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 3.02184815  0.         20.38296861 11.75200511 -7.1613998  -4.73398704\n",
      " -3.19679941  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 5)  new position  (3, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -6.8113059    0.         -21.07669981   0.         -16.50549553\n",
      "   0.         -15.3765337    0.           0.        ]\n",
      "max index list:  [1, 3, 5, 7, 8]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 6)  new position  (5, 6)\n",
      "0.0 0 98  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -7.97527942 -14.77943106  -1.90444502   0.          -8.46306274\n",
      "   0.          -0.74936342   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 6)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.          2.05839992 -4.76505137  0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 6)  new position  (4, 6)\n",
      "0.0 0 99  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-22.46219872 -21.10799119 -10.50727119   0.         -30.54636097\n",
      " -14.54521699 -20.87170605 -23.70244196   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 5)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-31.74745469 -15.58509501 -23.42303773   0.         -13.75112548\n",
      "   0.         -29.17189447   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 6)  new position  (3, 6)\n",
      "0.0 0 100  Episode:  0\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating trained mc agent with vision range  2\n",
      "creating trained mc agent with vision range  2\n",
      "Monte Carlo Episode  1\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "I havent seen this before, picking randomly\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 8)  new position  (2, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "I havent seen this before, picking randomly\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 9)  new position  (5, 9)\n",
      "0.0 0 1  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 5.29326101 14.0343668   0.         12.76521282  0.         11.86681313\n",
      "  0.         23.36407537  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 8)  new position  (2, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 0.57347029 -5.70525091 -0.08112917  1.63161255  0.38035348  6.68752561\n",
      " 17.30991094 -0.20297002  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 9)  new position  (4, 8)\n",
      "0.0 0 2  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.17279432  -5.48465481 -15.007718     4.83459011   7.9215505\n",
      "   3.70177794   5.64107757  24.11817614   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 9)  new position  (1, 8)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-14.89427287 -22.08880153   0.           3.64553484   0.\n",
      "  -1.30884592   0.         -31.87859896   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 8)  new position  (5, 8)\n",
      "0.0 0 3  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [29.11445124 22.74569532  0.         13.97051409  0.         27.76971107\n",
      " 26.97358949  2.41201194  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 8)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [24.27211104  4.74810304  0.         -1.5448469   0.         19.08709095\n",
      "  0.         20.93868559  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 8)  new position  (6, 8)\n",
      "0.0 0 4  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 7)  new position  (0, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [13.82148163 25.37561696  0.          8.2209153  18.64746376  7.86536258\n",
      "  0.         39.5350921   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 8)  new position  (7, 7)\n",
      "0.0 0 5  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.          16.9698146    6.44973079 -21.96511259   0.\n",
      "   0.          14.43365271  -6.44731182   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 6)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 22.28442192   0.         -10.43970862  19.80406284  23.41523361\n",
      "  -1.04973973   0.           0.40251036   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 7)  new position  (7, 8)\n",
      "0.0 0 6  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 0.0524524  -1.07588843 13.84614493 -5.96978828 10.63061446 11.30196963\n",
      " -3.86776533 11.58194732  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 6)  new position  (8, 7)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.95079847   2.32125026   3.02339452 -15.72914884   0.\n",
      "  -1.74845006   0.           8.41509627   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 8)  new position  (8, 9)\n",
      "0.0 0 7  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -7.27583255   5.37746227  -9.6949701   -1.3274837    5.90445295\n",
      "  -9.33393822 -10.37750032  -6.74506934   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 9)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-10.07688183 -28.80657813  -8.45533959  -0.51490706  -5.02334143\n",
      "  27.05083553 -18.10199168  -4.02836738   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 7)  new position  (9, 7)\n",
      "0.0 0 8  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 0)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -8.87358074 -15.49102038   2.13494745  -3.62945036  -2.43529659\n",
      "  -2.14232725  -9.99448069   0.04097482   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 7)  new position  (0, 6)\n",
      "0.0 0 9  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.          16.9698146    6.44973079 -21.96511259   0.\n",
      "   0.          14.43365271  -6.44731182   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  3\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 6)  new position  (9, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 9)  new position  (8, 9)\n",
      "0.0 0 10  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 9)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 5)  new position  (9, 6)\n",
      "0.0 0 11  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 6)  new position  (8, 7)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 0)  new position  (8, 0)\n",
      "0.0 0 12  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-10.07688183 -28.80657813  -8.45533959  -0.51490706  -5.02334143\n",
      "  27.05083553 -18.10199168  -4.02836738   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 7)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 0)  new position  (7, 0)\n",
      "0.0 0 13  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 6)  new position  (8, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 0)  new position  (8, 9)\n",
      "0.0 0 14  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 5)  new position  (9, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 9)  new position  (7, 0)\n",
      "0.0 0 15  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 5)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 0)  new position  (8, 9)\n",
      "0.0 0 16  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 5)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 9)  new position  (7, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 17  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 0)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 4)  new position  (8, 5)\n",
      "0.0 0 18  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 5)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 0)  new position  (6, 9)\n",
      "0.0 0 19  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -4.24702509 -14.13072768   0.9504665    0.83209999  -1.26152662\n",
      "  12.25639219  -7.01730338   9.02650008   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 9)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 3.6359424   4.66838662  3.61232695  0.69975406 -6.82693662 -1.95821441\n",
      " 21.32860793  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 4)  new position  (6, 4)\n",
      "0.0 0 20  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 4)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 0)  new position  (7, 0)\n",
      "0.0 0 21  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 3)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 0)  new position  (6, 0)\n",
      "0.0 0 22  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 4)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 0)  new position  (5, 1)\n",
      "0.0 0 23  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 1)  new position  (4, 2)\n",
      "0.0 0 24  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 2)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 6)  new position  (4, 5)\n",
      "0.0 0 25  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 2)  new position  (5, 3)\n",
      "0.0 0 26  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 6)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [21.41988973 15.2163434  14.9981694   3.8724467  16.65293183 22.21639471\n",
      " -0.54842173  0.59579406  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 3)  new position  (5, 4)\n",
      "0.0 0 27  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.24411559 -27.21679282 -20.80968719 -17.61812986 -21.83110905\n",
      " -14.74504735 -17.35307261 -22.3616452    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 5)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 4)  new position  (4, 3)\n",
      "0.0 0 28  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-25.00625208 -11.53066983 -17.27307747   4.39772647  14.28575499\n",
      "   7.20284293  12.66790755  10.94206454   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 3)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -7.07803166 -3.04000843 28.11194832 -2.61605162  3.03559342\n",
      " 15.67012642  0.          0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 4)  new position  (2, 5)\n",
      "0.0 0 29  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 4.7859348   0.         13.14632199  0.19711265  0.          6.04513649\n",
      "  4.73159292  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  3\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 5)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -2.96363687 -23.93053976 -13.84942238   1.94914906  -2.88028815\n",
      " -18.50708366 -13.78204445  -9.10087706   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 4)  new position  (3, 5)\n",
      "0.0 0 30  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [ -5.41940087 -16.14096363 -31.33982875  -7.09992264  -9.42600655\n",
      " -18.60961121 -11.45711969 -14.80910476   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.46722491 -1.77699393 -4.893463    0.         20.07332682 40.25356017\n",
      " 21.03019205  7.96800248  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 4)  new position  (2, 4)\n",
      "0.0 0 31  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [  8.74036753 -10.916433    -4.90416082  -0.93119271 -10.500926\n",
      "   0.05632198  -2.60929768   0.           0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 4)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-32.7115933  -24.04123785 -11.44405302   0.         -37.50508119\n",
      "   0.           0.           0.           0.        ]\n",
      "max index list:  [3, 5, 6, 7, 8]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 6)  new position  (5, 6)\n",
      "0.0 0 32  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           1.98564879 -22.90383733   0.           0.\n",
      "   0.          14.62621974   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 6)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [ -3.57958327  -7.95767713 -10.42785984  -8.48034222 -24.64851681\n",
      "   0.          -9.32871276 -17.8616374    0.        ]\n",
      "max index list:  [5, 8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 5)  new position  (3, 4)\n",
      "0.0 0 33  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-31.74745469 -15.58509501 -23.42303773   0.         -13.75112548\n",
      "   0.         -29.17189447   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 6)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-28.48672238  -9.48628113 -19.88386132   0.         -40.01108001\n",
      " -23.89973148 -24.39427121 -13.89609574   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 4)  new position  (4, 5)\n",
      "0.0 0 34  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [  0.         -12.3811205   -9.40370633 -15.19235861  -4.62930503\n",
      " -27.87064383 -24.74406087 -24.28939604   0.        ]\n",
      "max index list:  [0, 8]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-22.46219872 -21.10799119 -10.50727119   0.         -30.54636097\n",
      " -14.54521699 -20.87170605 -23.70244196   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 5)  new position  (3, 5)\n",
      "0.0 0 35  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-32.7115933  -24.04123785 -11.44405302   0.         -37.50508119\n",
      "   0.           0.           0.           0.        ]\n",
      "max index list:  [3, 5, 6, 7, 8]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 6)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [  0.         -12.3811205   -9.40370633 -15.19235861  -4.62930503\n",
      " -27.87064383 -24.74406087 -24.28939604   0.        ]\n",
      "max index list:  [0, 8]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 5)  new position  (4, 4)\n",
      "0.0 0 36  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-10.72162337 -14.92424716   0.          -8.81197591 -25.05303459\n",
      " -20.43723679 -17.94751419 -33.86963987   0.        ]\n",
      "max index list:  [2, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 5)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.86394552 -21.54636477 -18.83266217  -4.70837444 -26.92379372\n",
      "  -3.70878197 -32.78047131   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 4)  new position  (5, 5)\n",
      "0.0 0 37  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-29.97737159 -13.9275342  -12.67140677 -11.69837811 -22.29887088\n",
      " -21.82892052  -7.67597392  -1.31970687   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 5)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.         -21.06931044   5.20194887 -14.86784924   6.28204802\n",
      "   0.           2.61238427 -20.30270254   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 5)  new position  (4, 5)\n",
      "0.0 0 38  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-15.71526954  -8.17935001 -27.75188721 -30.52381955 -13.23280049\n",
      "   0.         -27.34079796 -27.36101321   0.        ]\n",
      "max index list:  [5, 8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 4)  new position  (2, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-29.00835659 -19.30451589 -18.82800739 -19.90379154 -17.92926811\n",
      " -17.29620314 -17.91343555 -19.69922061   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 5)  new position  (4, 4)\n",
      "0.0 0 39  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -8.57603141 -18.48591812 -25.98047554 -18.97721    -18.10258032\n",
      " -32.47280141 -19.9423762  -12.32768275   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 4)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 10.00485352  -9.50064392  15.91464858   2.30950735   4.99570472\n",
      "  13.66669149 -15.06031416  18.82974988   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 3)  new position  (1, 4)\n",
      "0.0 0 40  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [26.83542237  8.52167274 14.56363408  0.         16.2460588  13.91929606\n",
      " 39.40293621  9.50981246  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 4)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-14.40011852   0.          -8.75276058  -0.70130134 -23.25422688\n",
      " -18.56296923 -15.57673094 -12.03835661   0.        ]\n",
      "max index list:  [1, 8]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 5)  new position  (4, 6)\n",
      "0.0 0 41  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 3.02184815  0.         20.38296861 11.75200511 -7.1613998  -4.73398704\n",
      " -3.19679941  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 5)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 6)  new position  (4, 5)\n",
      "0.0 0 42  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 5)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.46722491 -1.77699393 -4.893463    0.         20.07332682 40.25356017\n",
      " 21.03019205  7.96800248  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 4)  new position  (0, 4)\n",
      "0.0 0 43  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 4)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 4)  new position  (1, 3)\n",
      "0.0 0 44  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 3)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 5)  new position  (4, 4)\n",
      "0.0 0 45  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.46722491 -1.77699393 -4.893463    0.         20.07332682 40.25356017\n",
      " 21.03019205  7.96800248  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 4)  new position  (2, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -8.57603141 -18.48591812 -25.98047554 -18.97721    -18.10258032\n",
      " -32.47280141 -19.9423762  -12.32768275   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 4)  new position  (4, 3)\n",
      "0.0 0 46  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-26.64601138   5.41808831   6.96831616   9.35134993  -6.8927583\n",
      "  -9.23700198   3.57646358   4.42085433   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 3)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-25.00625208 -11.53066983 -17.27307747   4.39772647  14.28575499\n",
      "   7.20284293  12.66790755  10.94206454   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 3)  new position  (3, 3)\n",
      "0.0 0 47  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.69931406   2.8824375  -13.08688145  15.20064426   4.27143766\n",
      "  16.90044104   1.48640802   0.           0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 3)  new position  (2, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-17.07863261 -14.06661703  19.34577755  -0.78133444   8.55704494\n",
      "  14.09215619   5.80485442   0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 4)  new position  (3, 4)\n",
      "0.0 0 48  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 4.95882087 10.01934036 -5.80795586 10.54475923 -9.36280454 -7.6483611\n",
      " 14.48332777  2.33004856  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 2)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-13.9214174   -2.5095287  -12.00970375 -27.17812546 -29.43184089\n",
      " -13.97100346  -2.47066491 -24.71836013   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 4)  new position  (2, 3)\n",
      "0.0 0 49  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.          -1.6789731   -7.76753337   8.26994735  -2.87776837\n",
      "  11.17766415 -10.3029383  -10.22268706   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (1, 3)  new position  (1, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -6.07882948  -9.4139596  -17.11607087  14.73174444 -20.35216588\n",
      "   3.50945527   0.           7.75088935   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 3)  new position  (3, 4)\n",
      "0.0 0 50  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 4.45768109 18.29926632  9.33596106 20.16527556  3.18771392 -2.52016552\n",
      " 12.38980025 -3.59747947  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 2)  new position  (2, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-19.03434795 -31.71623119 -13.78826607 -11.72813206 -19.01241558\n",
      " -34.34552831   6.47482686  -0.28835812   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 4)  new position  (3, 5)\n",
      "0.0 0 51  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 2)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 5)  new position  (2, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 52  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 1)  new position  (2, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 4)  new position  (2, 3)\n",
      "0.0 0 53  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  1.52125769   1.84936603   3.08586823  -3.02703375   3.01876459\n",
      "   7.41449825   2.60761054 -12.12537421   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 3)  new position  (3, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.72673634 -21.08429066   3.87476026  -9.42704352   3.47226748\n",
      "   0.         -10.20135672  10.93868021   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 1)  new position  (1, 2)\n",
      "0.0 0 54  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -6.16171954 -18.08612901   9.39520097  -3.84755653  -8.68209865\n",
      "   8.17624696  -1.70821784 -13.69268646   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 2)  new position  (2, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-12.13332075   0.          -3.97006123   0.68055633   3.31833028\n",
      "  -7.36893741  -4.07909027  16.03587251   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 2)  new position  (2, 3)\n",
      "0.0 0 55  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 2.28932152  9.14795468  0.          4.9148306   5.58631438  6.29699507\n",
      " -9.46505364 -9.42255591  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 3)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -2.1635692   -8.20310696  16.76614083 -15.83427223 -15.30527582\n",
      "  -8.57789565   2.74032922  -7.07017165   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 2)  new position  (3, 3)\n",
      "0.0 0 56  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 13.53429111 -10.76247106   7.78551541   4.87278436   2.82254065\n",
      "  -3.46033011 -10.2759357  -13.99181782   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 3)  new position  (3, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [37.53858372 -2.31160639 16.69134914  0.          1.38016379 14.35428276\n",
      " -7.53832916 26.06787607  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 4)  new position  (0, 4)\n",
      "0.0 0 57  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 2)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 4)  new position  (9, 5)\n",
      "0.0 0 58  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 3)  new position  (4, 2)\n",
      "0.0 0 59  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 2)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 6)  new position  (0, 5)\n",
      "0.0 0 60  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 2)  new position  (4, 2)\n",
      "0.0 0 61  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 2)  new position  (5, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 6)  new position  (0, 6)\n",
      "0.0 0 62  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.          16.9698146    6.44973079 -21.96511259   0.\n",
      "   0.          14.43365271  -6.44731182   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 6)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.00746759 -11.1412527  -19.62385086  -1.01596757  -1.60346104\n",
      "  15.51659352 -10.01204177  -5.37875616   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 3)  new position  (6, 2)\n",
      "0.0 0 63  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 2)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 5)  new position  (9, 5)\n",
      "0.0 0 64  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-24.65148099 -13.01934049  -3.07825177 -17.86824472   4.58633534\n",
      "  -4.07755584 -12.95474131 -19.48075669   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 5)  new position  (0, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 3)  new position  (8, 4)\n",
      "0.0 0 65  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         12.36849445 17.12043516 -0.38163343  0.          0.\n",
      " -7.51240165 14.31483331  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 6)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-13.0681584    3.4065754  -10.94664938 -11.83367233  -8.77066434\n",
      "  -9.06012927  -9.18046516  -3.46598799   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 4)  new position  (8, 5)\n",
      "0.0 0 66  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.78517704 -15.74471065  -6.23493326   1.12423282\n",
      "   0.          -2.5360555   -2.67952648   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 5)  new position  (0, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-14.15509638  -8.73828762 -15.76007607  -5.85867444  11.63335104\n",
      " -13.04174164  -3.24717887  -5.77747346   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 5)  new position  (9, 4)\n",
      "0.0 0 67  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  8.60790371   0.          -4.39478635   1.3070147   10.60197959\n",
      "   0.         -10.76979705  -4.40664589   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 4)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-22.76836925 -23.72163436  -8.59212353  -0.21755103  -0.8071996\n",
      " -13.84831876 -16.06665369  -4.91640802   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 4)  new position  (8, 3)\n",
      "0.0 0 68  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 3)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [14.56721948 14.68014692 24.00167985  0.          5.51894001 -0.39876764\n",
      " -9.37577928  2.66304527  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 4)  new position  (0, 4)\n",
      "0.0 0 69  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-16.41652707 -12.45919002  -2.8287721   -3.45216192   3.94001255\n",
      "   0.           2.42106922 -15.85016133   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 3)  new position  (8, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 27.87950273 -10.07152487  -4.31964626   9.92870826   1.36855177\n",
      "   0.          -4.21717058 -21.97818396   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 4)  new position  (0, 3)\n",
      "0.0 0 70  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-21.28171232 -13.66283044 -18.67371514  -9.71821422  -3.72576418\n",
      "   0.437106   -30.2606487  -29.50067353   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 3)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 4)  new position  (7, 3)\n",
      "0.0 0 71  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 3)  new position  (8, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (1, 3)  new position  (1, 2)\n",
      "0.0 0 72  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.66282809  -4.26618692   4.0766115  -11.41362679  10.69348545\n",
      "  11.03964625   1.67209256  -9.43224976   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 2)  new position  (0, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 4.3737139  -5.86813193 -6.96793671  6.06898547 12.20219373 -2.61784184\n",
      "  0.35790479  9.581756    0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 2)  new position  (7, 2)\n",
      "0.0 0 73  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 2)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 1)  new position  (9, 1)\n",
      "0.0 0 74  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 1)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 1)  new position  (9, 2)\n",
      "0.0 0 75  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 2)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.04744736  -5.64635074   3.66599274  12.73537913 -25.13640359\n",
      "   3.20590757  -7.84987033  -9.80192907   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 2)  new position  (0, 1)\n",
      "0.0 0 76  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 1)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 3)  new position  (4, 4)\n",
      "0.0 0 77  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 4)  new position  (5, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 1)  new position  (0, 0)\n",
      "0.0 0 78  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 0)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 4)  new position  (4, 4)\n",
      "0.0 0 79  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 0)  new position  (0, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 4)  new position  (4, 3)\n",
      "0.0 0 80  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 9)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 3)  new position  (3, 4)\n",
      "0.0 0 81  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.82087061 -36.97603648 -40.37821211 -63.24831962 -35.21881822\n",
      " -46.65772308 -36.22761687 -27.25788312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 4)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 9)  new position  (0, 9)\n",
      "0.0 0 82  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 5)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 9)  new position  (9, 9)\n",
      "0.0 0 83  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 9)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.82087061 -36.97603648 -40.37821211 -63.24831962 -35.21881822\n",
      " -46.65772308 -36.22761687 -27.25788312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 4)  new position  (3, 5)\n",
      "0.0 0 84  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 0)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 5)  new position  (2, 4)\n",
      "0.0 0 85  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 4)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 9)  new position  (9, 0)\n",
      "0.0 0 86  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 0)  new position  (9, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.78235861 -20.29112697   4.33069698   8.10174539  -2.16990499\n",
      "   8.68192784  -7.44704503  13.63322174   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 3)  new position  (2, 4)\n",
      "0.0 0 87  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 1)  new position  (8, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 4)  new position  (3, 5)\n",
      "0.0 0 88  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 5)  new position  (3, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 24.28279917  -3.2503315   -3.4938431    7.46755294 -10.64272157\n",
      "  -9.66022703   0.34289003   0.44213197   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 1)  new position  (8, 0)\n",
      "0.0 0 89  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 0)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.90260846 -56.34286608 -32.56947316   0.         -36.81589887\n",
      "   0.         -46.41929819   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 6)  new position  (4, 6)\n",
      "0.0 0 90  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old location  (9, 0)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 6)  new position  (3, 5)\n",
      "0.0 0 91  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 5)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 9)  new position  (7, 9)\n",
      "0.0 0 92  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 9)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 4)  new position  (2, 3)\n",
      "0.0 0 93  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 0)  new position  (5, 9)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -4.99900466  -0.94666676  -9.07740695  -7.11623038  -5.15010247\n",
      "  -8.596438   -12.97535374   5.53657469   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 3)  new position  (3, 4)\n",
      "0.0 0 94  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.82087061 -36.97603648 -40.37821211 -63.24831962 -35.21881822\n",
      " -46.65772308 -36.22761687 -27.25788312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 4)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-6.66359182  6.41168705 -3.88023906 14.53598089  6.82718405  4.08742847\n",
      " -7.17129168  5.8474412   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 9)  new position  (5, 0)\n",
      "0.0 0 95  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 3)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 0)  new position  (5, 9)\n",
      "0.0 0 96  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-6.66359182  6.41168705 -3.88023906 14.53598089  6.82718405  4.08742847\n",
      " -7.17129168  5.8474412   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 9)  new position  (4, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 4)  new position  (3, 4)\n",
      "0.0 0 97  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.82087061 -36.97603648 -40.37821211 -63.24831962 -35.21881822\n",
      " -46.65772308 -36.22761687 -27.25788312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 4)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 9)  new position  (5, 9)\n",
      "0.0 0 98  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-6.66359182  6.41168705 -3.88023906 14.53598089  6.82718405  4.08742847\n",
      " -7.17129168  5.8474412   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 9)  new position  (4, 9)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 3)  new position  (3, 3)\n",
      "0.0 0 99  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 3)  new position  (2, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 9)  new position  (4, 0)\n",
      "0.0 0 100  Episode:  1\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating trained mc agent with vision range  2\n",
      "creating trained mc agent with vision range  2\n",
      "Monte Carlo Episode  2\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          -1.4903461    4.6284158    3.70347166   5.62583431\n",
      "  40.80444431 -12.75769724  -2.35448146   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 9)  new position  (2, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.61484067  2.00698516  0.          7.24035934  3.25252428  1.49725015\n",
      " 11.48278915  1.66025544  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 9)  new position  (1, 0)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "0.0 0 1  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-17.41534256   7.88056845  19.86933146   3.28466474  -7.00820013\n",
      "  -8.01962613  -2.16061445   7.77434297   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 0)  new position  (0, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [32.96490603 24.53003099  0.         13.88686505  0.         30.55473887\n",
      "  0.         16.3680176   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 8)  new position  (3, 8)\n",
      "0.0 0 2  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 1)  new position  (1, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 3.63080493 11.99107695  0.          8.07308779  0.          0.32058105\n",
      "  0.         14.34706733  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 8)  new position  (2, 9)\n",
      "0.0 0 3  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  5.9620649    1.94641291  -7.96817294  10.9731139    0.\n",
      " -15.33374388   8.12074766  10.63479905   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (1, 0)  new position  (1, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.83296515   0.         -10.27361498   0.73942305   2.53754731\n",
      " -23.27991913 -11.67598903  11.13552553   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 9)  new position  (1, 0)\n",
      "0.0 0 4  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  3.60142962 -18.56584051  -6.65848261   0.           0.35812925\n",
      " -12.03332482   9.58767238  13.5344701    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 9)  new position  (2, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.         -13.54220717  -6.11270814   9.93276592  -9.38136106\n",
      "  -1.57234022  -1.56173153   9.98917652   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 0)  new position  (0, 1)\n",
      "0.0 0 5  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.06666242 -4.20933188 -2.46213356 -1.44939792 14.48048317  0.5425013\n",
      "  4.79674737 -4.37421352  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 0)  new position  (3, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 1)  new position  (1, 0)\n",
      "0.0 0 6  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.98112892  -6.48708255  -1.3463421   -3.50657532 -12.80156601\n",
      "   5.80166736 -15.71575467 -18.22213432   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 0)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 31.284968    -4.82813909 -10.67881149  -1.93536291   4.62396222\n",
      "   6.19813269  -5.04263151  -6.92183961   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 0)  new position  (4, 9)\n",
      "0.0 0 7  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 0)  new position  (0, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 9)  new position  (3, 9)\n",
      "0.0 0 8  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 1)  new position  (9, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 9)  new position  (4, 9)\n",
      "0.0 0 9  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 9)  new position  (4, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 1)  new position  (8, 0)\n",
      "0.0 0 10  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [89.7274398  89.06374487  0.         85.15883109  0.         74.77677234\n",
      "  0.         93.42730209  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 8)  new position  (3, 9)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 0)  new position  (8, 1)\n",
      "0.0 0 11  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 24.28279917  -3.2503315   -3.4938431    7.46755294 -10.64272157\n",
      "  -9.66022703   0.34289003   0.44213197   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 1)  new position  (9, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 9)  new position  (2, 9)\n",
      "0.0 0 12  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 9)  new position  (1, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 1)  new position  (0, 1)\n",
      "0.0 0 13  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 1)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -6.56207282 -27.12745343   0.           0.99556812   0.\n",
      "  13.14049554  -7.48214198 -19.41851234   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 8)  new position  (1, 9)\n",
      "0.0 0 14  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  5.83476666  -4.14122699  -0.62789221  -6.0263128   -1.30658723\n",
      "  -3.91831767 -19.44145968 -14.46370346   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 9)  new position  (2, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 1)  new position  (2, 0)\n",
      "0.0 0 15  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -5.50580899 -10.74986099  -8.57866959 -12.23305748  -8.07333893\n",
      "   3.10203569   7.86994172   0.92401401   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 0)  new position  (2, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [32.96490603 24.53003099  0.         13.88686505  0.         30.55473887\n",
      "  0.         16.3680176   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  3\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 8)  new position  (3, 9)\n",
      "0.0 0 16  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.60068912  -1.56527786 -18.21765062 -16.05815022 -20.15585262\n",
      "   3.74367229  -3.91270147  20.77314364   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 1)  new position  (3, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 9)  new position  (2, 0)\n",
      "0.0 0 17  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-13.05619289  -3.41845186  -4.93090876  -1.92882385 -21.04003561\n",
      "  -1.88909721  -8.5787828    3.77564221   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 2)  new position  (3, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.13376359   0.55244619   0.82926859   6.67461715  20.98817125\n",
      "  15.01263039  -1.1080356  -12.02177202   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 0)  new position  (3, 1)\n",
      "0.0 0 18  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.62444121 -23.73709456  16.42390605  -6.32526569  -0.22103035\n",
      "  -1.60394471  -7.56754667  -2.04256855   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 1)  new position  (3, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 21.75441026 -11.48021392   0.           1.37708057   3.21613605\n",
      "  21.15051738   8.84234351 -11.45321548   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 3)  new position  (2, 2)\n",
      "0.0 0 19  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-12.13332075   0.          -3.97006123   0.68055633   3.31833028\n",
      "  -7.36893741  -4.07909027  16.03587251   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 2)  new position  (2, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.3419291  -4.65151162 15.07035668  0.         -7.79190632  9.32424621\n",
      " -7.56232928 -7.69634258  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 2)  new position  (3, 3)\n",
      "0.0 0 20  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -6.25216174   0.          19.90048585  -6.64597213   5.22773845\n",
      " -16.60369325   3.62771101   5.45672303   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 3)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  4.36986028 -17.54049064  -5.79772208   0.          19.55877763\n",
      "  18.42901212  -6.39997064   2.28355688   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 3)  new position  (2, 2)\n",
      "0.0 0 21  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 6.34720588 -0.5220924   3.99474023 19.00091629  2.40122707 15.25507223\n",
      " 15.33683375  0.          0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 4)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 2)  new position  (2, 1)\n",
      "1.0 1 22  Episode:  2\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0  reward:  50\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [69.52881381  0.         70.05529972  0.         73.01764175 66.14749216\n",
      " 64.01145425  0.          0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 5)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 1)  new position  (2, 0)\n",
      "2.0 1 23  Episode:  2\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.13376359   0.55244619   0.82926859   6.67461715  20.98817125\n",
      "  15.01263039  -1.1080356  -12.02177202   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 0)  new position  (3, 9)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [137.71056696 135.24143756 156.23914474 124.14674984 146.84263267\n",
      " 156.50896616 151.1500504    0.           0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 5)  new position  (3, 6)\n",
      "3.0 1 24  Episode:  2\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 9)  new position  (3, 8)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 4.18947951  0.         17.90481277  0.         12.16772324  0.\n",
      " 14.5125065   0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 6)  new position  (3, 5)\n",
      "4.0 1 25  Episode:  2\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [137.71056696 135.24143756 156.23914474 124.14674984 146.84263267\n",
      " 156.50896616 151.1500504    0.           0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 5)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [93.76971626 87.15408575  0.         82.07992191  0.         66.07738731\n",
      "  0.         98.40101489  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  3\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 8)  new position  (4, 9)\n",
      "4.0 0 26  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [127.23467882 124.73130822 119.69452981 190.09474944 109.78810779\n",
      " 179.80810638 116.81832313 193.56433146   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 4)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 9)  new position  (5, 8)\n",
      "5.0 1 27  Episode:  2\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0  reward:  50\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [24.27211104  4.74810304  0.         -1.5448469   0.         19.08709095\n",
      "  0.         20.93868559  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 8)  new position  (4, 8)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 5)  new position  (4, 4)\n",
      "5.0 0 28  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [89.7274398  89.06374487  0.         85.15883109  0.         74.77677234\n",
      "  0.         93.42730209  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  3\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 8)  new position  (5, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [127.23467882 124.73130822 119.69452981 190.09474944 109.78810779\n",
      " 179.80810638 116.81832313 193.56433146   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 4)  new position  (4, 3)\n",
      "5.0 0 29  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 3)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-6.66359182  6.41168705 -3.88023906 14.53598089  6.82718405  4.08742847\n",
      " -7.17129168  5.8474412   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 9)  new position  (5, 0)\n",
      "5.0 0 30  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 0)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 7.32355607 -8.66171527 -7.66889639 10.87065744 -5.48073061 -3.35123564\n",
      " -7.94980429 17.937935    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 3)  new position  (4, 4)\n",
      "5.0 0 31  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 1)  new position  (4, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [127.23467882 124.73130822 119.69452981 190.09474944 109.78810779\n",
      " 179.80810638 116.81832313 193.56433146   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 4)  new position  (4, 3)\n",
      "5.0 0 32  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 18.79541185  15.29016956 -23.24279921  27.3998372    0.94856394\n",
      " -17.70957831  -8.0330003   -3.75463173   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 1)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -3.59784553  -0.35510994 -16.89936954  14.31025705   0.\n",
      "  -5.70362038  12.22733106 -20.67125455   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 3)  new position  (4, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 0 33  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.0693164    0.          11.95708115 -10.91229495  -2.20138681\n",
      " -11.04189672 -23.33025344  -6.21126516   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 2)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-12.11628669 -12.67246255  -2.30712483  17.684708     1.37747349\n",
      "  -0.55119908   1.70188875 -13.53912838   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 2)  new position  (3, 2)\n",
      "5.0 0 34  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 3)  new position  (5, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  4.78283262   9.7341236  -10.1199061   12.04763172  17.06204777\n",
      "  -8.2639297    3.07575223  -8.60477836   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 2)  new position  (2, 1)\n",
      "5.0 0 35  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-0.12219368 -1.17774797 12.51845612  0.58531854  1.43162645  2.09377057\n",
      " -0.60055713  0.37254496  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 1)  new position  (3, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.27734413  -3.44764895 -16.03392009  11.18062741   1.39313647\n",
      "   0.          14.85456514   5.4242984    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 4)  new position  (4, 4)\n",
      "5.0 0 36  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 20.93036861  10.55679232  -0.16105645   1.6087172   35.17889796\n",
      "  27.32356384  14.09692232 -13.31013925   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 2)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [16.81330578 17.46274163  0.         37.30053506 11.4216387  19.62244106\n",
      " 19.25415051 27.54018734  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 4)  new position  (5, 3)\n",
      "5.0 0 37  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           6.87447813  21.10015732   9.44557677  -9.64048519\n",
      " -14.96876791   3.80800026   1.21568588   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 3)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -0.69903417  10.97847514 -16.97540658   6.91061407 -15.12392938\n",
      "  14.15022956   9.9191946    1.97872133   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 3)  new position  (5, 2)\n",
      "5.0 0 38  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  2.18955782 -10.33555515 -19.12223687  14.37387755 -16.62766684\n",
      "   7.39668606   6.20906723  -5.8642132    0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 3)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 2)  new position  (6, 3)\n",
      "5.0 0 39  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 3)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [83.13354677 41.70036688 51.5136775  88.38224615 46.63233597 82.60630683\n",
      " 32.2643727   0.          0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 4)  new position  (3, 3)\n",
      "5.0 0 40  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 3.6359424   4.66838662  3.61232695  0.69975406 -6.82693662 -1.95821441\n",
      " 21.32860793  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 4)  new position  (8, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 3)  new position  (2, 3)\n",
      "5.0 0 41  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.68256275 -16.7174098    2.02334361 -19.31482151  10.02558202\n",
      "  10.69528709  12.2001919    2.2586472    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 3)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 5)  new position  (7, 5)\n",
      "5.0 0 42  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.52939684   0.         -16.17025784 -20.99596012 -43.77061943\n",
      " -17.78290425  10.17861875   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 5)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-13.66250169 -22.99056629 -14.69449532   0.         -12.46762071\n",
      "  11.60988195  -3.63171213 -14.89210718   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 4)  new position  (2, 5)\n",
      "6.0 1 43  Episode:  2\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0  reward:  50\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [69.52881381  0.         70.05529972  0.         73.01764175 66.14749216\n",
      " 64.01145425  0.          0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 5)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 3.6359424   4.66838662  3.61232695  0.69975406 -6.82693662 -1.95821441\n",
      " 21.32860793  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 4)  new position  (6, 4)\n",
      "6.0 0 44  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 4)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [83.13354677 41.70036688 51.5136775  88.38224615 46.63233597 82.60630683\n",
      " 32.2643727   0.          0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 4)  new position  (3, 5)\n",
      "7.0 1 45  Episode:  2\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0  reward:  50\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 3.6359424   4.66838662  3.61232695  0.69975406 -6.82693662 -1.95821441\n",
      " 21.32860793  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 4)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [137.71056696 135.24143756 156.23914474 124.14674984 146.84263267\n",
      " 156.50896616 151.1500504    0.           0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 5)  new position  (2, 5)\n",
      "8.0 1 46  Episode:  2\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 3)  new position  (7, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [69.52881381  0.         70.05529972  0.         73.01764175 66.14749216\n",
      " 64.01145425  0.          0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 5)  new position  (2, 4)\n",
      "8.0 0 47  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [83.13354677 41.70036688 51.5136775  88.38224615 46.63233597 82.60630683\n",
      " 32.2643727   0.          0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 4)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 2)  new position  (6, 3)\n",
      "8.0 0 48  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 3)  new position  (0, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 3)  new position  (7, 3)\n",
      "8.0 0 49  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 2)  new position  (1, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 3)  new position  (8, 2)\n",
      "8.0 0 50  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 2)  new position  (9, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.13656306  -2.34978131 -10.80859449  -1.07542513   3.75238168\n",
      "  -4.30916511  -9.32701915 -14.55468465   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 2)  new position  (0, 1)\n",
      "8.0 0 51  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.           5.54577636   1.40973193 -18.11687927   0.30538888\n",
      "   6.54617107  15.70643838   2.9957219    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 1)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [27.09344101  4.32839245 -7.01949953 14.6484538   1.40575974 -2.16608579\n",
      " 12.15797501 -7.22392786  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 1)  new position  (0, 0)\n",
      "8.0 0 52  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [15.20468107  4.80741379 -1.32301275  5.34196122 -2.21579628 -8.23900827\n",
      "  9.89604054  4.10739886  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 0)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 0)  new position  (9, 0)\n",
      "8.0 0 53  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "I havent seen this before, picking randomly\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 9)  new position  (8, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 10.24505455  -0.25021018 -17.45039611   2.52137083   6.32793729\n",
      "   6.34464879   2.95778244   5.57485265   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 0)  new position  (0, 0)\n",
      "8.0 0 54  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.99404961   0.30537518 -10.90900703  13.16880254  11.76403891\n",
      "  -1.98784644  10.11619996 -11.40819043   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 0)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [0.224      1.06407782 0.         0.         0.632      0.071744\n",
      " 0.6        0.         0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 8)  new position  (7, 8)\n",
      "8.0 0 55  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -0.78358319   9.99560513  -0.8186839    7.1561357    4.67896793\n",
      "  17.49584075 -11.43981585 -11.16205718   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 0)  new position  (0, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 1.71163303  5.56985383  2.5412142  25.58548787  8.48147069  7.12962517\n",
      "  0.          1.7325241   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 8)  new position  (8, 8)\n",
      "8.0 0 56  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 9.22531561 15.6433996   3.39909888  5.97978173 12.51848946 11.77001539\n",
      " -3.45792158 -6.09752539  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 9)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -4.083294     6.54414723 -10.66878848  -5.50605004  14.6984957\n",
      " -11.96483812  -1.46162072 -13.15035899   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 8)  new position  (9, 8)\n",
      "8.0 0 57  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.90552927  -8.8917334  -22.94309302  -6.01524759   5.08220054\n",
      " -12.04637986   7.96967495  15.55758489   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 8)  new position  (9, 7)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 0)  new position  (8, 1)\n",
      "8.0 0 58  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 2.14605619 -0.62442465  3.24       -0.544      -0.45385538  0.316032\n",
      "  3.528      -2.2720487   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 1)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 7)  new position  (9, 8)\n",
      "8.0 0 59  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -7.38270594 -10.37359532  -1.24613361 -31.32755166  -3.79228522\n",
      "   3.33454445   0.72222754 -13.56420957   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 0)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-11.9904182    0.24357795  -4.22444428   0.           1.59861694\n",
      "  -0.0415142    1.34027771  12.57872778   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 8)  new position  (8, 9)\n",
      "8.0 0 60  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  5.48223628   0.         -11.84385001  -1.32601664 -15.84773121\n",
      "   9.25363603  -6.98963165  -0.69487528   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 9)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -3.60636607  -2.59721818  17.33075996  13.35257287  -4.40038354\n",
      "   8.68353356 -17.55711743   0.94073827   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 9)  new position  (9, 9)\n",
      "8.0 0 61  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-12.62775099  -5.49317457 -12.22643532 -11.22630308   0.\n",
      "  -9.30173304 -17.98718326   0.           0.        ]\n",
      "max index list:  [4, 7, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 8)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -2.80356564 -26.91569072   0.          -4.88088474  -5.61840494\n",
      " -12.73124762  -4.50120113 -11.60416826   0.        ]\n",
      "max index list:  [2, 8]\n",
      "max_action:  2\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 9)  new position  (8, 0)\n",
      "8.0 0 62  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  8.67515208   2.97629578  -5.79841661 -22.46322027 -11.63944705\n",
      "  -6.38075077  -1.7968488    1.18632839   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 8)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [ -1.47517522   3.52722627 -13.11934236   8.04235891   0.\n",
      "  -6.03612902   5.64101728 -10.50915518   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 0)  new position  (7, 9)\n",
      "8.0 0 63  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.21879171  10.60283054  16.61980262  14.68863002  -2.2771483\n",
      " -13.44593254 -11.76688611   8.78525981   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 9)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  5.48223628   0.         -11.84385001  -1.32601664 -15.84773121\n",
      "   9.25363603  -6.98963165  -0.69487528   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 9)  new position  (9, 0)\n",
      "8.0 0 64  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-16.03795492  -8.61329575   2.58363614   0.04447955  -1.31435269\n",
      "  -1.31211427   0.          -4.74990052   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 0)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -6.27106819  15.34185642  -3.28348814   0.82273743  -0.26963898\n",
      "   8.07432565 -25.12518318  27.03640858   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 9)  new position  (8, 8)\n",
      "8.0 0 65  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  7.67984474  -3.60193524 -18.0192976    1.23113909 -10.12706638\n",
      "   5.48482189  -2.0733215    4.54473225   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 8)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 0)  new position  (1, 0)\n",
      "8.0 0 66  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 9)  new position  (8, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [13.12601014 12.14844637 -1.10361793 14.52261584 -4.11906788 -0.46161736\n",
      "  7.60447512 -0.15495821  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 0)  new position  (1, 1)\n",
      "8.0 0 67  Episode:  2\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 8)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 1)  new position  (0, 1)\n",
      "10.0 2 68  Episode:  2\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0  reward:  50\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 8)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 1)  new position  (1, 0)\n",
      "12.0 2 69  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 2.61369517  0.95811668 -4.95438632 -1.1843165  17.41112786 -2.10671623\n",
      " -7.94371399  7.12117821  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 0)  new position  (2, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.64139206 -12.06297389   0.64816965  -0.91339204   0.\n",
      " -21.16890148  -6.05296217   3.77778747   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 8)  new position  (9, 9)\n",
      "14.0 2 70  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 9)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 9)  new position  (3, 8)\n",
      "16.0 2 71  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-43.47508282 -47.5209709    0.         -40.65976171   0.\n",
      " -48.0735796    0.         -43.32979656   0.        ]\n",
      "max index list:  [2, 4, 6, 8]\n",
      "max_action:  7\n",
      "index max:  4\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 8)  new position  (2, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 9)  new position  (8, 8)\n",
      "18.0 2 72  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 9)  new position  (1, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 8)  new position  (7, 7)\n",
      "20.0 2 73  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -6.56207282 -27.12745343   0.           0.99556812   0.\n",
      "  13.14049554  -7.48214198 -19.41851234   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 8)  new position  (1, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -2.49282953   0.          -5.10580674  16.41220304 -19.35421611\n",
      "   7.70304236   0.           6.33952232   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 7)  new position  (7, 8)\n",
      "22.0 2 74  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [11.58237875 -6.20658786  2.99980652  2.47050577  5.14152991  6.87504739\n",
      "  1.44654251 -0.35355501  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 9)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 1.71163303  5.56985383  2.5412142  25.58548787  8.48147069  7.12962517\n",
      "  0.          1.7325241   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 8)  new position  (8, 7)\n",
      "24.0 2 75  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-10.07688183 -28.80657813  -8.45533959  -0.51490706  -5.02334143\n",
      "  27.05083553 -18.10199168  -4.02836738   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 7)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 0)  new position  (9, 9)\n",
      "26.0 2 76  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 6)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 9)  new position  (8, 8)\n",
      "28.0 2 77  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.         -26.28149844 -13.90777681   1.26203274 -37.85780123\n",
      "   0.         -31.23643274 -33.57173093   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 5)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 8)  new position  (8, 9)\n",
      "30.0 2 78  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 9)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-13.66250169 -22.99056629 -14.69449532   0.         -12.46762071\n",
      "  11.60988195  -3.63171213 -14.89210718   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 4)  new position  (2, 5)\n",
      "32.0 2 79  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 9)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-67.0279355    0.         -62.2929404    0.         -68.30444373\n",
      " -77.1730768  -74.99008767   0.           0.        ]\n",
      "max index list:  [1, 3, 7, 8]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 5)  new position  (2, 4)\n",
      "34.0 2 80  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-66.73016431 -67.96617461 -55.21826397 -76.19583442 -63.20159073\n",
      " -72.3736375  -69.90636166   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 4)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 0)  new position  (7, 1)\n",
      "36.0 2 81  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 1)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-75.81380959 -70.16986165 -75.09523808 -80.12394475 -77.44444748\n",
      "   0.         -78.54820119   0.           0.        ]\n",
      "max index list:  [5, 7, 8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 5)  new position  (3, 6)\n",
      "38.0 2 82  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.         -91.4346591    0.         -73.95859063\n",
      "   0.         -78.95043809   0.           0.        ]\n",
      "max index list:  [0, 1, 3, 5, 7, 8]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 6)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 0)  new position  (9, 1)\n",
      "40.0 2 83  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-75.81380959 -70.16986165 -75.09523808 -80.12394475 -77.44444748\n",
      "   0.         -78.54820119   0.           0.        ]\n",
      "max index list:  [5, 7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 5)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 1)  new position  (9, 2)\n",
      "42.0 2 84  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.04744736  -5.64635074   3.66599274  12.73537913 -25.13640359\n",
      "   3.20590757  -7.84987033  -9.80192907   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 2)  new position  (8, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-78.71581564 -80.43253486 -69.33476811   0.         -70.88229148\n",
      " -73.74777987 -80.8521758  -82.15321311   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 5)  new position  (5, 5)\n",
      "44.0 2 85  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         69.53816915 34.71487743 28.19813359 25.10245093  0.\n",
      " 48.31414373  0.          0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 5)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 24.28279917  -3.2503315   -3.4938431    7.46755294 -10.64272157\n",
      "  -9.66022703   0.34289003   0.44213197   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 1)  new position  (7, 2)\n",
      "46.0 2 86  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-68.92743653 -68.70849608 -69.70747863 -70.1128838  -61.56628743\n",
      " -62.51875833 -66.24857449 -70.72833417   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 4)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 2)  new position  (6, 3)\n",
      "48.0 2 87  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-19.07216686  19.27841881  -7.53295476   3.18871625  -5.09643561\n",
      "  -0.46281475   6.94239151   2.55012035   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 3)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -3.59784553  -0.35510994 -16.89936954  14.31025705   0.\n",
      "  -5.70362038  12.22733106 -20.67125455   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 3)  new position  (4, 4)\n",
      "50.0 2 88  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -6.94593416   0.1283805   -5.26032004  13.15440561  -8.00861099\n",
      "   5.27550465  18.64669643 -18.44587756   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 2)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-68.92743653 -68.70849608 -69.70747863 -70.1128838  -61.56628743\n",
      " -62.51875833 -66.24857449 -70.72833417   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 4)  new position  (3, 4)\n",
      "52.0 2 89  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 1)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-70.12404941 -82.08217971 -69.14018024 -74.9788424  -76.45323677\n",
      " -79.63171014 -75.16631978 -73.47128783   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 4)  new position  (4, 5)\n",
      "54.0 2 90  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 1)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-78.71581564 -80.43253486 -69.33476811   0.         -70.88229148\n",
      " -73.74777987 -80.8521758  -82.15321311   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 5)  new position  (5, 6)\n",
      "56.0 2 91  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.          0.         49.69430598  0.          0.          0.\n",
      " 64.97458251  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 6)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 2)  new position  (4, 3)\n",
      "58.0 2 92  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.55398187  6.3321269   2.69355226 -3.95548218  8.72557649 17.64383716\n",
      "  0.89348879 -8.57084647  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 3)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         20.67988478  6.64521021 24.95784245 19.0445307   0.\n",
      " 16.48573212  0.          0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 5)  new position  (5, 4)\n",
      "60.0 2 93  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [11.02798533  6.55218241 11.95776869  6.99790826 10.05445967  0.\n",
      " 16.13928933  7.07586023  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 4)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [  0.          -2.34095323  -4.6593708   -8.35324084  -7.11245351\n",
      "  -7.36976475 -10.41060851  -5.61636555   0.        ]\n",
      "max index list:  [0, 8]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 4)  new position  (3, 5)\n",
      "62.0 2 94  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-12.94092668  -8.62775278 -10.37409869 -11.64315009   0.\n",
      "   0.         -10.61877067   0.           0.        ]\n",
      "max index list:  [4, 5, 7, 8]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 5)  new position  (3, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-2.44070163 -7.04025644 -4.78714726 -0.272512   -1.42868255 -5.88336022\n",
      "  0.096      -4.07247734  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 4)  new position  (4, 3)\n",
      "64.0 2 95  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.         -91.4346591    0.         -73.95859063\n",
      "   0.         -78.95043809   0.           0.        ]\n",
      "max index list:  [0, 1, 3, 5, 7, 8]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 6)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 5.68623556 -5.36856678 -0.2822764  -1.91755964  2.22291319  5.3957343\n",
      "  4.54076576 -4.44925096  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 3)  new position  (5, 2)\n",
      "66.0 2 96  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 2)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-78.71581564 -80.43253486 -69.33476811   0.         -70.88229148\n",
      " -73.74777987 -80.8521758  -82.15321311   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 5)  new position  (5, 4)\n",
      "68.0 2 97  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 1)  new position  (4, 1)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [18.70422713 55.98165731 38.76218371 75.8867771  24.14929277  0.\n",
      " 39.0148697  81.90653168  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 4)  new position  (4, 5)\n",
      "70.0 2 98  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.91631167  -9.40148994  -9.11867023 -16.11964308 -14.68625626\n",
      "  15.86562523 -10.67987709  -2.1308996    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 1)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-78.71581564 -80.43253486 -69.33476811   0.         -70.88229148\n",
      " -73.74777987 -80.8521758  -82.15321311   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 5)  new position  (4, 4)\n",
      "72.0 2 99  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 1)  new position  (4, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -5.33442671 -14.42731732 -10.22620461  -7.26514133  -9.11385125\n",
      "  -6.17207025  -6.81264809 -10.99120312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 4)  new position  (5, 4)\n",
      "74.0 2 100  Episode:  2\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating trained mc agent with vision range  2\n",
      "creating trained mc agent with vision range  2\n",
      "Monte Carlo Episode  3\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 0.    -0.104  0.     0.     0.     0.     0.     0.     0.   ]\n",
      "max index list:  [0, 2, 3, 4, 5, 6, 7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 2)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-13.46811705  -9.26349957 -15.31080056   1.66490696  -9.38345781\n",
      "  -2.91629704   0.         -12.33343293   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 3)  new position  (2, 2)\n",
      "0.0 0 1  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-18.93521507 -11.00090258 -12.70187624  -3.25112457 -19.00214649\n",
      "  -6.60934224  -3.24248697  12.51441603   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 2)  new position  (9, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 2)  new position  (2, 3)\n",
      "0.0 0 2  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 1)  new position  (0, 1)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-3.97137973  9.28149264 -9.89224823 -8.25442123 -2.18714208 -1.59021699\n",
      " -3.31154358 -5.46967022  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 3)  new position  (1, 2)\n",
      "0.0 0 3  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 16.64443965  -2.39886161 -23.76753891   0.2880325    3.61551049\n",
      "   0.          -5.8100863    1.49306026   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 1)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.38186384   0.           0.77545337   0.04193359 -13.87484205\n",
      "  -1.00541516   4.106414     2.20689087   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 2)  new position  (2, 2)\n",
      "0.0 0 4  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-18.93521507 -11.00090258 -12.70187624  -3.25112457 -19.00214649\n",
      "  -6.60934224  -3.24248697  12.51441603   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 2)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.74792593 -22.90964885  -3.5764448  -11.12989785  -3.76186982\n",
      "  -5.97372463   0.           4.35819548   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 2)  new position  (1, 2)\n",
      "0.0 0 5  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  7.95734772 -25.07054638   0.           2.82370114 -13.94244974\n",
      "  -7.12983107   4.42167415  -0.32916439   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 2)  new position  (2, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.55804349  14.4671326  -12.26087558 -18.42959976 -10.81152182\n",
      "   0.           9.10546053  -9.19181535   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 1)  new position  (1, 2)\n",
      "0.0 0 6  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.           1.43038152 -17.28649641  -5.52861442 -22.96676617\n",
      " -22.79450346   5.32404366 -13.38737296   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 2)  new position  (0, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  1.16010968 -17.09051427   1.00457846  -8.67631189 -20.72116434\n",
      "  -3.52687154  -4.02970642  -9.30755014   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 2)  new position  (2, 3)\n",
      "0.0 0 7  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  8.52930166 -13.49981213   6.59844351 -29.37566318 -21.93611009\n",
      "   6.74349691  -0.84907558   6.45153221   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 3)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-6.77589748  0.         -3.91147067 -6.09299104 -7.36606801 -8.3308153\n",
      " -8.69291398  6.27753362  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 3)  new position  (3, 2)\n",
      "0.0 0 8  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-15.04641842  -8.62196131  16.15787068  -0.46142169   0.1978661\n",
      "  20.56651994 -14.6688173   -9.23393779   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 2)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 3)  new position  (0, 4)\n",
      "0.0 0 9  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 4)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 3)  new position  (5, 3)\n",
      "0.0 0 10  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 4)  new position  (0, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.00746759 -11.1412527  -19.62385086  -1.01596757  -1.60346104\n",
      "  15.51659352 -10.01204177  -5.37875616   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 3)  new position  (5, 4)\n",
      "0.0 0 11  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 4)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 3)  new position  (9, 3)\n",
      "0.0 0 12  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 4)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 3)  new position  (8, 4)\n",
      "0.0 0 13  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 5)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -7.45004409  -6.33676499 -13.79110225  -4.2957285   -3.51702294\n",
      "  -2.78236652  11.52325912   7.97171486   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 4)  new position  (9, 5)\n",
      "0.0 0 14  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 4)  new position  (7, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in max action with choice\n",
      "Q state is  [ 19.06459769  -2.17534761 -17.00417613   1.73007078 -17.81628548\n",
      "  -2.66133757 -12.83715154 -32.99968788   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 5)  new position  (0, 5)\n",
      "0.0 0 15  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 5)  new position  (0, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.52939684   0.         -16.17025784 -20.99596012 -43.77061943\n",
      " -17.78290425  10.17861875   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 5)  new position  (8, 5)\n",
      "0.0 0 16  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -6.75282024 -12.85090611  -4.80386391   3.4681171  -27.9779682\n",
      "   5.05603726  -9.46382883   7.48818584   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 5)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.           7.25430077  -1.50929953   4.57094407   0.\n",
      "   0.         -10.6949828   -5.77434817   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 6)  new position  (9, 7)\n",
      "0.0 0 17  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 7)  new position  (8, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-19.33579748 -14.1667125    6.4914146   -5.2758401   -0.96428484\n",
      "  -7.63875371  -5.16911044  -4.08162076   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 4)  new position  (9, 5)\n",
      "0.0 0 18  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-16.12196777 -11.50532382 -14.6371846   -9.99033582 -12.90899084\n",
      "  -6.87889305   1.44988238   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 5)  new position  (8, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-18.43704595 -20.40882822   0.          -2.9487985  -10.17881919\n",
      "  -5.53174997  -6.08534879 -15.37782126   0.        ]\n",
      "max index list:  [2, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 6)  new position  (7, 6)\n",
      "0.0 0 19  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-14.73881104 -23.75324173 -17.1830991  -12.88378937  -9.6575349\n",
      "  -9.52377256  -2.22223779   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-23.36746611   0.          16.50826588  -2.66830932 -17.36230227\n",
      "  -5.23969945   0.           0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 6)  new position  (7, 5)\n",
      "0.0 0 20  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  7.83249285 -17.05816363  -6.03445727   6.49932201  -3.56478612\n",
      "   5.20664695 -15.59356632  -2.12830939   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 6)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.52939684   0.         -16.17025784 -20.99596012 -43.77061943\n",
      " -17.78290425  10.17861875   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 5)  new position  (7, 4)\n",
      "0.0 0 21  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 7)  new position  (1, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 3.6359424   4.66838662  3.61232695  0.69975406 -6.82693662 -1.95821441\n",
      " 21.32860793  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 4)  new position  (6, 3)\n",
      "0.0 0 22  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 3)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [29.11445124 22.74569532  0.         13.97051409  0.         27.76971107\n",
      " 26.97358949  2.41201194  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 8)  new position  (0, 9)\n",
      "0.0 0 23  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 9)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 4)  new position  (5, 5)\n",
      "0.0 0 24  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 8)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 5)  new position  (6, 4)\n",
      "0.0 0 25  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 9)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 4)  new position  (7, 5)\n",
      "0.0 0 26  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-3.51807784 -9.02952573 18.98586574  4.09787806  6.95132486 13.33474817\n",
      " 13.18001185  1.94647051  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 9)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.52939684   0.         -16.17025784 -20.99596012 -43.77061943\n",
      " -17.78290425  10.17861875   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 5)  new position  (8, 4)\n",
      "0.0 0 27  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 0)  new position  (8, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 4)  new position  (8, 3)\n",
      "0.0 0 28  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.99033685  -0.21369508 -17.78386442 -17.73534807  -0.73535487\n",
      "   3.25833488   4.01263436 -18.12220829   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 3)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -3.74153777  10.13759838   1.16262612  -5.09640635  -5.39328865\n",
      " -29.96581532  -3.92845763   3.50949081   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 1)  new position  (7, 2)\n",
      "0.0 0 29  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-7.19684951  3.76933477  0.         -7.88422537  6.89985758 -3.4502183\n",
      "  9.13642104  9.91375109  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 3)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-17.48703515  -2.8621897   11.41292642  -8.99247044  -9.00496607\n",
      " -14.77126539  14.63240104   0.99390264   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 2)  new position  (6, 3)\n",
      "0.0 0 30  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [15.07382067  0.4524692   2.80711762  0.         -5.36909771  1.30728464\n",
      " 10.3422522  20.29917306  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 3)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-2.37428174 11.73697951  1.35024868  0.          0.         25.72635023\n",
      " 18.08528393 19.57173576  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 4)  new position  (5, 5)\n",
      "0.0 0 31  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -2.58620332 12.43419045  7.58168105 -6.64198416  0.\n",
      "  2.75743108 -5.82558898  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 5)  new position  (5, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 3)  new position  (6, 4)\n",
      "0.0 0 32  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.          3.18429357  5.50003645  0.          0.          0.\n",
      " 16.51203474  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  2\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 6)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [28.26415834  6.46044902 11.72500892  0.          2.5346311   5.73246354\n",
      "  3.66062093 22.88536221  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 4)  new position  (7, 4)\n",
      "0.0 0 33  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 5)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 3.6359424   4.66838662  3.61232695  0.69975406 -6.82693662 -1.95821441\n",
      " 21.32860793  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 4)  new position  (7, 3)\n",
      "0.0 0 34  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 3)  new position  (8, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 4)  new position  (5, 4)\n",
      "0.0 0 35  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 4)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 10.27603174  -5.63920849  -4.0714197  -23.42504938 -17.31816488\n",
      "  -7.89739378  -2.34866149   5.54928772   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 2)  new position  (7, 3)\n",
      "0.0 0 36  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.82702598  -0.70796141 -13.83061141  -7.85426937   5.82730974\n",
      "   2.89429255   0.14135414   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 3)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.59932772 14.25865817  0.          0.          0.33051561 39.33702272\n",
      " 26.78188402 37.16980425  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 4)  new position  (5, 5)\n",
      "0.0 0 37  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.          17.51276463  22.95281402  11.28281692   4.04047059\n",
      "   0.         -11.52109418  19.23887483   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 3)  new position  (7, 3)\n",
      "0.0 0 38  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 6)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  7.78327428   2.27825802   0.64604386 -12.29494344  -2.8019016\n",
      "   5.17787179  -9.78357728  11.02081898   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 3)  new position  (8, 2)\n",
      "0.0 0 39  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 2)  new position  (9, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 5)  new position  (5, 4)\n",
      "0.0 0 40  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 4)  new position  (5, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.04744736  -5.64635074   3.66599274  12.73537913 -25.13640359\n",
      "   3.20590757  -7.84987033  -9.80192907   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 2)  new position  (8, 3)\n",
      "0.0 0 41  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.00746759 -11.1412527  -19.62385086  -1.01596757  -1.60346104\n",
      "  15.51659352 -10.01204177  -5.37875616   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 3)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-10.34988975   3.91464264 -25.11182866 -14.65202055   0.1510253\n",
      " -18.08752687   0.98402814  -0.76309958   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 3)  new position  (7, 3)\n",
      "0.0 0 42  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-2.37428174 11.73697951  1.35024868  0.          0.         25.72635023\n",
      " 18.08528393 19.57173576  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 4)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  3.97733771   0.          -4.04135695 -22.30479101   8.23714887\n",
      "   3.60014314 -16.08795149   9.5269079    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 3)  new position  (8, 2)\n",
      "0.0 0 43  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -8.33446556  -1.15223345   5.71108528 -13.47462203  -3.90361434\n",
      " -24.95516828  -8.10887156 -19.1604145    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 2)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -3.35936419  12.14963837   1.84434718 -29.86547573   5.17617944\n",
      " -27.53489966  -1.87480419 -31.38996799   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 3)  new position  (5, 2)\n",
      "0.0 0 44  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-12.89576728  14.03638652   0.70366456  -9.681752     0.66277386\n",
      " -18.67467127   4.40047242  10.08356341   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 2)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 1)  new position  (8, 2)\n",
      "0.0 0 45  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 3)  new position  (4, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 2)  new position  (9, 1)\n",
      "0.0 0 46  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.15097503 -12.14609808  -2.91612341 -14.75613918  -7.56585743\n",
      "   5.78604482  -0.56091308  -7.45090002   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 1)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 2)  new position  (5, 3)\n",
      "0.0 0 47  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.00746759 -11.1412527  -19.62385086  -1.01596757  -1.60346104\n",
      "  15.51659352 -10.01204177  -5.37875616   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 3)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 0)  new position  (9, 9)\n",
      "0.0 0 48  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 0.32335442  8.76943245 -0.28093388  7.85075325  5.54436921 10.52129201\n",
      "  0.27451954 -5.1413338   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 9)  new position  (0, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 4)  new position  (4, 5)\n",
      "0.0 0 49  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 5)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30851671  15.10050741   3.98786117  14.30216312   0.\n",
      "  -0.27077046 -11.8440253    6.7174165    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 8)  new position  (9, 9)\n",
      "0.0 0 50  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 5)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 9)  new position  (8, 9)\n",
      "0.0 0 51  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 9)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 4)  new position  (1, 4)\n",
      "0.0 0 52  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.46722491 -1.77699393 -4.893463    0.         20.07332682 40.25356017\n",
      " 21.03019205  7.96800248  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 4)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 9)  new position  (6, 9)\n",
      "0.0 0 53  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 5)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -4.24702509 -14.13072768   0.9504665    0.83209999  -1.26152662\n",
      "  12.25639219  -7.01730338   9.02650008   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 9)  new position  (7, 9)\n",
      "0.0 0 54  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 9)  new position  (8, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 4)  new position  (9, 5)\n",
      "0.0 0 55  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 5)  new position  (8, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  3.98077757  -5.84942215  -8.43020152 -23.58875588  -8.40623058\n",
      " -20.70784334 -22.51703133   8.38134944   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 8)  new position  (8, 9)\n",
      "0.0 0 56  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-19.25776765 -16.71693457   1.14421871  -7.78298141 -19.78017513\n",
      "   2.83826722 -12.45137718   6.02343751   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 6)  new position  (9, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 9)  new position  (9, 9)\n",
      "0.0 0 57  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 9)  new position  (8, 8)\n",
      "0.0 0 58  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.29245369 -28.014305    -6.08881321   6.55427912  -4.52073332\n",
      " -14.95606829  -4.49349115 -15.69917286   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 6)  new position  (9, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 8)  new position  (7, 7)\n",
      "0.0 0 59  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.53543245  -1.27353541 -19.01995703  -5.02509242 -15.24345707\n",
      "   1.47603992 -12.79209983  -1.44418165   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 5)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -2.49282953   0.          -5.10580674  16.41220304 -19.35421611\n",
      "   7.70304236   0.           6.33952232   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  4\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 7)  new position  (8, 8)\n",
      "0.0 0 60  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 8)  new position  (7, 8)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 5)  new position  (9, 6)\n",
      "0.0 0 61  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 0.0524524  -1.07588843 13.84614493 -5.96978828 10.63061446 11.30196963\n",
      " -3.86776533 11.58194732  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 6)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 1.71163303  5.56985383  2.5412142  25.58548787  8.48147069  7.12962517\n",
      "  0.          1.7325241   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 8)  new position  (8, 9)\n",
      "0.0 0 62  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 9)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 5)  new position  (0, 4)\n",
      "0.0 0 63  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 4)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 8)  new position  (0, 7)\n",
      "0.0 0 64  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -6.7799212   9.27118107 -2.31061223  6.23764537  0.\n",
      " 10.52735415 32.07100339  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 5)  new position  (0, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 0.          6.58554926  0.         12.65575428  0.         27.81135189\n",
      "  5.59474868 -7.28967706  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  4\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 7)  new position  (9, 8)\n",
      "0.0 0 65  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -9.46082257  -7.29992874  -8.05031137  -7.05108657  -8.08311869\n",
      " -15.09374417  -9.42907564   0.44971668   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 8)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.          16.9698146    6.44973079 -21.96511259   0.\n",
      "   0.          14.43365271  -6.44731182   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 6)  new position  (9, 6)\n",
      "0.0 0 66  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 9)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-20.77095707 -18.99464603 -13.74600454   7.84990542  -2.50551014\n",
      " -11.21174437   3.27913443   0.85953069   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 6)  new position  (0, 6)\n",
      "0.0 0 67  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         15.63336926 -1.4554295  -2.89274993  0.          0.\n",
      " 10.32667263  2.87868672  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 6)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  5.21862476 -17.13679682  -7.17152059  -0.43525302   0.\n",
      "  -2.10292699 -12.74452988   2.26725107   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 8)  new position  (8, 9)\n",
      "0.0 0 68  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 0.         33.27517496 30.11542266 14.50083924  0.          2.25772946\n",
      " 25.71940252  3.92782485  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 7)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -8.19886298  -9.69667369 -14.43046374  -0.91308149   0.\n",
      "   5.3047776  -15.49698183 -17.36706712   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 9)  new position  (8, 8)\n",
      "0.0 0 69  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-11.31165367   0.          16.65206416  -4.81185939 -14.72562791\n",
      " -16.45226248  -6.05320358   5.50559149   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 8)  new position  (0, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 8.575585   10.37980377 17.65760317  0.80470341 -0.14565256 -8.89312102\n",
      "  4.82473474 -0.13940526  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 8)  new position  (8, 7)\n",
      "0.0 0 70  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.56652411   6.54543242 -19.82157095   0.1218561    1.18982778\n",
      " -11.90931214  -5.72961713   4.73795296   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 9)  new position  (1, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-10.07688183 -28.80657813  -8.45533959  -0.51490706  -5.02334143\n",
      "  27.05083553 -18.10199168  -4.02836738   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 7)  new position  (9, 6)\n",
      "0.0 0 71  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [13.12601014 12.14844637 -1.10361793 14.52261584 -4.11906788 -0.46161736\n",
      "  7.60447512 -0.15495821  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (1, 0)  new position  (1, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 6)  new position  (0, 5)\n",
      "0.0 0 72  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [11.58237875 -6.20658786  2.99980652  2.47050577  5.14152991  6.87504739\n",
      "  1.44654251 -0.35355501  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 9)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 5)  new position  (9, 5)\n",
      "0.0 0 73  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 0)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 5)  new position  (8, 6)\n",
      "0.0 0 74  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 1)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-19.25776765 -16.71693457   1.14421871  -7.78298141 -19.78017513\n",
      "   2.83826722 -12.45137718   6.02343751   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 6)  new position  (7, 6)\n",
      "0.0 0 75  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  3.16222337   0.         -19.97131568  -3.71037322 -30.7942956\n",
      "  -1.52010631   0.           0.           0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 6)  new position  (7, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 2)  new position  (9, 2)\n",
      "0.0 0 76  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.52939684   0.         -16.17025784 -20.99596012 -43.77061943\n",
      " -17.78290425  10.17861875   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 5)  new position  (8, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-10.4271199  -18.30851808  -5.64799845 -32.90507545 -12.48735046\n",
      " -11.63434604 -29.05170535 -28.36500736   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 2)  new position  (0, 2)\n",
      "0.0 0 77  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-17.36112948  -7.00031114  -8.94310554 -20.19554192  -5.71550601\n",
      "  -1.54812084  -5.60400036  -4.99549304   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 2)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-17.25023968  -8.99405837 -17.67365233   6.99122197   0.\n",
      " -17.72728152 -15.14178416 -10.39314981   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 4)  new position  (7, 5)\n",
      "0.0 0 78  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 2.71882757  0.          6.85717525 25.5043576   4.09899906  5.77706887\n",
      " 15.15739062  0.          0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 5)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.05026425 -17.32444897   2.30127591 -27.37459091  11.25377427\n",
      "  -8.67030158 -20.14420072 -24.92887259   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 3)  new position  (0, 4)\n",
      "0.0 0 79  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 4)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-23.05959988   7.0209269    7.47867815  21.78457998  -2.23654234\n",
      "   0.84546426   2.24622199   0.           0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 4)  new position  (7, 3)\n",
      "0.0 0 80  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -4.05369449 -27.81453681 -10.92363383 -17.22001373   8.11765341\n",
      "  -0.11479772  -9.19944819 -16.72470014   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 3)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-26.5404509  -13.02524076   9.54924236 -14.01810807  -1.80573777\n",
      "  -2.61825893 -15.64057285 -20.49474233   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 3)  new position  (8, 2)\n",
      "0.0 0 81  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -3.0665162   21.51410773   1.3850607    4.84281287  -8.99647324\n",
      "  13.82377096  -1.54433096 -29.24939002   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 2)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-19.92364983  -6.02737552   0.          -0.19478609  -6.71756543\n",
      " -13.95623558  -6.60894064 -17.33049871   0.        ]\n",
      "max index list:  [2, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 4)  new position  (0, 4)\n",
      "0.0 0 82  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 17.02305732  17.82953209 -12.50005649  11.3381356    9.0313245\n",
      "   0.           0.          18.76358737   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 4)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.95006707  -4.38095309  -3.99171717 -22.57037976 -17.61884865\n",
      "  -1.14296873  -5.28443411  -7.7881217    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 3)  new position  (0, 2)\n",
      "0.0 0 83  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.83560386  16.51488306 -17.70132416   3.77501496   8.83634104\n",
      "   0.          -2.8711654  -13.4556378    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 2)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-10.73407729  12.07087137 -12.54406941  -6.95009791  -4.93792001\n",
      "  -6.09345984   2.77194754 -14.60714638   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 3)  new position  (0, 4)\n",
      "0.0 0 84  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 4)  new position  (9, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 1)  new position  (0, 0)\n",
      "0.0 0 85  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 5)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 0)  new position  (0, 9)\n",
      "0.0 0 86  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 4)  new position  (8, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 9)  new position  (1, 8)\n",
      "0.0 0 87  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 4)  new position  (9, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [29.11445124 22.74569532  0.         13.97051409  0.         27.76971107\n",
      " 26.97358949  2.41201194  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  4\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 8)  new position  (2, 9)\n",
      "0.0 0 88  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 9)  new position  (2, 8)\n",
      "0.0 0 89  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 6)  new position  (9, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 5.29326101 14.0343668   0.         12.76521282  0.         11.86681313\n",
      "  0.         23.36407537  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 8)  new position  (1, 8)\n",
      "0.0 0 90  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 5)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [29.11445124 22.74569532  0.         13.97051409  0.         27.76971107\n",
      " 26.97358949  2.41201194  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 8)  new position  (1, 9)\n",
      "0.0 0 91  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 4)  new position  (0, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [11.58237875 -6.20658786  2.99980652  2.47050577  5.14152991  6.87504739\n",
      "  1.44654251 -0.35355501  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 9)  new position  (2, 8)\n",
      "0.0 0 92  Episode:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 5.29326101 14.0343668   0.         12.76521282  0.         11.86681313\n",
      "  0.         23.36407537  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 8)  new position  (1, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 3)  new position  (9, 4)\n",
      "0.0 0 93  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 4)  new position  (0, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [11.58237875 -6.20658786  2.99980652  2.47050577  5.14152991  6.87504739\n",
      "  1.44654251 -0.35355501  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 9)  new position  (0, 9)\n",
      "0.0 0 94  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 4)  new position  (0, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 9)  new position  (1, 8)\n",
      "0.0 0 95  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [29.11445124 22.74569532  0.         13.97051409  0.         27.76971107\n",
      " 26.97358949  2.41201194  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 8)  new position  (2, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 3)  new position  (0, 4)\n",
      "0.0 0 96  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 9)  new position  (2, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 4)  new position  (1, 4)\n",
      "0.0 0 97  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.46722491 -1.77699393 -4.893463    0.         20.07332682 40.25356017\n",
      " 21.03019205  7.96800248  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 4)  new position  (0, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.13376359   0.55244619   0.82926859   6.67461715  20.98817125\n",
      "  15.01263039  -1.1080356  -12.02177202   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 0)  new position  (2, 1)\n",
      "0.0 0 98  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 1)  new position  (3, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 4)  new position  (9, 5)\n",
      "0.0 0 99  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 1)  new position  (3, 0)\n",
      "0.0 0 100  Episode:  3\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating trained mc agent with vision range  2\n",
      "creating trained mc agent with vision range  2\n",
      "Monte Carlo Episode  4\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 0)  new position  (6, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 5)  new position  (3, 6)\n",
      "0.0 0 1  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.90260846 -56.34286608 -32.56947316   0.         -36.81589887\n",
      "   0.         -46.41929819   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 6)  new position  (2, 6)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -4.24702509 -14.13072768   0.9504665    0.83209999  -1.26152662\n",
      "  12.25639219  -7.01730338   9.02650008   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 9)  new position  (7, 8)\n",
      "0.0 0 2  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 1.71163303  5.56985383  2.5412142  25.58548787  8.48147069  7.12962517\n",
      "  0.          1.7325241   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 8)  new position  (6, 8)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-0.4139219   0.          9.4992501   0.          7.46749503  0.\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 6)  new position  (3, 6)\n",
      "0.0 0 3  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [13.82148163 25.37561696  0.          8.2209153  18.64746376  7.86536258\n",
      "  0.         39.5350921   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 8)  new position  (5, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.90260846 -56.34286608 -32.56947316   0.         -36.81589887\n",
      "   0.         -46.41929819   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 6)  new position  (3, 5)\n",
      "0.0 0 4  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-6.66359182  6.41168705 -3.88023906 14.53598089  6.82718405  4.08742847\n",
      " -7.17129168  5.8474412   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 9)  new position  (6, 8)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 5)  new position  (2, 5)\n",
      "0.0 0 5  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-11.5691717    0.          18.83737322  12.54358916   7.61177462\n",
      " -18.91666582  -3.0443907    0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 5)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [13.82148163 25.37561696  0.          8.2209153  18.64746376  7.86536258\n",
      "  0.         39.5350921   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 8)  new position  (7, 9)\n",
      "0.0 0 6  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 9)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.82087061 -36.97603648 -40.37821211 -63.24831962 -35.21881822\n",
      " -46.65772308 -36.22761687 -27.25788312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 4)  new position  (2, 5)\n",
      "0.0 0 7  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 0)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-11.5691717    0.          18.83737322  12.54358916   7.61177462\n",
      " -18.91666582  -3.0443907    0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 5)  new position  (1, 4)\n",
      "0.0 0 8  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-0.84012175 -1.55122963  0.42301023  0.          0.64739084  4.86577667\n",
      " -0.17933864 -0.24602101  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 4)  new position  (0, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 1)  new position  (7, 2)\n",
      "0.0 0 9  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 2)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.37558439  8.56369179 26.48926408  0.91339227  9.54219903 -1.21212757\n",
      "  8.03918043  4.65296513  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 3)  new position  (9, 4)\n",
      "0.0 0 10  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 1)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-0.01981132 -3.15282673  2.24127581 -0.36445072  7.50199341 -4.41298077\n",
      " -4.77353582 -3.31684871  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 4)  new position  (9, 5)\n",
      "0.0 0 11  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 1.184       2.581184    0.24        0.          0.251136    0.248\n",
      "  1.24877722 -1.277312    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 5)  new position  (0, 4)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 0)  new position  (9, 1)\n",
      "0.0 0 12  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 1)  new position  (8, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-9.01071681 -4.41477086  8.96804753 19.98065796 -0.31706278  0.\n",
      "  8.42694419 13.0248104   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 4)  new position  (9, 4)\n",
      "0.0 0 13  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -3.0665162   21.51410773   1.3850607    4.84281287  -8.99647324\n",
      "  13.82377096  -1.54433096 -29.24939002   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 2)  new position  (7, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-20.73410821  -3.2286344  -17.67387294  11.1369929   -2.11261116\n",
      " -10.35721382   4.88551999  -9.6647841    0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 4)  new position  (8, 4)\n",
      "0.0 0 14  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-20.91455993  -9.78015452  -0.63137885 -32.01479603  -4.21282507\n",
      " -13.88670386  -6.83178996 -26.22832515   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 4)  new position  (7, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 2)  new position  (6, 3)\n",
      "0.0 0 15  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 2.29652011  0.          9.6149941  -1.86985702  8.27396573  8.32880033\n",
      " 22.76751897  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 5)  new position  (8, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.76985426  -3.00375504 -11.16410121  -5.55591313 -16.40064535\n",
      "  -6.64537213  -2.24725537  11.25184291   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 3)  new position  (6, 2)\n",
      "0.0 0 16  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-31.81780201   1.33785236 -21.69686867  -5.25796577 -12.9060176\n",
      " -14.07674145 -24.63764734 -16.80723375   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 2)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-14.67429578 -12.24215203 -20.5801667  -17.51072018   9.35698949\n",
      "   6.0546727   -1.49622716 -14.22890975   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 4)  new position  (8, 5)\n",
      "0.0 0 17  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-30.9784088  -18.13056239   1.39515796   2.13181365  -0.86297513\n",
      " -25.81614103 -10.26975119   4.1250176    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 3)  new position  (7, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 5)  new position  (8, 6)\n",
      "0.0 0 18  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-19.25776765 -16.71693457   1.14421871  -7.78298141 -19.78017513\n",
      "   2.83826722 -12.45137718   6.02343751   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 6)  new position  (9, 7)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 2)  new position  (6, 1)\n",
      "0.0 0 19  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 7)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 1)  new position  (5, 1)\n",
      "0.0 0 20  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 1)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  5\n",
      "index max:  4\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 7)  new position  (1, 8)\n",
      "0.0 0 21  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [29.11445124 22.74569532  0.         13.97051409  0.         27.76971107\n",
      " 26.97358949  2.41201194  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 8)  new position  (2, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 0)  new position  (5, 1)\n",
      "0.0 0 22  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 1)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 5.29326101 14.0343668   0.         12.76521282  0.         11.86681313\n",
      "  0.         23.36407537  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 8)  new position  (3, 8)\n",
      "0.0 0 23  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 0)  new position  (6, 9)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -9.87839345 -15.1274594    0.          -2.17515231   0.\n",
      "  -1.58522554   0.         -14.75501756   0.        ]\n",
      "max index list:  [2, 4, 6, 8]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 8)  new position  (3, 9)\n",
      "0.0 0 24  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -4.24702509 -14.13072768   0.9504665    0.83209999  -1.26152662\n",
      "  12.25639219  -7.01730338   9.02650008   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 9)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 9)  new position  (4, 9)\n",
      "0.0 0 25  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 9)  new position  (4, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 0)  new position  (8, 0)\n",
      "0.0 0 26  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-14.89427287 -22.08880153   0.           3.64553484   0.\n",
      "  -1.30884592   0.         -31.87859896   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 8)  new position  (3, 8)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 0)  new position  (7, 0)\n",
      "0.0 0 27  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 0)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -9.87839345 -15.1274594    0.          -2.17515231   0.\n",
      "  -1.58522554   0.         -14.75501756   0.        ]\n",
      "max index list:  [2, 4, 6, 8]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 8)  new position  (3, 9)\n",
      "0.0 0 28  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 9)  new position  (4, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-4.18335116 -6.50833232 -0.10193288 -6.35441189  1.81551866  9.02198876\n",
      "  1.45591218 -4.31384391  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 0)  new position  (6, 1)\n",
      "0.0 0 29  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-14.89427287 -22.08880153   0.           3.64553484   0.\n",
      "  -1.30884592   0.         -31.87859896   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 8)  new position  (4, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [20.07564083 -9.15676338  7.30082377  0.25521104  7.28019748  3.61257055\n",
      " -2.94978238 -1.71021064  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 1)  new position  (6, 0)\n",
      "0.0 0 30  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-0.67454163  5.33699703 -9.46563734 10.04362749 -3.25205121 12.6057817\n",
      "  6.87137999  6.45783243  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 0)  new position  (5, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.66000822 -10.62178918   2.78230243  -9.31694118   3.80302373\n",
      "   0.           4.28068835   3.95679136   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 9)  new position  (3, 0)\n",
      "0.0 0 31  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.13912114   0.83464041 -11.32736652  -8.18631766   5.75526849\n",
      "  20.50637939 -16.52722114  -6.76210856   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 0)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 0.    -2.496  0.     0.     0.     0.64  -1.584  0.     0.   ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 0)  new position  (4, 9)\n",
      "0.0 0 32  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.12144303 -13.31383119  -2.90995225 -10.23892848   2.36628746\n",
      "  19.62298119   0.73705909 -13.55450326   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 1)  new position  (4, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 9)  new position  (3, 0)\n",
      "0.0 0 33  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-22.59013011  -2.33855026  -8.43396235 -12.54207333   0.70728622\n",
      "   2.11721595  -2.06098719  14.55506964   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 0)  new position  (3, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -2.87063516   5.10900426  -3.08859228  10.81976439 -13.35214313\n",
      "  -4.12332322   0.           6.00178503   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 2)  new position  (3, 2)\n",
      "0.0 0 34  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  6.03652011  -1.935919     0.          25.23014016 -12.08743599\n",
      "  -1.04070451 -32.02228412   8.90803424   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 2)  new position  (2, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -7.84226385   0.           6.64526097   1.32925885 -10.07356362\n",
      "  -4.18655331 -16.78877078 -15.27514016   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 1)  new position  (4, 2)\n",
      "0.0 0 35  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -3.25366358  -8.96824651  -7.12079596 -14.9716126    6.00047077\n",
      "   0.69560256 -14.38906026  -9.15033527   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 1)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 2)  new position  (5, 2)\n",
      "0.0 0 36  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 1)  new position  (2, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 2)  new position  (6, 2)\n",
      "0.0 0 37  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 1)  new position  (2, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 2)  new position  (7, 1)\n",
      "0.0 0 38  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 2)  new position  (2, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 1)  new position  (6, 1)\n",
      "0.0 0 39  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 1)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 1)  new position  (6, 2)\n",
      "0.0 0 40  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 1)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 2)  new position  (5, 2)\n",
      "0.0 0 41  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 0)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 2)  new position  (4, 1)\n",
      "0.0 0 42  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.91631167  -9.40148994  -9.11867023 -16.11964308 -14.68625626\n",
      "  15.86562523 -10.67987709  -2.1308996    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 1)  new position  (4, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 0)  new position  (0, 9)\n",
      "0.0 0 43  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 9)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 2)  new position  (5, 2)\n",
      "0.0 0 44  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 8)  new position  (9, 7)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 2)  new position  (5, 1)\n",
      "0.0 0 45  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 1)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 7)  new position  (0, 8)\n",
      "0.0 0 46  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 2)  new position  (5, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30851671  15.10050741   3.98786117  14.30216312   0.\n",
      "  -0.27077046 -11.8440253    6.7174165    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 8)  new position  (9, 7)\n",
      "0.0 0 47  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 7)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.00746759 -11.1412527  -19.62385086  -1.01596757  -1.60346104\n",
      "  15.51659352 -10.01204177  -5.37875616   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 3)  new position  (5, 2)\n",
      "0.0 0 48  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 8)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 2)  new position  (4, 2)\n",
      "0.0 0 49  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 9)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 2)  new position  (4, 3)\n",
      "0.0 0 50  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 9)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 3)  new position  (3, 3)\n",
      "0.0 0 51  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 3)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 8)  new position  (0, 9)\n",
      "0.0 0 52  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 9)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 4)  new position  (3, 5)\n",
      "0.0 0 53  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 0)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 5)  new position  (2, 6)\n",
      "0.0 0 54  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 0)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-0.4139219   0.          9.4992501   0.          7.46749503  0.\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 6)  new position  (2, 5)\n",
      "0.0 0 55  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 9)  new position  (8, 8)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-11.5691717    0.          18.83737322  12.54358916   7.61177462\n",
      " -18.91666582  -3.0443907    0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 5)  new position  (3, 5)\n",
      "0.0 0 56  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 8)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 5)  new position  (3, 6)\n",
      "0.0 0 57  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 9)  new position  (8, 8)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.90260846 -56.34286608 -32.56947316   0.         -36.81589887\n",
      "   0.         -46.41929819   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 6)  new position  (2, 5)\n",
      "0.0 0 58  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 8)  new position  (8, 7)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-11.5691717    0.          18.83737322  12.54358916   7.61177462\n",
      " -18.91666582  -3.0443907    0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 5)  new position  (2, 4)\n",
      "0.0 0 59  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 4)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-10.07688183 -28.80657813  -8.45533959  -0.51490706  -5.02334143\n",
      "  27.05083553 -18.10199168  -4.02836738   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 7)  new position  (9, 7)\n",
      "0.0 0 60  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 5)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 7)  new position  (0, 8)\n",
      "0.0 0 61  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-11.5691717    0.          18.83737322  12.54358916   7.61177462\n",
      " -18.91666582  -3.0443907    0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 5)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30851671  15.10050741   3.98786117  14.30216312   0.\n",
      "  -0.27077046 -11.8440253    6.7174165    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 8)  new position  (9, 7)\n",
      "0.0 0 62  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 7)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 5)  new position  (3, 6)\n",
      "0.0 0 63  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30851671  15.10050741   3.98786117  14.30216312   0.\n",
      "  -0.27077046 -11.8440253    6.7174165    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 8)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.90260846 -56.34286608 -32.56947316   0.         -36.81589887\n",
      "   0.         -46.41929819   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 6)  new position  (4, 6)\n",
      "0.0 0 64  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 9)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 6)  new position  (5, 6)\n",
      "0.0 0 65  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         10.6286564  18.21014817  0.          0.          0.\n",
      " 10.65163663  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 6)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 0)  new position  (9, 9)\n",
      "0.0 0 66  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 9)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 6)  new position  (4, 5)\n",
      "0.0 0 67  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 0)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 5)  new position  (5, 5)\n",
      "0.0 0 68  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [8.66552923 0.26758858 5.26688414 3.6698233  8.73338803 7.48075658\n",
      " 1.51378335 9.49805923 0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 0)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 5)  new position  (4, 6)\n",
      "0.0 0 69  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 6)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 9)  new position  (8, 8)\n",
      "0.0 0 70  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 5)  new position  (5, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 8)  new position  (7, 8)\n",
      "0.0 0 71  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.16729125  3.7889746   0.          0.          0.\n",
      " 15.3142292   0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 6)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 1.71163303  5.56985383  2.5412142  25.58548787  8.48147069  7.12962517\n",
      "  0.          1.7325241   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 8)  new position  (7, 9)\n",
      "0.0 0 72  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 6)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 9)  new position  (8, 0)\n",
      "0.0 0 73  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 0)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 5)  new position  (2, 4)\n",
      "0.0 0 74  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-5.26142413 -3.85586743 -3.074176   -0.03777434 -6.36471755 -1.1868247\n",
      "  0.97566874  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 4)  new position  (2, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 9)  new position  (7, 0)\n",
      "0.0 0 75  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -4.99900466  -0.94666676  -9.07740695  -7.11623038  -5.15010247\n",
      "  -8.596438   -12.97535374   5.53657469   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 3)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 0)  new position  (7, 1)\n",
      "0.0 0 76  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 4)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 1)  new position  (6, 1)\n",
      "0.0 0 77  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.82087061 -36.97603648 -40.37821211 -63.24831962 -35.21881822\n",
      " -46.65772308 -36.22761687 -27.25788312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 4)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 1)  new position  (7, 2)\n",
      "0.0 0 78  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 4)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 2)  new position  (7, 1)\n",
      "0.0 0 79  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 1)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.82087061 -36.97603648 -40.37821211 -63.24831962 -35.21881822\n",
      " -46.65772308 -36.22761687 -27.25788312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 4)  new position  (2, 3)\n",
      "0.0 0 80  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.68256275 -16.7174098    2.02334361 -19.31482151  10.02558202\n",
      "  10.69528709  12.2001919    2.2586472    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 3)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 0)  new position  (9, 9)\n",
      "0.0 0 81  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 9)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 4)  new position  (1, 4)\n",
      "0.0 0 82  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30851671  15.10050741   3.98786117  14.30216312   0.\n",
      "  -0.27077046 -11.8440253    6.7174165    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 8)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.46722491 -1.77699393 -4.893463    0.         20.07332682 40.25356017\n",
      " 21.03019205  7.96800248  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 4)  new position  (0, 3)\n",
      "0.0 0 83  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  3\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 7)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 3)  new position  (9, 4)\n",
      "0.0 0 84  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -7.72906    -44.00486623  10.02028528   1.97634447 -21.54110801\n",
      "  -6.86092079 -16.44925105  -2.22759344   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 4)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -4.1142942   -1.38563688   1.75166024 -12.33386373   0.\n",
      "  -7.4986089  -24.38383823  -6.09913154   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 6)  new position  (0, 7)\n",
      "0.0 0 85  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -6.7799212   9.27118107 -2.31061223  6.23764537  0.\n",
      " 10.52735415 32.07100339  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 5)  new position  (0, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 7)  new position  (9, 8)\n",
      "0.0 0 86  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 4)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 8)  new position  (8, 8)\n",
      "0.0 0 87  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.46722491 -1.77699393 -4.893463    0.         20.07332682 40.25356017\n",
      " 21.03019205  7.96800248  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 4)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 8)  new position  (7, 8)\n",
      "0.0 0 88  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 1.71163303  5.56985383  2.5412142  25.58548787  8.48147069  7.12962517\n",
      "  0.          1.7325241   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 8)  new position  (8, 8)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 5)  new position  (9, 4)\n",
      "0.0 0 89  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 8)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 4)  new position  (8, 5)\n",
      "0.0 0 90  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 9)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 5)  new position  (7, 5)\n",
      "0.0 0 91  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 0)  new position  (6, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.52939684   0.         -16.17025784 -20.99596012 -43.77061943\n",
      " -17.78290425  10.17861875   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 5)  new position  (8, 4)\n",
      "0.0 0 92  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -4.24702509 -14.13072768   0.9504665    0.83209999  -1.26152662\n",
      "  12.25639219  -7.01730338   9.02650008   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 9)  new position  (5, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 4)  new position  (8, 3)\n",
      "0.0 0 93  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 3)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-6.66359182  6.41168705 -3.88023906 14.53598089  6.82718405  4.08742847\n",
      " -7.17129168  5.8474412   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 9)  new position  (4, 8)\n",
      "0.0 0 94  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-14.89427287 -22.08880153   0.           3.64553484   0.\n",
      "  -1.30884592   0.         -31.87859896   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  3\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 8)  new position  (5, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 4)  new position  (9, 5)\n",
      "0.0 0 95  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 5)  new position  (8, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-6.66359182  6.41168705 -3.88023906 14.53598089  6.82718405  4.08742847\n",
      " -7.17129168  5.8474412   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 9)  new position  (4, 8)\n",
      "0.0 0 96  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 5)  new position  (9, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-14.89427287 -22.08880153   0.           3.64553484   0.\n",
      "  -1.30884592   0.         -31.87859896   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 8)  new position  (3, 8)\n",
      "0.0 0 97  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 5)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -9.87839345 -15.1274594    0.          -2.17515231   0.\n",
      "  -1.58522554   0.         -14.75501756   0.        ]\n",
      "max index list:  [2, 4, 6, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 8)  new position  (2, 8)\n",
      "0.0 0 98  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 4)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 5.29326101 14.0343668   0.         12.76521282  0.         11.86681313\n",
      "  0.         23.36407537  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 8)  new position  (2, 9)\n",
      "0.0 0 99  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 9)  new position  (2, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 5)  new position  (9, 4)\n",
      "0.0 0 100  Episode:  4\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating trained mc agent with vision range  2\n",
      "creating trained mc agent with vision range  2\n",
      "Monte Carlo Episode  5\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.27320786   4.36423413   0.         -10.88148942 -14.31594885\n",
      "  -4.0239842    0.          -3.50424995   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 8)  new position  (7, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 2)  new position  (1, 3)\n",
      "0.0 0 1  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 3)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.86126497  -7.64853507 -24.73028016 -22.18007293 -20.5990663\n",
      " -15.40596975   0.         -25.58865073   0.        ]\n",
      "max index list:  [6, 8]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 8)  new position  (8, 9)\n",
      "0.0 0 2  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.46722491 -1.77699393 -4.893463    0.         20.07332682 40.25356017\n",
      " 21.03019205  7.96800248  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (1, 4)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 9)  new position  (9, 9)\n",
      "0.0 0 3  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 9)  new position  (0, 9)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 3)  new position  (2, 3)\n",
      "0.0 0 4  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.68256275 -16.7174098    2.02334361 -19.31482151  10.02558202\n",
      "  10.69528709  12.2001919    2.2586472    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 3)  new position  (2, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 9)  new position  (0, 0)\n",
      "0.0 0 5  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -5.22280124   4.89087723  -6.17603464   7.59289108  24.78183166\n",
      " -12.42891233  -8.59235052  16.77685299   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 2)  new position  (2, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 0)  new position  (1, 0)\n",
      "0.0 0 6  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [13.12601014 12.14844637 -1.10361793 14.52261584 -4.11906788 -0.46161736\n",
      "  7.60447512 -0.15495821  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 0)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-21.22575547   1.51275235   5.69740192  25.06792769  -1.14242336\n",
      " -20.54246056 -28.51284053 -25.17277316   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 3)  new position  (2, 4)\n",
      "0.0 0 7  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [14.38368477 -0.91408419  4.00528915  5.31654658 -1.569726   37.6939877\n",
      " -9.63336455  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 4)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-16.60642732  13.46288183 -11.72264408   6.2421417   11.99764665\n",
      "  -3.6694549    4.2009393    3.37930887   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 1)  new position  (2, 1)\n",
      "0.0 0 8  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  3.16246687  -4.2555943   -5.98525881   1.07319825 -22.19520145\n",
      "   6.18172314  -6.09282438 -20.06809951   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (1, 3)  new position  (1, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.51509279   7.04288376   1.43097087 -24.62435608  -0.95985762\n",
      "  -8.62278237  15.54993388   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 1)  new position  (3, 1)\n",
      "0.0 0 9  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.04561739  -2.55035688 -17.36678065  -1.38642189 -10.52336774\n",
      "  -5.21183316  17.46009159   2.69330674   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 2)  new position  (2, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.67600607   2.27227481   1.56738192   7.42320844   5.60532073\n",
      " -10.59945639   4.64887718   0.           0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 1)  new position  (4, 0)\n",
      "0.0 0 10  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-21.12985421  -0.29172097  -7.42821738   6.19456088  -4.78367848\n",
      "  22.92019964   8.06453159  12.25498963   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 2)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.78690545  6.15444352  3.85395187 -0.15385486 -5.38010634 15.5076787\n",
      " 14.73589801 13.31404824  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 0)  new position  (3, 0)\n",
      "0.0 0 11  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 3)  new position  (2, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 31.284968    -4.82813909 -10.67881149  -1.93536291   4.62396222\n",
      "   6.19813269  -5.04263151  -6.92183961   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 0)  new position  (4, 9)\n",
      "0.0 0 12  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 9)  new position  (5, 0)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.68256275 -16.7174098    2.02334361 -19.31482151  10.02558202\n",
      "  10.69528709  12.2001919    2.2586472    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 3)  new position  (1, 3)\n",
      "0.0 0 13  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  2.42480516   9.42987775  30.39471773   5.94875575   2.64087889\n",
      "  -3.12714747  10.76768182 -11.53038458   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 0)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (1, 3)  new position  (1, 2)\n",
      "0.0 0 14  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 0)  new position  (5, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.66282809  -4.26618692   4.0766115  -11.41362679  10.69348545\n",
      "  11.03964625   1.67209256  -9.43224976   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 2)  new position  (1, 3)\n",
      "0.0 0 15  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 0)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 3)  new position  (0, 2)\n",
      "0.0 0 16  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 2)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 0)  new position  (5, 9)\n",
      "0.0 0 17  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 3)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q state is  [-6.66359182  6.41168705 -3.88023906 14.53598089  6.82718405  4.08742847\n",
      " -7.17129168  5.8474412   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 9)  new position  (5, 8)\n",
      "0.0 0 18  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 2)  new position  (0, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [16.46582943 24.91244294  0.         24.96752822  0.         18.96689488\n",
      "  0.         34.79674133  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 8)  new position  (6, 8)\n",
      "0.0 0 19  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.27320786   4.36423413   0.         -10.88148942 -14.31594885\n",
      "  -4.0239842    0.          -3.50424995   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 8)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 1)  new position  (0, 0)\n",
      "0.0 0 20  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 9)  new position  (6, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 0)  new position  (0, 9)\n",
      "0.0 0 21  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 9)  new position  (1, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -4.24702509 -14.13072768   0.9504665    0.83209999  -1.26152662\n",
      "  12.25639219  -7.01730338   9.02650008   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 9)  new position  (5, 9)\n",
      "0.0 0 22  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-6.66359182  6.41168705 -3.88023906 14.53598089  6.82718405  4.08742847\n",
      " -7.17129168  5.8474412   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 9)  new position  (4, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [13.12601014 12.14844637 -1.10361793 14.52261584 -4.11906788 -0.46161736\n",
      "  7.60447512 -0.15495821  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 0)  new position  (0, 0)\n",
      "0.0 0 23  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 0)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 9)  new position  (4, 0)\n",
      "0.0 0 24  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.78690545  6.15444352  3.85395187 -0.15385486 -5.38010634 15.5076787\n",
      " 14.73589801 13.31404824  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 0)  new position  (3, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 0)  new position  (9, 9)\n",
      "0.0 0 25  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 9)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 9)  new position  (4, 9)\n",
      "0.0 0 26  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 9)  new position  (5, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 0)  new position  (9, 9)\n",
      "0.0 0 27  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 9)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [16.46582943 24.91244294  0.         24.96752822  0.         18.96689488\n",
      "  0.         34.79674133  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 8)  new position  (4, 8)\n",
      "0.0 0 28  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [81.24221728 85.84675297  0.         65.47966762  0.         67.96030225\n",
      "  0.         92.97428663  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 8)  new position  (4, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30851671  15.10050741   3.98786117  14.30216312   0.\n",
      "  -0.27077046 -11.8440253    6.7174165    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 8)  new position  (9, 9)\n",
      "0.0 0 29  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 9)  new position  (3, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 9)  new position  (9, 8)\n",
      "0.0 0 30  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 8)  new position  (8, 8)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [74.2936648  87.63956574  0.         75.23779423  0.         75.39360634\n",
      "  0.         67.21443747  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 8)  new position  (3, 9)\n",
      "0.0 0 31  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 9)  new position  (2, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 8)  new position  (7, 9)\n",
      "0.0 0 32  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 9)  new position  (3, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 9)  new position  (7, 8)\n",
      "0.0 0 33  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [74.2936648  87.63956574  0.         75.23779423  0.         75.39360634\n",
      "  0.         67.21443747  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 8)  new position  (3, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.86126497  -7.64853507 -24.73028016 -22.18007293 -20.5990663\n",
      " -15.40596975   0.         -25.58865073   0.        ]\n",
      "max index list:  [6, 8]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 8)  new position  (6, 9)\n",
      "0.0 0 34  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 9)  new position  (3, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q state is  [ -4.24702509 -14.13072768   0.9504665    0.83209999  -1.26152662\n",
      "  12.25639219  -7.01730338   9.02650008   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 9)  new position  (5, 9)\n",
      "0.0 0 35  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [8.50771729 1.85460332 0.         7.79181093 0.         5.04543571\n",
      " 0.         9.26344768 0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  3\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 8)  new position  (4, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.28204336   0.          12.16527866   8.92107518   0.84794853\n",
      "  -6.82101423 -24.43248168   4.39280981   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 9)  new position  (5, 8)\n",
      "0.0 0 36  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 0.61616161 -2.43530458  0.         -1.24675511  0.         -0.26604054\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 8)  new position  (4, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.23533225 -14.56548596   0.          -1.03022995 -12.44924079\n",
      "  -6.14849322  -8.40453165  -6.01755621   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 9)  new position  (3, 0)\n",
      "0.0 0 37  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-0.44255365 26.90095058  0.         17.35525874  0.          8.9529639\n",
      "  0.         12.10225023  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 8)  new position  (3, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -5.62794129   2.52266167   0.         -10.03406338   0.27046152\n",
      " -14.31596012  -2.14496636   7.45462791   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 0)  new position  (2, 0)\n",
      "0.0 0 38  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -2.88416868 -17.09181984 -17.2521028  -12.59152919 -12.4199031\n",
      "   6.46421043   8.54724162   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 9)  new position  (3, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-20.50924166  11.73547366 -14.91978854   9.21874192  -3.06856505\n",
      "   8.31749096  -8.01859952  -1.31143641   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 0)  new position  (1, 1)\n",
      "0.0 0 39  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 1)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [74.2936648  87.63956574  0.         75.23779423  0.         75.39360634\n",
      "  0.         67.21443747  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 8)  new position  (2, 8)\n",
      "0.0 0 40  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-4.5650891   9.37310176 14.84500126 -2.60756469 -3.01856337 -2.300997\n",
      " -1.60475032  1.72226795  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 0)  new position  (0, 9)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [19.40617485 -7.40097178  0.          8.41840653  0.         -1.88140427\n",
      "  0.         -4.7580949   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 8)  new position  (2, 9)\n",
      "0.0 0 41  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-5.23848019 -2.12208181  4.25686363  4.97995692  3.22954115 -4.80730767\n",
      " 17.47871887 -3.32387793  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 9)  new position  (1, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 5.79521161 -7.24602349  1.33015351 -5.63551014 -9.78642208 -7.88406712\n",
      "  7.45884596  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 9)  new position  (3, 9)\n",
      "0.0 0 42  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-2.70184458 -2.40104271 -2.00156315  5.76629199 -3.03096892  2.93113289\n",
      " -3.80153292 -7.30870592  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 9)  new position  (4, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [13.12601014 12.14844637 -1.10361793 14.52261584 -4.11906788 -0.46161736\n",
      "  7.60447512 -0.15495821  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (1, 0)  new position  (1, 9)\n",
      "0.0 0 43  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [11.58237875 -6.20658786  2.99980652  2.47050577  5.14152991  6.87504739\n",
      "  1.44654251 -0.35355501  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 9)  new position  (2, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [15.20237533 14.27837022  0.          4.07323423  0.          9.8308099\n",
      "  0.         10.82532658  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  4\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 8)  new position  (3, 9)\n",
      "0.0 0 44  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -2.88416868 -17.09181984 -17.2521028  -12.59152919 -12.4199031\n",
      "   6.46421043   8.54724162   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 9)  new position  (4, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  9.1808464   -7.23356976   8.70554325 -20.81015331   8.14614145\n",
      "  -6.78769377   3.80646142  -0.55115739   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 0)  new position  (2, 1)\n",
      "0.0 0 45  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.08061546 -12.10875858   8.85291109  -4.37409183  -5.99558989\n",
      "  -7.93831035  -0.63304037  -1.55425411   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 9)  new position  (4, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.75898308   6.05892026 -26.56687427   2.60840564   8.51629104\n",
      " -20.89425623  -8.6135406   -0.97179555   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 1)  new position  (3, 2)\n",
      "0.0 0 46  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-14.64393789   3.88868713 -10.65004757  -8.92902921 -22.27403017\n",
      "  -5.85184405 -17.87599725   3.48081316   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 0)  new position  (3, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-14.85681829   7.13171334  -4.05269289  -3.20556523  -9.85409792\n",
      " -11.43089404   1.90990563  10.8935385    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 2)  new position  (3, 3)\n",
      "0.0 0 47  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 3)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 31.284968    -4.82813909 -10.67881149  -1.93536291   4.62396222\n",
      "   6.19813269  -5.04263151  -6.92183961   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 0)  new position  (3, 1)\n",
      "0.0 0 48  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 17.53286833  -7.49103994   8.97148714 -13.54099556  26.95985594\n",
      "  -8.87711545  -4.32674378   8.48385821   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 1)  new position  (3, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -0.61479829 -10.77536034  -9.13684454  16.23209759 -18.15831393\n",
      "  15.47442106   0.         -18.05433243   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 3)  new position  (5, 2)\n",
      "0.0 0 49  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-19.66219417  14.54149983  -8.98235936 -25.36532062  -2.92176106\n",
      "  -9.88454157 -13.74584196   5.93457698   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 2)  new position  (4, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.48519797 20.13308057  7.07143589  3.41937367 10.50540363 -4.2761765\n",
      "  0.         -6.71976669  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 2)  new position  (6, 2)\n",
      "0.0 0 50  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 6.21991836 -1.07555454  3.57067246 14.5757595  12.97366571 -6.57466007\n",
      " -6.03747083 10.04090553  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 1)  new position  (3, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 2)  new position  (7, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 51  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 3)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 31.284968    -4.82813909 -10.67881149  -1.93536291   4.62396222\n",
      "   6.19813269  -5.04263151  -6.92183961   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 0)  new position  (2, 9)\n",
      "0.0 0 52  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-13.97820741 -34.50005644 -33.08951058 -42.95138697 -21.4127722\n",
      " -25.63012661 -23.53734744   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 4)  new position  (8, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 9)  new position  (2, 8)\n",
      "0.0 0 53  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 5.29326101 14.0343668   0.         12.76521282  0.         11.86681313\n",
      "  0.         23.36407537  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 8)  new position  (1, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 4)  new position  (9, 4)\n",
      "0.0 0 54  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [11.58237875 -6.20658786  2.99980652  2.47050577  5.14152991  6.87504739\n",
      "  1.44654251 -0.35355501  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 9)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 4)  new position  (8, 3)\n",
      "0.0 0 55  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30851671  15.10050741   3.98786117  14.30216312   0.\n",
      "  -0.27077046 -11.8440253    6.7174165    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 8)  new position  (1, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 3)  new position  (8, 2)\n",
      "0.0 0 56  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-5.88136669  0.62307256  8.34130486  2.36110733 11.57893202 19.11781084\n",
      "  7.38516108 13.80266182  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 2)  new position  (8, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [29.11445124 22.74569532  0.         13.97051409  0.         27.76971107\n",
      " 26.97358949  2.41201194  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 8)  new position  (2, 8)\n",
      "0.0 0 57  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 5.29326101 14.0343668   0.         12.76521282  0.         11.86681313\n",
      "  0.         23.36407537  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  3\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 8)  new position  (3, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 3)  new position  (7, 2)\n",
      "0.0 0 58  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 2)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 9)  new position  (2, 9)\n",
      "0.0 0 59  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 1)  new position  (8, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 9)  new position  (3, 0)\n",
      "0.0 0 60  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 31.284968    -4.82813909 -10.67881149  -1.93536291   4.62396222\n",
      "   6.19813269  -5.04263151  -6.92183961   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 0)  new position  (3, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 24.28279917  -3.2503315   -3.4938431    7.46755294 -10.64272157\n",
      "  -9.66022703   0.34289003   0.44213197   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 1)  new position  (7, 2)\n",
      "0.0 0 61  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 1)  new position  (4, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 2)  new position  (6, 1)\n",
      "0.0 0 62  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [27.33902184 14.29120747 18.4943025  13.36447315  3.63661903 20.81798963\n",
      "  7.11995901  7.94931904  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 1)  new position  (7, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.78690545  6.15444352  3.85395187 -0.15385486 -5.38010634 15.5076787\n",
      " 14.73589801 13.31404824  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 0)  new position  (5, 0)\n",
      "0.0 0 63  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  7.80033294   0.76884812   0.06095548 -20.45278573  15.88022859\n",
      "  -6.46248773  11.96708192 -12.77358072   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 2)  new position  (8, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 0)  new position  (6, 9)\n",
      "0.0 0 64  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -4.24702509 -14.13072768   0.9504665    0.83209999  -1.26152662\n",
      "  12.25639219  -7.01730338   9.02650008   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 9)  new position  (7, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 2)  new position  (8, 3)\n",
      "0.0 0 65  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.86126497  -7.64853507 -24.73028016 -22.18007293 -20.5990663\n",
      " -15.40596975   0.         -25.58865073   0.        ]\n",
      "max index list:  [6, 8]\n",
      "max_action:  4\n",
      "index max:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 8)  new position  (8, 7)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 3)  new position  (7, 2)\n",
      "0.0 0 66  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-0.02111993 -2.90858093 -1.03643272  4.54336683  9.28846005 -6.46754869\n",
      " -5.35658392  1.12867418  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 7)  new position  (9, 7)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 2)  new position  (7, 3)\n",
      "0.0 0 67  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 3)  new position  (7, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 2.69528234 -2.01177643  4.48753864 -3.20515438 -5.42035199  2.12164019\n",
      "  0.2877118  -0.72797479  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 7)  new position  (9, 8)\n",
      "0.0 0 68  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-1.22411488  3.43565015 -2.97412836  0.92114095  4.32256453  0.45827862\n",
      " -3.55555835 -3.72591111  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 8)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 2)  new position  (8, 2)\n",
      "0.0 0 69  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-2.78740822 -1.49912675  1.36908556  6.94465166  0.          1.00046379\n",
      "  3.34105265 -5.2466785   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 8)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 2)  new position  (9, 3)\n",
      "0.0 0 70  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 9)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 3)  new position  (9, 4)\n",
      "0.0 0 71  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30851671  15.10050741   3.98786117  14.30216312   0.\n",
      "  -0.27077046 -11.8440253    6.7174165    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 8)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 4)  new position  (9, 5)\n",
      "0.0 0 72  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 8)  new position  (9, 7)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -2.61253101 -21.70192118  -7.84621048 -19.97095195 -30.69715729\n",
      " -26.85147421   0.82900865 -12.12125421   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 5)  new position  (8, 4)\n",
      "0.0 0 73  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 7)  new position  (8, 7)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 4)  new position  (9, 5)\n",
      "0.0 0 74  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-23.43821096   2.42321089 -16.87114662  -8.27538674 -13.4216809\n",
      "  -6.48710346  -8.99260968 -11.99114337   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 7)  new position  (8, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-16.12196777 -11.50532382 -14.6371846   -9.99033582 -12.90899084\n",
      "  -6.87889305   1.44988238   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 5)  new position  (0, 4)\n",
      "0.0 0 75  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-3.37742914 -4.2078381   6.43016987  5.39300112 -9.73359225  0.\n",
      "  2.05862047  0.54717556  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 4)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-19.25776765 -16.71693457   1.14421871  -7.78298141 -19.78017513\n",
      "   2.83826722 -12.45137718   6.02343751   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 6)  new position  (9, 7)\n",
      "0.0 0 76  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 3)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 7)  new position  (8, 8)\n",
      "0.0 0 77  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 2)  new position  (9, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 8)  new position  (7, 8)\n",
      "0.0 0 78  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 1)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -6.55305587 -10.49926428 -15.19595365  -6.42881243  -6.00586135\n",
      "  -2.61379888   0.          -4.4453086    0.        ]\n",
      "max index list:  [6, 8]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 8)  new position  (8, 9)\n",
      "0.0 0 79  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [ -3.04498307 -12.90930061   0.           6.94320617   5.65680093\n",
      "  -6.12498054   2.86118696   4.94982631   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 0)  new position  (9, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 7.16034593  1.2712861  -0.19557753 -2.90778006 -7.45055835  1.38501192\n",
      "  6.82334711  6.35741638  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 9)  new position  (9, 8)\n",
      "0.0 0 80  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 1)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.90552927  -8.8917334  -22.94309302  -6.01524759   5.08220054\n",
      " -12.04637986   7.96967495  15.55758489   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 8)  new position  (9, 7)\n",
      "0.0 0 81  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 7)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q state is  [ -7.38270594 -10.37359532  -1.24613361 -31.32755166  -3.79228522\n",
      "   3.33454445   0.72222754 -13.56420957   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 0)  new position  (0, 9)\n",
      "0.0 0 82  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-11.82597347   7.13422772  -4.43119044  10.73765427   1.43941722\n",
      "   2.69332272   0.          -9.02054588   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 9)  new position  (1, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-2.73911918 -0.63466267 -8.55332762 -1.63424877  5.75231488 -4.50218088\n",
      " -8.29436922 13.12541864  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 8)  new position  (8, 8)\n",
      "0.0 0 83  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [29.11445124 22.74569532  0.         13.97051409  0.         27.76971107\n",
      " 26.97358949  2.41201194  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  3\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 8)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-14.70144806  -3.69114136   0.28105784  19.79670524 -20.26574256\n",
      "   2.08170483 -17.93206496   2.79496359   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 8)  new position  (9, 9)\n",
      "0.0 0 84  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 0.          6.1197951  -7.97611801  4.40420518  0.          9.89820805\n",
      " -6.23728781 -7.7660731   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 7)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  1.64174119   2.79872355  -7.05101133   3.68750485   0.\n",
      " -29.84772796   8.43311924  -0.2155205    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 9)  new position  (8, 0)\n",
      "0.0 0 85  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.44683629 -10.95320976  13.0463705   11.12080254   0.\n",
      "  11.92643533  -1.980986     2.72050688   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 8)  new position  (1, 8)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 0)  new position  (7, 9)\n",
      "0.0 0 86  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 9)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [29.11445124 22.74569532  0.         13.97051409  0.         27.76971107\n",
      " 26.97358949  2.41201194  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 8)  new position  (0, 9)\n",
      "0.0 0 87  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [  3.70190658   5.76489599  -3.67925267   4.38045146  -4.66954598\n",
      " -13.10590594   7.93243354   0.31463805   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 0)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  9.83920353   0.          -3.04679845   0.70234783  -0.23102789\n",
      "   6.00966162 -26.77607257  -1.36350117   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 9)  new position  (9, 8)\n",
      "0.0 0 88  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -2.80356564 -26.91569072   0.          -4.88088474  -5.61840494\n",
      " -12.73124762  -4.50120113 -11.60416826   0.        ]\n",
      "max index list:  [2, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 9)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -7.4233308  -20.57162345 -19.60130136   1.06724913   9.69366678\n",
      "  -2.65229539  -9.68950687   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 8)  new position  (0, 9)\n",
      "0.0 0 89  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-1.55101345 -2.99308013 16.77153417  2.39038211  2.15020464  7.31473979\n",
      " -0.13553474 10.65815902  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 9)  new position  (1, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 9)  new position  (9, 0)\n",
      "0.0 0 90  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -2.73473475  -0.21663159 -12.74645179   6.79275188  -2.80281251\n",
      "  10.38518944   3.21732005   6.80014211   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 0)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [13.12601014 12.14844637 -1.10361793 14.52261584 -4.11906788 -0.46161736\n",
      "  7.60447512 -0.15495821  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 0)  new position  (0, 9)\n",
      "0.0 0 91  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-1.55101345 -2.99308013 16.77153417  2.39038211  2.15020464  7.31473979\n",
      " -0.13553474 10.65815902  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 9)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -3.60636607  -2.59721818  17.33075996  13.35257287  -4.40038354\n",
      "   8.68353356 -17.55711743   0.94073827   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 9)  new position  (8, 8)\n",
      "0.0 0 92  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 1.16925042 16.048749   -1.05598456 29.84867441  0.          1.17305831\n",
      " 13.23286847  8.10991961  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 8)  new position  (1, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 8)  new position  (8, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 93  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [29.11445124 22.74569532  0.         13.97051409  0.         27.76971107\n",
      " 26.97358949  2.41201194  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 8)  new position  (1, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-10.07688183 -28.80657813  -8.45533959  -0.51490706  -5.02334143\n",
      "  27.05083553 -18.10199168  -4.02836738   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 7)  new position  (8, 6)\n",
      "0.0 0 94  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [11.58237875 -6.20658786  2.99980652  2.47050577  5.14152991  6.87504739\n",
      "  1.44654251 -0.35355501  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 9)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-19.25776765 -16.71693457   1.14421871  -7.78298141 -19.78017513\n",
      "   2.83826722 -12.45137718   6.02343751   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 6)  new position  (9, 6)\n",
      "0.0 0 95  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 0)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 6)  new position  (9, 5)\n",
      "0.0 0 96  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 5)  new position  (8, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 0)  new position  (9, 1)\n",
      "0.0 0 97  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 1)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-19.25776765 -16.71693457   1.14421871  -7.78298141 -19.78017513\n",
      "   2.83826722 -12.45137718   6.02343751   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 6)  new position  (7, 6)\n",
      "0.0 0 98  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-23.55340442   0.         -22.71395271 -35.44510746 -11.99189123\n",
      " -32.07197023   0.           0.           0.        ]\n",
      "max index list:  [1, 6, 7, 8]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 6)  new position  (7, 7)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 0)  new position  (1, 1)\n",
      "0.0 0 99  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 1)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-29.35454541   0.         -33.4132459   -6.23944979 -37.15959742\n",
      " -19.84513931   0.         -14.1175831    0.        ]\n",
      "max index list:  [1, 6, 8]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 7)  new position  (7, 6)\n",
      "0.0 0 100  Episode:  5\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating trained mc agent with vision range  2\n",
      "creating trained mc agent with vision range  2\n",
      "Monte Carlo Episode  6\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.624       0.67113047 -6.22016958  6.52731994 -5.00961963  0.\n",
      "  0.205248   -4.5368002   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 4)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [19.16280625 -8.35751425  6.32085021  7.16124988 -0.67086604 -3.62465922\n",
      " -1.14537676  2.55186975  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 2)  new position  (0, 2)\n",
      "0.0 0 1  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 5)  new position  (5, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 2)  new position  (1, 2)\n",
      "0.0 0 2  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.66282809  -4.26618692   4.0766115  -11.41362679  10.69348545\n",
      "  11.03964625   1.67209256  -9.43224976   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 2)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         10.6286564  18.21014817  0.          0.          0.\n",
      " 10.65163663  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 6)  new position  (5, 5)\n",
      "0.0 0 3  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 5)  new position  (5, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 2)  new position  (0, 3)\n",
      "0.0 0 4  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         10.6286564  18.21014817  0.          0.          0.\n",
      " 10.65163663  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 6)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 3)  new position  (1, 4)\n",
      "0.0 0 5  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [12.46722491 -1.77699393 -4.893463    0.         20.07332682 40.25356017\n",
      " 21.03019205  7.96800248  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 4)  new position  (0, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 5)  new position  (6, 4)\n",
      "0.0 0 6  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 4)  new position  (5, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 4)  new position  (9, 5)\n",
      "0.0 0 7  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 5)  new position  (0, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 4)  new position  (5, 3)\n",
      "0.0 0 8  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.00746759 -11.1412527  -19.62385086  -1.01596757  -1.60346104\n",
      "  15.51659352 -10.01204177  -5.37875616   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 3)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 4)  new position  (0, 5)\n",
      "0.0 0 9  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 5)  new position  (0, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 4)  new position  (4, 5)\n",
      "0.0 0 10  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 4)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 5)  new position  (3, 5)\n",
      "0.0 0 11  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 5)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 3)  new position  (0, 3)\n",
      "0.0 0 12  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-15.30770689  -0.02310879   2.21819481 -20.34163604  14.42987352\n",
      "  16.91183794 -10.28009242  -6.85822503   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 3)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-11.5691717    0.          18.83737322  12.54358916   7.61177462\n",
      " -18.91666582  -3.0443907    0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 5)  new position  (3, 5)\n",
      "0.0 0 13  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 4)  new position  (8, 3)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 5)  new position  (4, 5)\n",
      "0.0 0 14  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 3)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 5)  new position  (3, 4)\n",
      "0.0 0 15  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 3)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.82087061 -36.97603648 -40.37821211 -63.24831962 -35.21881822\n",
      " -46.65772308 -36.22761687 -27.25788312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 4)  new position  (4, 4)\n",
      "0.0 0 16  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 1.29424199 33.67671323  3.10235569  0.         13.19320864 13.92825615\n",
      " 12.86609924  0.3207369   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 4)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 4)  new position  (4, 3)\n",
      "0.0 0 17  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 3.6359424   4.66838662  3.61232695  0.69975406 -6.82693662 -1.95821441\n",
      " 21.32860793  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 4)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  6.37975911   5.16386412 -13.12246634  20.78696663   2.88450628\n",
      "  -3.43625113  -8.57318634   6.62922912   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 3)  new position  (5, 4)\n",
      "0.0 0 18  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.          8.58593534  2.827821   21.11110779  9.86667574  0.\n",
      " 19.29039385  7.4519474   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 4)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-9.47359013 12.16517891 25.42125912  0.         24.81709346 32.34925469\n",
      " 23.01308649  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 4)  new position  (7, 3)\n",
      "0.0 0 19  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -2.58620332 12.43419045  7.58168105 -6.64198416  0.\n",
      "  2.75743108 -5.82558898  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 5)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.82702598  -0.70796141 -13.83061141  -7.85426937   5.82730974\n",
      "   2.89429255   0.14135414   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 3)  new position  (7, 4)\n",
      "0.0 0 20  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.           1.54419733  -4.58163181   0.          -4.17538304\n",
      "   6.54896545 -13.03301365  27.69568537   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  4\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 4)  new position  (7, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.46718256 12.99294992 -3.43230564  0.         12.83884125 -4.56649131\n",
      " 11.27934756  0.          0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 4)  new position  (7, 3)\n",
      "0.0 0 21  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [24.40028667  0.         11.91104575 16.5401927   5.07502957  0.78239113\n",
      " 25.51291691  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 5)  new position  (8, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-12.10774421  -6.49778083 -22.75961354  -2.14876205 -11.48534148\n",
      "   0.          15.5540825  -21.275959     0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 3)  new position  (7, 2)\n",
      "0.0 0 22  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  6.09405114  -9.14002388 -11.11312829 -19.31642426 -13.27036687\n",
      "  -4.86725442   0.77972261 -15.44803011   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 2)  new position  (8, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 4)  new position  (9, 3)\n",
      "0.0 0 23  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.14446615 -15.15585157  -7.43477394  -9.40141769  -5.37446378\n",
      " -22.77481806  16.82458917 -16.05052064   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 1)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 3)  new position  (8, 2)\n",
      "0.0 0 24  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -9.68524504   0.2754947  -13.01663237   3.86903878   9.15091199\n",
      "   0.91643932   1.66063553   7.93832682   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 2)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 0)  new position  (9, 9)\n",
      "0.0 0 25  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 9)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -2.42986165 -19.35135882  14.9286029    7.05827051   3.26492171\n",
      "  13.51251725   3.90901174   4.59857753   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 3)  new position  (6, 3)\n",
      "0.0 0 26  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 3)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 9)  new position  (9, 0)\n",
      "0.0 0 27  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 2)  new position  (6, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 0)  new position  (8, 1)\n",
      "0.0 0 28  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.77764865  -1.75096387 -10.82208213 -11.82711716 -12.06603501\n",
      "  -3.42712575 -12.4968946    7.73306886   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 1)  new position  (7, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 0.          6.08065157  2.43484505  1.8862158   9.15348795 -3.40859069\n",
      " -6.36112657 10.33111779  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 2)  new position  (5, 3)\n",
      "0.0 0 29  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.64107516 -19.6295525  -25.98508065  22.12975048   0.77560906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9.09941686 -11.35233412  10.1239181    0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 3)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-17.48703515  -2.8621897   11.41292642  -8.99247044  -9.00496607\n",
      " -14.77126539  14.63240104   0.99390264   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 2)  new position  (8, 2)\n",
      "0.0 0 30  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 10.27603174  -5.63920849  -4.0714197  -23.42504938 -17.31816488\n",
      "  -7.89739378  -2.34866149   5.54928772   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 2)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 4)  new position  (5, 4)\n",
      "0.0 0 31  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 1)  new position  (6, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [14.2002521  18.56701244  1.45144572 25.24702936 12.71624074  0.\n",
      "  2.64612873  1.28237876  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 4)  new position  (6, 4)\n",
      "0.0 0 32  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-18.21395631  18.0136876   13.34186127   0.          -0.47761199\n",
      "  22.52260591  15.13909192  -8.61636553   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 4)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.88442563   5.1479857  -11.33568751  -0.04709022  -9.97998983\n",
      "  -7.20735583   4.48061612   1.96237111   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 2)  new position  (5, 1)\n",
      "0.0 0 33  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 3.6359424   4.66838662  3.61232695  0.69975406 -6.82693662 -1.95821441\n",
      " 21.32860793  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 4)  new position  (8, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 1)  new position  (5, 0)\n",
      "0.0 0 34  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 3)  new position  (8, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 0)  new position  (5, 9)\n",
      "0.0 0 35  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 2)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.11988277 -4.07913728 20.38979342  9.0197903  -2.99440205  2.53546518\n",
      "  5.2438629  23.37620963  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 9)  new position  (6, 0)\n",
      "0.0 0 36  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-13.9505043   30.20900922   1.49320551  -3.83147095   6.36819417\n",
      "   9.49709516   0.          -2.08036263   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 1)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.           3.32011144 -10.19153667   1.42337179  -5.80587771\n",
      "  -7.8160771   17.03771639  13.96222444   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 0)  new position  (7, 1)\n",
      "0.0 0 37  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-24.06101779  -0.53929609 -10.35643124   0.           5.38594319\n",
      "  -9.74319043  -9.92765541  11.77418214   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 0)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-2.03673164 -1.11524775 -9.59978138 -2.61599872  3.02959711  5.38981056\n",
      " -1.16927965 -3.88182476  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 1)  new position  (6, 0)\n",
      "0.0 0 38  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  5.09370162 -10.82769219 -10.72047025  -2.6968508    0.\n",
      " -13.51901237   8.01613468  -2.77046312   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 0)  new position  (6, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-26.78036549   0.           8.4893565    6.4675201  -12.46431305\n",
      "   9.99268243   6.0341334    2.62618286   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 9)  new position  (7, 8)\n",
      "0.0 0 39  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  6.43833396   8.09790205  -5.63738654  -7.26152047   0.\n",
      " -10.96454706  -9.53816313   6.73411338   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 9)  new position  (5, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [12.43446445 11.16869863 -2.22159837  3.6934306  -1.17611184  4.37811339\n",
      "  0.          5.68817766  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 8)  new position  (8, 9)\n",
      "0.0 0 40  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 9)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 2.28911021  5.70659506  0.         -0.07721949  0.          7.2705561\n",
      "  0.          5.1681752   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 8)  new position  (6, 8)\n",
      "0.0 0 41  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  3.08627845 -18.19141321  -9.19603577 -15.40976558   1.46044375\n",
      "   3.52550784   0.         -13.86775772   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 9)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [18.19058652  5.82647607  0.          0.2863648   1.18409601  8.07016103\n",
      "  0.         -6.07654185  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  4\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 8)  new position  (7, 9)\n",
      "0.0 0 42  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  5.09370162 -10.82769219 -10.72047025  -2.6968508    0.\n",
      " -13.51901237   8.01613468  -2.77046312   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 0)  new position  (7, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  9.85848174   3.71350658  -4.57260331   0.           0.10043645\n",
      "   0.54404312  -8.56074848 -15.16303117   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 9)  new position  (8, 9)\n",
      "0.0 0 43  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-20.71779582   7.10185688  -0.53097567 -10.58553808   0.\n",
      "  -1.95886326   9.99825711   0.6055224    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 0)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  5.19610429 -14.75564996   5.75988009  -2.12596802  -5.56972002\n",
      "   7.20970837 -16.24814786  -0.52961354   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 9)  new position  (9, 0)\n",
      "0.0 0 44  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 0)  new position  (8, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -4.38738457   6.26537181  -7.91755209  17.67981218 -12.9423612\n",
      "  -6.01597829   2.52058234  12.57304967   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 0)  new position  (5, 1)\n",
      "0.0 0 45  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 1)  new position  (6, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.77764865  -1.75096387 -10.82208213 -11.82711716 -12.06603501\n",
      "  -3.42712575 -12.4968946    7.73306886   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 1)  new position  (7, 1)\n",
      "0.0 0 46  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-8.34163689  2.33424732  4.53351757 -4.04304428 -1.98500802 -0.44717439\n",
      "  4.38218289  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 1)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -4.00454268  16.00344929   0.           3.79865727 -24.58270416\n",
      " -16.52722329   7.14915628 -20.65813186   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 2)  new position  (5, 2)\n",
      "0.0 0 47  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-9.05793949 -0.49637044 -1.90057624 -7.62942418 -5.31554887 -1.75998072\n",
      " 10.30935264  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 1)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.19167827 -12.60905395 -13.2858872   10.75372013   5.12465601\n",
      "  10.95440652   2.87874002   5.14089495   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 2)  new position  (6, 2)\n",
      "0.0 0 48  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  1.49194904   3.04511779  -9.16901272   1.61079245  -3.02148133\n",
      "  -1.86482157   5.50650278 -12.07827213   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 0)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.13618495  -6.38826598  -2.44000947   6.81094996   0.\n",
      "  -8.23450844   1.38483119 -10.91782312   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 2)  new position  (5, 3)\n",
      "0.0 0 49  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.04821364  -0.36104703 -10.937889    -5.22536786  -3.93571957\n",
      "  -0.04657731 -13.16320521 -23.09992099   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 1)  new position  (8, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.00746759 -11.1412527  -19.62385086  -1.01596757  -1.60346104\n",
      "  15.51659352 -10.01204177  -5.37875616   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 3)  new position  (6, 2)\n",
      "0.0 0 50  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -7.75510028  -5.89342066   2.53819243  -8.04443739 -11.79012662\n",
      "   0.70141708  -9.67319877  -1.16855252   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 2)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72128394  2.06398574  3.84225412 21.90938855  0.40760259  0.\n",
      "  1.27331955 -2.93132446  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 2)  new position  (7, 2)\n",
      "0.0 0 51  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-7.19684951  3.76933477  0.         -7.88422537  6.89985758 -3.4502183\n",
      "  9.13642104  9.91375109  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 3)  new position  (8, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 18.82162326  -5.0511549  -24.73719063  15.95936683 -13.54284363\n",
      "   0.         -21.89234592  -8.17629134   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 2)  new position  (7, 1)\n",
      "0.0 0 52  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-20.70855043 -10.15961848  -6.84003134  -8.74893921   8.63200059\n",
      "  -6.26675963  -6.21500425  -8.96674815   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 3)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 1)  new position  (8, 0)\n",
      "0.0 0 53  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 4)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 0)  new position  (9, 0)\n",
      "0.0 0 54  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 5)  new position  (0, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 0)  new position  (8, 9)\n",
      "0.0 0 55  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.          16.9698146    6.44973079 -21.96511259   0.\n",
      "   0.          14.43365271  -6.44731182   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 6)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 7.79532914 -9.35240866 -5.81092358  7.40363378 -8.98180241 -8.56225511\n",
      " -8.76019825  0.75333113  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 9)  new position  (8, 8)\n",
      "0.0 0 56  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-14.70144806  -3.69114136   0.28105784  19.79670524 -20.26574256\n",
      "   2.08170483 -17.93206496   2.79496359   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 8)  new position  (7, 7)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 7)  new position  (9, 7)\n",
      "0.0 0 57  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -3.02695838 -37.64974215   4.06067368 -16.83588252  -9.15664225\n",
      "  14.64054177  -6.55077212  -9.28371926   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 7)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 7.6689067   0.          0.25757709 28.24739754 -9.11198391  0.24613763\n",
      "  0.         -7.51180213  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 7)  new position  (8, 7)\n",
      "0.0 0 58  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  3.80301906  -2.45719824  -3.57158988  -4.34264703 -15.77621092\n",
      "  18.11097241   0.         -13.51537183   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 8)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-24.58305204  11.41563144  -9.0078624  -31.45049693 -14.63128301\n",
      " -13.91880319   5.4891795   10.00727112   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action I choose is  DOWN\n",
      "Old location  (8, 7)  new position  (7, 7)\n",
      "0.0 0 59  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 0.11090585  0.         -4.18204774  3.33756224  6.90213021 14.26749045\n",
      "  0.         19.98506432  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 7)  new position  (8, 7)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.78543462 -10.04978974  -6.0339254  -15.75452366  -5.75064466\n",
      "  -7.52201279  -3.78458698   3.52562514   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 9)  new position  (8, 8)\n",
      "0.0 0 60  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -9.7012958  -12.34994982  -9.19938536   0.         -17.36004939\n",
      " -17.27392134 -20.40401948  -2.86586588   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 7)  new position  (7, 7)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-11.90941852 -13.15838035 -12.01135031 -14.80435474 -12.95525278\n",
      "  -8.19914085   0.           0.24045809   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 8)  new position  (9, 7)\n",
      "0.0 0 61  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -3.02695838 -37.64974215   4.06067368 -16.83588252  -9.15664225\n",
      "  14.64054177  -6.55077212  -9.28371926   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 7)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -2.49282953   0.          -5.10580674  16.41220304 -19.35421611\n",
      "   7.70304236   0.           6.33952232   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  4\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 7)  new position  (8, 8)\n",
      "0.0 0 62  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-14.70144806  -3.69114136   0.28105784  19.79670524 -20.26574256\n",
      "   2.08170483 -17.93206496   2.79496359   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 8)  new position  (7, 7)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 7)  new position  (9, 7)\n",
      "0.0 0 63  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 17.2000515    0.          12.50350521   3.51332567 -10.80765117\n",
      "   4.81669707   0.          -0.5459686    0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 7)  new position  (7, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-31.31404861 -11.0485512   10.556336   -12.2109446   -5.25077201\n",
      "   2.38004713 -12.94641378  -1.20226394   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 7)  new position  (8, 7)\n",
      "0.0 0 64  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-8.57868718 -9.48728853  0.06754052  1.43980615 -4.61243567 10.99094541\n",
      " -7.25208266  0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 7)  new position  (8, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.         -10.50272327 -16.19703072 -17.23587095  -0.40678183\n",
      "   8.8945287    0.           6.96020338   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 8)  new position  (8, 7)\n",
      "0.0 0 65  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -9.7012958  -12.34994982  -9.19938536   0.         -17.36004939\n",
      " -17.27392134 -20.40401948  -2.86586588   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 7)  new position  (7, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.9768037   -5.73317934 -18.71966867  11.47454593  -2.93067523\n",
      " -11.3945192  -21.39637851  -8.08222236   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 8)  new position  (8, 7)\n",
      "0.0 0 66  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.15722294   0.           2.13069065   3.65302877 -10.47529726\n",
      "   0.           0.           0.           0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 6)  new position  (7, 7)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.45973019   0.         -15.85637272 -24.7807963    5.23576007\n",
      "  -7.38856005  -6.58179937  -5.85226816   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 7)  new position  (7, 6)\n",
      "0.0 0 67  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-2.87301888  0.         -2.45337127  0.          4.4190993  -3.15333306\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 6)  new position  (8, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  9.28842813   0.          -7.1179168   -0.79655782 -11.99771781\n",
      "   5.87941248   0.           4.13529107   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 7)  new position  (7, 6)\n",
      "0.0 0 68  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-14.73881104 -23.75324173 -17.1830991  -12.88378937  -9.6575349\n",
      "  -9.52377256  -2.22223779   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 5)  new position  (7, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 2.63561845  0.          0.          1.0466835  -4.34343957 10.70550028\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 6)  new position  (8, 5)\n",
      "0.0 0 69  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-21.05994777   0.           0.87077306   3.07423584  -8.99820644\n",
      " -12.93524447   5.12790315  -5.63808035   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [15.58144951  0.          4.42410909  3.83436155 -5.40833047 17.80947672\n",
      "  4.7520184   0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 5)  new position  (7, 4)\n",
      "0.0 0 70  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 7.53883407 -2.01403892  3.03130081 14.15965782 11.12209608  3.96372445\n",
      " -4.14352921  0.          0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 4)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 6)  new position  (0, 7)\n",
      "0.0 0 71  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  5\n",
      "index max:  4\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 7)  new position  (1, 8)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 4)  new position  (5, 5)\n",
      "1.0 1 72  Episode:  6\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0  reward:  50\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.          76.41176288  76.41477163  53.78114578  75.55085715\n",
      "   0.         119.20236977  72.61717149   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 5)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.25630474  -4.81181327   0.           7.26364528   0.\n",
      "   5.22697059 -18.5995958   12.94315109   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  4\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 8)  new position  (2, 9)\n",
      "1.0 0 73  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 9)  new position  (1, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 4)  new position  (5, 4)\n",
      "1.0 0 74  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.25630474  -4.81181327   0.           7.26364528   0.\n",
      "   5.22697059 -18.5995958   12.94315109   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 8)  new position  (1, 9)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 63.64220868  90.91526271  78.06323289 106.24483303  52.34608867\n",
      "   0.          86.20117852 124.23391292   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 4)  new position  (5, 5)\n",
      "2.0 1 75  Episode:  6\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0  reward:  50\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [11.58237875 -6.20658786  2.99980652  2.47050577  5.14152991  6.87504739\n",
      "  1.44654251 -0.35355501  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 9)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.          76.41176288  76.41477163  53.78114578  75.55085715\n",
      "   0.         119.20236977  72.61717149   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 5)  new position  (5, 6)\n",
      "3.0 1 76  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 0)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         61.0946131  87.02704625  0.          0.          0.\n",
      " 66.75884664  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 6)  new position  (4, 6)\n",
      "4.0 1 77  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [156.99361674   0.         153.79959178   0.         161.4842145\n",
      "   0.         140.02850683   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 6)  new position  (5, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 9)  new position  (8, 0)\n",
      "5.0 1 78  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         61.0946131  87.02704625  0.          0.          0.\n",
      " 66.75884664  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 6)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 0)  new position  (9, 1)\n",
      "6.0 1 79  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 1)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [156.99361674   0.         153.79959178   0.         161.4842145\n",
      "   0.         140.02850683   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 6)  new position  (3, 5)\n",
      "7.0 1 80  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [163.58428822 153.56600235 155.72497235   0.         180.0674096\n",
      " 138.93088928 179.46339368 157.10825024   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 5)  new position  (2, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 0)  new position  (0, 9)\n",
      "8.0 1 81  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 0.          0.         57.34890654  0.         51.79872875  0.\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 6)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 9)  new position  (1, 9)\n",
      "9.0 1 82  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [163.58428822 153.56600235 155.72497235   0.         180.0674096\n",
      " 138.93088928 179.46339368 157.10825024   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 5)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [11.58237875 -6.20658786  2.99980652  2.47050577  5.14152991  6.87504739\n",
      "  1.44654251 -0.35355501  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 9)  new position  (0, 8)\n",
      "9.0 0 83  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [163.52229653 196.5143223  170.38265533 227.42537612 157.26478553\n",
      " 221.04856132 178.36707012 213.84496508   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 4)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30851671  15.10050741   3.98786117  14.30216312   0.\n",
      "  -0.27077046 -11.8440253    6.7174165    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 8)  new position  (0, 9)\n",
      "10.0 1 84  Episode:  6\n",
      "New cow in the goal: 1\n",
      "cows in goal:  1 , previous_cow_count:  0  reward:  50\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 9)  new position  (1, 8)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.          76.41176288  76.41477163  53.78114578  75.55085715\n",
      "   0.         119.20236977  72.61717149   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 5)  new position  (4, 5)\n",
      "11.0 1 85  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.25630474  -4.81181327   0.           7.26364528   0.\n",
      "   5.22697059 -18.5995958   12.94315109   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 8)  new position  (0, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [175.22511218 161.60021279 189.6361078  148.2184543  195.06708777\n",
      " 177.10589894 162.03836022   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 5)  new position  (3, 5)\n",
      "12.0 1 86  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [163.58428822 153.56600235 155.72497235   0.         180.0674096\n",
      " 138.93088928 179.46339368 157.10825024   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.5082414  -23.73468044  -3.28677313   4.02298018   0.06729362\n",
      "  19.11588588  -3.03641048 -10.14972972   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 9)  new position  (1, 0)\n",
      "13.0 1 87  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [156.99361674   0.         153.79959178   0.         161.4842145\n",
      "   0.         140.02850683   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  3\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 6)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [13.12601014 12.14844637 -1.10361793 14.52261584 -4.11906788 -0.46161736\n",
      "  7.60447512 -0.15495821  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 0)  new position  (0, 1)\n",
      "14.0 1 88  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 1)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [163.58428822 153.56600235 155.72497235   0.         180.0674096\n",
      " 138.93088928 179.46339368 157.10825024   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 5)  new position  (4, 5)\n",
      "15.0 1 89  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 0)  new position  (0, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [175.22511218 161.60021279 189.6361078  148.2184543  195.06708777\n",
      " 177.10589894 162.03836022   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 5)  new position  (4, 6)\n",
      "16.0 1 90  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 1)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [156.99361674   0.         153.79959178   0.         161.4842145\n",
      "   0.         140.02850683   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  3\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 6)  new position  (3, 5)\n",
      "17.0 1 91  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [163.58428822 153.56600235 155.72497235   0.         180.0674096\n",
      " 138.93088928 179.46339368 157.10825024   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 5)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 2)  new position  (1, 1)\n",
      "18.0 1 92  Episode:  6\n",
      "cows in goal:  1 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 1)  new position  (2, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [175.22511218 161.60021279 189.6361078  148.2184543  195.06708777\n",
      " 177.10589894 162.03836022   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 5)  new position  (4, 4)\n",
      "18.0 0 93  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  1  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [163.52229653 196.5143223  170.38265533 227.42537612 157.26478553\n",
      " 221.04856132 178.36707012 213.84496508   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 4)  new position  (5, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 1)  new position  (1, 1)\n",
      "18.0 0 94  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 1)  new position  (2, 0)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 63.64220868  90.91526271  78.06323289 106.24483303  52.34608867\n",
      "   0.          86.20117852 124.23391292   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 4)  new position  (6, 3)\n",
      "18.0 0 95  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 3)  new position  (5, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.13376359   0.55244619   0.82926859   6.67461715  20.98817125\n",
      "  15.01263039  -1.1080356  -12.02177202   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 0)  new position  (3, 1)\n",
      "18.0 0 96  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 1)  new position  (2, 1)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 63.64220868  90.91526271  78.06323289 106.24483303  52.34608867\n",
      "   0.          86.20117852 124.23391292   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 4)  new position  (6, 3)\n",
      "18.0 0 97  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 1)  new position  (1, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 3)  new position  (7, 3)\n",
      "18.0 0 98  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [13.12601014 12.14844637 -1.10361793 14.52261584 -4.11906788 -0.46161736\n",
      "  7.60447512 -0.15495821  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (1, 0)  new position  (1, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 3)  new position  (8, 2)\n",
      "18.0 0 99  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-5.88136669  0.62307256  8.34130486  2.36110733 11.57893202 19.11781084\n",
      "  7.38516108 13.80266182  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 2)  new position  (9, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.77527659  -0.0494795    3.45184861   2.81171506 -11.89464174\n",
      "  11.74256936  -8.19991571  -8.2114626    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 9)  new position  (2, 8)\n",
      "18.0 0 100  Episode:  6\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating trained mc agent with vision range  2\n",
      "creating trained mc agent with vision range  2\n",
      "Monte Carlo Episode  7\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.91631167  -9.40148994  -9.11867023 -16.11964308 -14.68625626\n",
      "  15.86562523 -10.67987709  -2.1308996    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 1)  new position  (3, 1)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -3.9461035   1.712       0.          0.          0.\n",
      "  0.352      -1.24043955  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 6)  new position  (0, 5)\n",
      "2.0 2 1  Episode:  7\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0.0  reward:  50\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 5)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 1)  new position  (2, 0)\n",
      "4.0 2 2  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-9.47831964 -2.34800689 14.5283572   0.         -1.17056943  7.14850982\n",
      "  1.77685227 23.89548512  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 4)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.13376359   0.55244619   0.82926859   6.67461715  20.98817125\n",
      "  15.01263039  -1.1080356  -12.02177202   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 0)  new position  (1, 0)\n",
      "6.0 2 3  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [13.12601014 12.14844637 -1.10361793 14.52261584 -4.11906788 -0.46161736\n",
      "  7.60447512 -0.15495821  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 0)  new position  (2, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [72.01656095  0.         58.99843892 58.70014762 55.31353047  0.\n",
      " 52.95675423  0.          0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 5)  new position  (2, 4)\n",
      "8.0 2 4  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 9)  new position  (2, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 79.82949131  44.14555826  45.03403519  70.46315465  42.93996505\n",
      " 103.16859918  29.35840305   0.           0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 4)  new position  (2, 3)\n",
      "10.0 2 5  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.68256275 -16.7174098    2.02334361 -19.31482151  10.02558202\n",
      "  10.69528709  12.2001919    2.2586472    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 3)  new position  (1, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  1.34801353 -17.44635347  -6.54410773  19.11014708  -4.51332702\n",
      "   4.09432508  12.62488448  -1.42090326   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 0)  new position  (2, 9)\n",
      "12.0 2 6  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 9)  new position  (1, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.66282809  -4.26618692   4.0766115  -11.41362679  10.69348545\n",
      "  11.03964625   1.67209256  -9.43224976   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 2)  new position  (2, 1)\n",
      "14.0 2 7  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.71426643   0.10538484  -4.46184201  -0.497052   -21.20662052\n",
      "  10.37154569  10.94642464 -17.68928536   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 9)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 1)  new position  (2, 2)\n",
      "16.0 2 8  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30851671  15.10050741   3.98786117  14.30216312   0.\n",
      "  -0.27077046 -11.8440253    6.7174165    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 8)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 2)  new position  (3, 1)\n",
      "18.0 2 9  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 7)  new position  (0, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 1)  new position  (3, 2)\n",
      "20.0 2 10  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.          16.9698146    6.44973079 -21.96511259   0.\n",
      "   0.          14.43365271  -6.44731182   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 6)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 2)  new position  (2, 1)\n",
      "22.0 2 11  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 1)  new position  (3, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 6)  new position  (9, 5)\n",
      "24.0 2 12  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 2)  new position  (4, 3)\n",
      "26.0 2 13  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 6)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 3)  new position  (3, 4)\n",
      "28.0 2 14  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 7)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-70.93126472 -73.4111154  -67.7413191  -69.923616   -65.90782688\n",
      " -74.05274233 -58.30750172 -77.16063432   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 4)  new position  (4, 5)\n",
      "30.0 2 15  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-73.2289672  -72.20818575 -71.47301811 -74.2673748  -69.44691826\n",
      "   0.         -72.26400531   0.           0.        ]\n",
      "max index list:  [5, 7, 8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 5)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 8)  new position  (9, 7)\n",
      "32.0 2 16  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 7)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-70.93126472 -73.4111154  -67.7413191  -69.923616   -65.90782688\n",
      " -74.05274233 -58.30750172 -77.16063432   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 4)  new position  (2, 5)\n",
      "34.0 2 17  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 6)  new position  (9, 7)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [72.01656095  0.         58.99843892 58.70014762 55.31353047  0.\n",
      " 52.95675423  0.          0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 5)  new position  (2, 4)\n",
      "36.0 2 18  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 79.82949131  44.14555826  45.03403519  70.46315465  42.93996505\n",
      " 103.16859918  29.35840305   0.           0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 4)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 7)  new position  (0, 7)\n",
      "38.0 2 19  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  3\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 7)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -5.88218069  -5.1689681   -2.61361446   0.         -11.49354324\n",
      "   5.35931118   2.8797236   -3.75263094   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 4)  new position  (0, 3)\n",
      "40.0 2 20  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 6)  new position  (9, 7)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action I choose is  UP\n",
      "Old location  (0, 3)  new position  (1, 3)\n",
      "42.0 2 21  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 7)  new position  (0, 8)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 3)  new position  (0, 4)\n",
      "44.0 2 22  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30851671  15.10050741   3.98786117  14.30216312   0.\n",
      "  -0.27077046 -11.8440253    6.7174165    0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 8)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 4)  new position  (1, 3)\n",
      "46.0 2 23  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 9)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 3)  new position  (0, 3)\n",
      "48.0 2 24  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 0)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 3)  new position  (1, 3)\n",
      "50.0 2 25  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 3)  new position  (0, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 9)  new position  (7, 8)\n",
      "52.0 2 26  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.86126497  -7.64853507 -24.73028016 -22.18007293 -20.5990663\n",
      " -15.40596975   0.         -25.58865073   0.        ]\n",
      "max index list:  [6, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 8)  new position  (8, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 3)  new position  (9, 2)\n",
      "54.0 2 27  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.04744736  -5.64635074   3.66599274  12.73537913 -25.13640359\n",
      "   3.20590757  -7.84987033  -9.80192907   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 2)  new position  (8, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 8)  new position  (8, 9)\n",
      "56.0 2 28  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  8.42957643   4.30619397  10.40492242  -6.65464979   3.32153718\n",
      " -10.170147    14.73312872  -1.49134481   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 1)  new position  (7, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 9)  new position  (7, 0)\n",
      "58.0 2 29  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -9.82450512  -0.89403116  20.06955841 -10.17175174 -13.73004827\n",
      "   7.37264771   3.16739316 -11.592771     0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 0)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -7.7108958    3.73687574   1.39600688 -11.7577625   -2.14957684\n",
      " -17.88033222   0.          -8.40944262   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 2)  new position  (7, 3)\n",
      "60.0 2 30  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -2.37991928 -11.78287794 -17.18967957 -10.64195451  -1.42950023\n",
      " -18.33901302   9.99246755  12.27711319   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 1)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30850445   1.92981212 -12.14167679  -0.22045468   4.37259792\n",
      " -12.28833056  -0.58196766   1.87789158   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 3)  new position  (6, 4)\n",
      "62.0 2 31  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.39952389  10.5248983   -1.65126703   0.          15.11186674\n",
      " -16.43199171 -12.69227348   1.07660813   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 4)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.90679296 -12.26381286 -11.30116226  -4.70312865   2.60442381\n",
      "   8.35962202  -8.90364629  -8.55491398   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 1)  new position  (4, 2)\n",
      "64.0 2 32  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-12.11628669 -12.67246255  -2.30712483  17.684708     1.37747349\n",
      "  -0.55119908   1.70188875 -13.53912838   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 2)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -2.71951291   2.15256908 -11.08435228 -16.13277406   5.83459863\n",
      " -31.29136664  -2.73402971  -1.7492954    0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 3)  new position  (7, 3)\n",
      "66.0 2 33  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.30850445   1.92981212 -12.14167679  -0.22045468   4.37259792\n",
      " -12.28833056  -0.58196766   1.87789158   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 3)  new position  (8, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 1)  new position  (4, 1)\n",
      "68.0 2 34  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 4)  new position  (8, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 4.25942076 18.30805778  8.46826689 -0.07707364 15.80609069 10.23490179\n",
      " 12.96778101 22.00097905  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 1)  new position  (3, 1)\n",
      "70.0 2 35  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 3)  new position  (9, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 1.75993472  6.00861286 -1.12373605  9.33958465  8.03813157 14.71014713\n",
      "  7.61938025  6.81502989  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 1)  new position  (2, 2)\n",
      "72.0 2 36  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [7.06793075 1.37351855 0.96442711 1.24550836 9.12046571 1.91997423\n",
      " 4.86029555 2.92365554 0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 2)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -7.80650801 -14.46026923   1.09401236 -11.09786258 -23.60658762\n",
      "   9.01827416 -20.14242301  -4.60644887   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 2)  new position  (8, 2)\n",
      "74.0 2 37  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 2)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 3)  new position  (0, 3)\n",
      "76.0 2 38  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 3)  new position  (0, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 3)  new position  (6, 2)\n",
      "78.0 2 39  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 9.82896816  9.81512862  7.65487603 12.05970353  4.20623768 -3.33583982\n",
      "  1.49920169  8.37986383  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 2)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 4)  new position  (0, 3)\n",
      "80.0 2 40  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 1)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 3)  new position  (9, 4)\n",
      "82.0 2 41  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 1)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 4)  new position  (9, 3)\n",
      "84.0 2 42  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 3)  new position  (0, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 1)  new position  (4, 2)\n",
      "86.0 2 43  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 2)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 3)  new position  (9, 3)\n",
      "88.0 2 44  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 3)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 3)  new position  (3, 2)\n",
      "90.0 2 45  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 2)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 2)  new position  (0, 3)\n",
      "92.0 2 46  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 3)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -1.67553359 -19.92774525  -3.18557343 -12.4944173  -12.359542\n",
      " -15.80472475 -14.81315404 -13.21724532   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 3)  new position  (3, 4)\n",
      "94.0 2 47  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 4)  new position  (8, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-70.93126472 -73.4111154  -67.7413191  -69.923616   -65.90782688\n",
      " -74.05274233 -58.30750172 -77.16063432   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 4)  new position  (2, 3)\n",
      "96.0 2 48  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 5)  new position  (8, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.68256275 -16.7174098    2.02334361 -19.31482151  10.02558202\n",
      "  10.69528709  12.2001919    2.2586472    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 3)  new position  (2, 4)\n",
      "98.0 2 49  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 79.82949131  44.14555826  45.03403519  70.46315465  42.93996505\n",
      " 103.16859918  29.35840305   0.           0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 4)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 4)  new position  (9, 5)\n",
      "100.0 2 50  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 5)  new position  (0, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [12.11714826  0.         16.77491105  8.47337148  4.14299156  0.\n",
      "  7.72138989  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 5)  new position  (2, 6)\n",
      "102.0 2 51  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [14.34828855 -3.98250857  1.43295085  5.95738509 -9.99964444  0.\n",
      "  0.03403482 11.52220618  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 4)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ 0.          0.          0.56638996  0.         -0.58774054  0.\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  0\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 6)  new position  (2, 5)\n",
      "104.0 2 52  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-11.56057763   0.           2.80832939   0.02275544  12.41548042\n",
      "   0.           0.           0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 5)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-0.61520442 14.55434344 15.34225365  0.          0.78118936 -3.61670008\n",
      "  6.62942776  1.73983225  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 4)  new position  (2, 3)\n",
      "106.0 2 53  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [ -8.66874011  -6.34200229 -16.24779887   0.          -8.58153073\n",
      "  -7.4873508   -4.37137825  -8.76839073   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 5)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [11.253605    2.14865343 -4.34616463 18.41948849 21.31976501  9.50491336\n",
      " -8.03178259 -3.13048319  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 3)  new position  (2, 2)\n",
      "108.0 2 54  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-6.15085064 -2.59411931 -5.44656128 -2.12207485 -9.46668354 -2.15018753\n",
      " -5.80843971 -2.27402125  0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 4)  new position  (5, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 2)  new position  (3, 2)\n",
      "110.0 2 55  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.15324898  -4.98908646  -4.70333886  -1.38680146 -17.74890866\n",
      "   0.99914663  -7.61068117  -0.80068544   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 2)  new position  (4, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.00555645  -2.0083946    1.6102916   -7.51815699   1.11887454\n",
      " -12.31782758 -16.5348394    7.59283973   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 3)  new position  (4, 3)\n",
      "112.0 2 56  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 18.79541185  15.29016956 -23.24279921  27.3998372    0.94856394\n",
      " -17.70957831  -8.0330003   -3.75463173   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 1)  new position  (5, 0)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 3)  new position  (4, 2)\n",
      "114.0 2 57  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 11.00739254 -17.79043114 -19.59639161   9.47979815 -31.04604997\n",
      "   1.87066736 -21.90425195  -8.60524743   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 2)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 0)  new position  (5, 1)\n",
      "116.0 2 58  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 2.99627977e-01 -1.75724250e+01 -1.19984462e+01 -1.40701561e+01\n",
      "  1.66888910e-02  4.46171797e+00  5.16417277e+00  4.57194031e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 1)  new position  (4, 1)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-2.94035331 -6.86174902  2.50325758  1.39144159 -9.35797826 -0.09943269\n",
      " -5.62522288 18.56927906  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 3)  new position  (3, 4)\n",
      "118.0 2 59  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.91631167  -9.40148994  -9.11867023 -16.11964308 -14.68625626\n",
      "  15.86562523 -10.67987709  -2.1308996    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 1)  new position  (3, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-70.93126472 -73.4111154  -67.7413191  -69.923616   -65.90782688\n",
      " -74.05274233 -58.30750172 -77.16063432   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 4)  new position  (2, 3)\n",
      "120.0 2 60  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.68256275 -16.7174098    2.02334361 -19.31482151  10.02558202\n",
      "  10.69528709  12.2001919    2.2586472    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 3)  new position  (2, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -2.80316386 -14.81881769  11.66303636 -16.17465392  11.75968023\n",
      "  22.63804581 -11.62440004   3.55186452   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 0)  new position  (3, 1)\n",
      "122.0 2 61  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-2.71292225e+01 -1.90164993e+00 -1.97316906e+01  2.65164882e+01\n",
      "  0.00000000e+00  7.60479726e-03 -1.12308381e+01 -9.63686602e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 2)  new position  (1, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  3.4608418  -13.85655797  -4.71398128   5.19161675  -8.96004199\n",
      "  -2.48151931   1.37306385   1.59713202   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 1)  new position  (4, 2)\n",
      "124.0 2 62  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 2)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.66282809  -4.26618692   4.0766115  -11.41362679  10.69348545\n",
      "  11.03964625   1.67209256  -9.43224976   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 2)  new position  (0, 3)\n",
      "126.0 2 63  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 3)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.92934273  -3.10279504 -10.63598502  -0.88361604   4.3308623\n",
      "   0.7884244    7.71821639   7.0047347    0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 1)  new position  (6, 1)\n",
      "128.0 2 64  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-9.47831964 -2.34800689 14.5283572   0.         -1.17056943  7.14850982\n",
      "  1.77685227 23.89548512  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 4)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 1)  new position  (5, 0)\n",
      "130.0 2 65  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 0)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [72.01656095  0.         58.99843892 58.70014762 55.31353047  0.\n",
      " 52.95675423  0.          0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 5)  new position  (3, 4)\n",
      "132.0 2 66  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-70.93126472 -73.4111154  -67.7413191  -69.923616   -65.90782688\n",
      " -74.05274233 -58.30750172 -77.16063432   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 4)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 0)  new position  (6, 1)\n",
      "134.0 2 67  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.67696869  -4.56407168  -9.62783587  -0.17228761 -18.59672138\n",
      "   9.06464419  14.0056912   -7.30073364   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 1)  new position  (5, 0)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 3)  new position  (5, 4)\n",
      "136.0 2 68  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-54.06629126 -58.51584254 -57.01004062 -66.19381931 -61.80038203\n",
      "   0.         -70.02963398 -64.11833195   0.        ]\n",
      "max index list:  [5, 8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 4)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 0)  new position  (6, 0)\n",
      "138.0 2 69  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 0)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 3)  new position  (5, 2)\n",
      "140.0 2 70  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  1.93906758  -7.06639436  -1.23246565  -2.9272165   -7.73159699\n",
      " -13.43563008   8.6426112   -0.44638951   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 0)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-12.89576728  14.03638652   0.70366456  -9.681752     0.66277386\n",
      " -18.67467127   4.40047242  10.08356341   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 2)  new position  (5, 1)\n",
      "142.0 2 71  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.04790503  -6.91119453  10.69838566  10.69237109  11.01950045\n",
      "  -9.55730118  -1.01933127  -1.62389131   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 1)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-13.9505043   30.20900922   1.49320551  -3.83147095   6.36819417\n",
      "   9.49709516   0.          -2.08036263   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 1)  new position  (7, 2)\n",
      "144.0 2 72  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-11.91438267  17.85548001   8.61877333 -18.59494699  -6.56754554\n",
      " -10.28465597  20.85642643   1.01825748   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 2)  new position  (6, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 1.89941278 11.16017412 -0.15372215  3.14998037 -3.16511013 -2.7291049\n",
      " 12.24882599 17.31282742  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 0)  new position  (7, 9)\n",
      "146.0 2 73  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 2)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -7.7570729   -6.15643969   0.22763399 -17.74841309  14.38006568\n",
      "  -9.2499113    6.19395807 -11.46650165   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 9)  new position  (6, 8)\n",
      "148.0 2 74  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 1)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.27320786   4.36423413   0.         -10.88148942 -14.31594885\n",
      "  -4.0239842    0.          -3.50424995   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  5\n",
      "index max:  4\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 8)  new position  (7, 9)\n",
      "150.0 2 75  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-2.03673164 -1.11524775 -9.59978138 -2.61599872  3.02959711  5.38981056\n",
      " -1.16927965 -3.88182476  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 1)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -7.7570729   -6.15643969   0.22763399 -17.74841309  14.38006568\n",
      "  -9.2499113    6.19395807 -11.46650165   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 9)  new position  (8, 8)\n",
      "152.0 2 76  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 8)  new position  (9, 7)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 1)  new position  (6, 0)\n",
      "154.0 2 77  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 7)  new position  (0, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 0)  new position  (6, 1)\n",
      "156.0 2 78  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 1)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.          16.9698146    6.44973079 -21.96511259   0.\n",
      "   0.          14.43365271  -6.44731182   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 6)  new position  (0, 7)\n",
      "158.0 2 79  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  5\n",
      "index max:  4\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 7)  new position  (1, 8)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 0)  new position  (5, 1)\n",
      "160.0 2 80  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.25630474  -4.81181327   0.           7.26364528   0.\n",
      "   5.22697059 -18.5995958   12.94315109   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  3\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 8)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 1)  new position  (6, 2)\n",
      "162.0 2 81  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 7)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 2)  new position  (5, 1)\n",
      "164.0 2 82  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 8)  new position  (8, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 1)  new position  (6, 2)\n",
      "166.0 2 83  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 2)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 8)  new position  (7, 7)\n",
      "168.0 2 84  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-29.35454541   0.         -33.4132459   -6.23944979 -37.15959742\n",
      " -19.84513931   0.         -14.1175831    0.        ]\n",
      "max index list:  [1, 6, 8]\n",
      "max_action:  5\n",
      "index max:  4\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 7)  new position  (8, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 3)  new position  (6, 4)\n",
      "170.0 2 85  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 8)  new position  (8, 7)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.39952389  10.5248983   -1.65126703   0.          15.11186674\n",
      " -16.43199171 -12.69227348   1.07660813   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 4)  new position  (7, 4)\n",
      "172.0 2 86  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-10.07688183 -28.80657813  -8.45533959  -0.51490706  -5.02334143\n",
      "  27.05083553 -18.10199168  -4.02836738   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 7)  new position  (9, 7)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-13.97820741 -34.50005644 -33.08951058 -42.95138697 -21.4127722\n",
      " -25.63012661 -23.53734744   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 4)  new position  (8, 3)\n",
      "174.0 2 87  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 7)  new position  (8, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 3)  new position  (7, 4)\n",
      "176.0 2 88  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 8)  new position  (9, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-13.97820741 -34.50005644 -33.08951058 -42.95138697 -21.4127722\n",
      " -25.63012661 -23.53734744   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 4)  new position  (7, 5)\n",
      "178.0 2 89  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-33.75832968   0.         -35.01569024 -27.83497248 -25.25622167\n",
      " -36.40610341 -27.59878094   0.           0.        ]\n",
      "max index list:  [1, 7, 8]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 5)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 9)  new position  (8, 8)\n",
      "180.0 2 90  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.69444638  12.9219764   -3.95824293 -12.33355204  -3.72420751\n",
      "  -1.36248818  -0.25430817  16.19640063   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 8)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-13.97820741 -34.50005644 -33.08951058 -42.95138697 -21.4127722\n",
      " -25.63012661 -23.53734744   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 4)  new position  (8, 4)\n",
      "182.0 2 91  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 9)  new position  (7, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 4)  new position  (8, 5)\n",
      "184.0 2 92  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.86126497  -7.64853507 -24.73028016 -22.18007293 -20.5990663\n",
      " -15.40596975   0.         -25.58865073   0.        ]\n",
      "max index list:  [6, 8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 8)  new position  (8, 7)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-14.35204173 -28.41041387 -13.80794262 -11.60254552 -21.30627545\n",
      " -13.609253    18.65191085 -11.61525291   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 5)  new position  (7, 4)\n",
      "186.0 2 93  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-13.97820741 -34.50005644 -33.08951058 -42.95138697 -21.4127722\n",
      " -25.63012661 -23.53734744   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 4)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-10.07688183 -28.80657813  -8.45533959  -0.51490706  -5.02334143\n",
      "  27.05083553 -18.10199168  -4.02836738   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 7)  new position  (8, 6)\n",
      "188.0 2 94  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 3)  new position  (6, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-19.25776765 -16.71693457   1.14421871  -7.78298141 -19.78017513\n",
      "   2.83826722 -12.45137718   6.02343751   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 6)  new position  (7, 5)\n",
      "190.0 2 95  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 2)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.64170128   0.         -13.12072424 -20.55153074 -10.64379352\n",
      " -10.71795157 -20.74647953   0.           0.        ]\n",
      "max index list:  [1, 7, 8]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 5)  new position  (8, 6)\n",
      "192.0 2 96  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-19.25776765 -16.71693457   1.14421871  -7.78298141 -19.78017513\n",
      "   2.83826722 -12.45137718   6.02343751   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 6)  new position  (7, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.29826187   7.94378168  -4.60427867  -3.0868811  -25.39427411\n",
      " -12.68990964 -24.87351516   3.02337029   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 3)  new position  (5, 2)\n",
      "194.0 2 97  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 2)  new position  (5, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -8.94499651   0.          -5.72214743  -6.57260421 -15.17097502\n",
      " -15.37521572 -15.23099928   0.           0.        ]\n",
      "max index list:  [1, 7, 8]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 5)  new position  (8, 6)\n",
      "196.0 2 98  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-19.25776765 -16.71693457   1.14421871  -7.78298141 -19.78017513\n",
      "   2.83826722 -12.45137718   6.02343751   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 6)  new position  (7, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-4.18919668 -2.66186847  2.9624218  -7.40825326  0.57851045 -1.23927418\n",
      " 13.80897004 -0.68971041  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 3)  new position  (6, 4)\n",
      "198.0 2 99  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-3.87460007 12.91032634 -2.96912718  0.         -7.31464736  0.\n",
      "  4.02436461  1.76031042  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 4)  new position  (5, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-26.78637458   0.         -27.34713675  12.53504062  -6.55323713\n",
      "  -4.21313147 -24.5635357    0.           0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 5)  new position  (7, 6)\n",
      "200.0 2 100  Episode:  7\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating trained mc agent with vision range  2\n",
      "creating trained mc agent with vision range  2\n",
      "Monte Carlo Episode  8\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 0.       0.       0.       0.      -3.184    0.92032  0.       0.968\n",
      "  0.     ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 2)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.78690545  6.15444352  3.85395187 -0.15385486 -5.38010634 15.5076787\n",
      " 14.73589801 13.31404824  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 0)  new position  (3, 0)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "2.0 2 1  Episode:  8\n",
      "New cow in the goal: 2\n",
      "cows in goal:  2 , previous_cow_count:  0.0  reward:  50\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 31.284968    -4.82813909 -10.67881149  -1.93536291   4.62396222\n",
      "   6.19813269  -5.04263151  -6.92183961   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 0)  new position  (2, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 3)  new position  (8, 4)\n",
      "4.0 2 2  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 4)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 9)  new position  (3, 8)\n",
      "6.0 2 3  Episode:  8\n",
      "cows in goal:  2 , previous_cow_count:  2  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-2.56958567  5.55606195 -4.89817012  5.40181264  2.88851505 -5.46861209\n",
      " -0.61109235 -5.42846889  0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 4)  new position  (0, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-36.26970234 -32.35536858   0.         -36.07764051   0.\n",
      " -23.22363694   0.         -51.51648354   0.        ]\n",
      "max index list:  [2, 4, 6, 8]\n",
      "max_action:  5\n",
      "index max:  3\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 8)  new position  (4, 9)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "9.0 3 4  Episode:  8\n",
      "New cow in the goal: 3\n",
      "cows in goal:  3 , previous_cow_count:  2  reward:  50\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 9)  new position  (4, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 3)  new position  (9, 4)\n",
      "12.0 3 5  Episode:  8\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 4)  new position  (8, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-41.0616023  -20.76241479   0.         -24.16437899   0.\n",
      " -28.94581702   0.         -27.53899179   0.        ]\n",
      "max index list:  [2, 4, 6, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 8)  new position  (5, 8)\n",
      "15.0 3 6  Episode:  8\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-41.83313579 -37.23296051   0.         -39.91915187   0.\n",
      " -47.17094216   0.         -37.13265026   0.        ]\n",
      "max index list:  [2, 4, 6, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 8)  new position  (4, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 5)  new position  (8, 4)\n",
      "18.0 3 7  Episode:  8\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 4)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-41.0616023  -20.76241479   0.         -24.16437899   0.\n",
      " -28.94581702   0.         -27.53899179   0.        ]\n",
      "max index list:  [2, 4, 6, 8]\n",
      "max_action:  7\n",
      "index max:  4\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 8)  new position  (3, 9)\n",
      "21.0 3 8  Episode:  8\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 9)  new position  (4, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-13.97820741 -34.50005644 -33.08951058 -42.95138697 -21.4127722\n",
      " -25.63012661 -23.53734744   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 4)  new position  (8, 3)\n",
      "24.0 3 9  Episode:  8\n",
      "cows in goal:  3 , previous_cow_count:  3  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 3)  new position  (7, 4)\n",
      "COW HERDED COW HERDED COW HERDED COW HERDED COW HERDED \n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.78690545  6.15444352  3.85395187 -0.15385486 -5.38010634 15.5076787\n",
      " 14.73589801 13.31404824  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 0)  new position  (3, 1)\n",
      "28.0 4 10  Episode:  8\n",
      "New cow in the goal: 4\n",
      "cows in goal:  4 , previous_cow_count:  3  reward:  50\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-13.97820741 -34.50005644 -33.08951058 -42.95138697 -21.4127722\n",
      " -25.63012661 -23.53734744   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 4)  new position  (8, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 1)  new position  (4, 1)\n",
      "32.0 4 11  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 3)  new position  (8, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.91631167  -9.40148994  -9.11867023 -16.11964308 -14.68625626\n",
      "  15.86562523 -10.67987709  -2.1308996    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 1)  new position  (5, 0)\n",
      "36.0 4 12  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 2)  new position  (9, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 0)  new position  (5, 1)\n",
      "40.0 4 13  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.04744736  -5.64635074   3.66599274  12.73537913 -25.13640359\n",
      "   3.20590757  -7.84987033  -9.80192907   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 2)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 1)  new position  (4, 0)\n",
      "44.0 4 14  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.78690545  6.15444352  3.85395187 -0.15385486 -5.38010634 15.5076787\n",
      " 14.73589801 13.31404824  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 0)  new position  (4, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 3)  new position  (9, 2)\n",
      "48.0 4 15  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 9)  new position  (4, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.04744736  -5.64635074   3.66599274  12.73537913 -25.13640359\n",
      "   3.20590757  -7.84987033  -9.80192907   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 2)  new position  (8, 1)\n",
      "52.0 4 16  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [0.4       0.4       0.        1.5809022 0.        0.        0.\n",
      " 0.4       0.       ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  4\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 8)  new position  (3, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 24.28279917  -3.2503315   -3.4938431    7.46755294 -10.64272157\n",
      "  -9.66022703   0.34289003   0.44213197   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 1)  new position  (7, 2)\n",
      "56.0 4 17  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 9)  new position  (4, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 2)  new position  (7, 1)\n",
      "60.0 4 18  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 1)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.24979463  12.08678496   0.25557425 -10.63410889  11.29279005\n",
      "  10.83118175  -1.55123701   5.62332611   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 0)  new position  (4, 9)\n",
      "64.0 4 19  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.39490843  -4.98331374  12.46495181  14.81317966 -10.69503425\n",
      "   3.80487489  -2.98673911   9.68295205   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 9)  new position  (3, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 1)  new position  (7, 2)\n",
      "68.0 4 20  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 2)  new position  (6, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [0.        0.7968    0.        0.4       0.        1.1904256 0.\n",
      " 0.        0.       ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 8)  new position  (2, 8)\n",
      "72.0 4 21  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 2)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-22.429658   -32.69779518   0.         -22.09446822   0.\n",
      " -29.50940188   0.         -31.24192571   0.        ]\n",
      "max index list:  [2, 4, 6, 8]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 8)  new position  (3, 8)\n",
      "76.0 4 22  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 2)  new position  (6, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [0.        0.7968    0.        0.4       0.        1.1904256 0.\n",
      " 0.        0.       ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 8)  new position  (2, 8)\n",
      "80.0 4 23  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-22.429658   -32.69779518   0.         -22.09446822   0.\n",
      " -29.50940188   0.         -31.24192571   0.        ]\n",
      "max index list:  [2, 4, 6, 8]\n",
      "max_action:  2\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 8)  new position  (2, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 2)  new position  (7, 2)\n",
      "84.0 4 24  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 2)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 9)  new position  (1, 8)\n",
      "88.0 4 25  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 3)  new position  (7, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-26.04632583 -47.11024243   0.         -40.39295208   0.\n",
      " -30.73127695 -46.68286327 -55.53802315   0.        ]\n",
      "max index list:  [2, 4, 8]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 8)  new position  (1, 9)\n",
      "92.0 4 26  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-13.97820741 -34.50005644 -33.08951058 -42.95138697 -21.4127722\n",
      " -25.63012661 -23.53734744   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 4)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [11.58237875 -6.20658786  2.99980652  2.47050577  5.14152991  6.87504739\n",
      "  1.44654251 -0.35355501  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 9)  new position  (2, 9)\n",
      "96.0 4 27  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 9)  new position  (1, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 3)  new position  (5, 4)\n",
      "100.0 4 28  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [13.12601014 12.14844637 -1.10361793 14.52261584 -4.11906788 -0.46161736\n",
      "  7.60447512 -0.15495821  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 0)  new position  (2, 0)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-44.99557044 -53.26278804 -53.57962983 -51.16538263 -49.55254365\n",
      "   0.         -49.08370036 -54.71202634   0.        ]\n",
      "max index list:  [5, 8]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 4)  new position  (4, 5)\n",
      "104.0 4 29  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "I havent seen this before, picking randomly\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 5)  new position  (5, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.13376359   0.55244619   0.82926859   6.67461715  20.98817125\n",
      "  15.01263039  -1.1080356  -12.02177202   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 0)  new position  (2, 1)\n",
      "108.0 4 30  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-44.99557044 -53.26278804 -53.57962983 -51.16538263 -49.55254365\n",
      "   0.         -49.08370036 -54.71202634   0.        ]\n",
      "max index list:  [5, 8]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 4)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 1)  new position  (2, 0)\n",
      "112.0 4 31  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "I havent seen this before, picking randomly\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 5)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.13376359   0.55244619   0.82926859   6.67461715  20.98817125\n",
      "  15.01263039  -1.1080356  -12.02177202   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 0)  new position  (3, 0)\n",
      "116.0 4 32  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 31.284968    -4.82813909 -10.67881149  -1.93536291   4.62396222\n",
      "   6.19813269  -5.04263151  -6.92183961   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 0)  new position  (2, 9)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "I havent seen this before, picking randomly\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (4, 4)  new position  (4, 3)\n",
      "120.0 4 33  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 3)  new position  (5, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 9)  new position  (3, 9)\n",
      "124.0 4 34  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 9)  new position  (2, 8)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.00746759 -11.1412527  -19.62385086  -1.01596757  -1.60346104\n",
      "  15.51659352 -10.01204177  -5.37875616   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 3)  new position  (5, 2)\n",
      "128.0 4 35  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 2)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-22.429658   -32.69779518   0.         -22.09446822   0.\n",
      " -29.50940188   0.         -31.24192571   0.        ]\n",
      "max index list:  [2, 4, 6, 8]\n",
      "max_action:  5\n",
      "index max:  3\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 8)  new position  (3, 9)\n",
      "132.0 4 36  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  7.13965593 -18.67048333 -18.12273731  11.27897271  -2.01612375\n",
      " -22.83391636   2.15137828   4.07927187   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (5, 1)  new position  (4, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.39103024 -6.8132958  -3.36289713  7.70438756 -7.36487349  0.\n",
      "  2.33976568 -1.47406917  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 9)  new position  (4, 9)\n",
      "136.0 4 37  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 0.34299733 -2.60903048  0.          6.96527839 -2.18658923  0.9209786\n",
      " -0.03328074 -3.98190549  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 0)  new position  (3, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -6.02268675 -15.25619904 -13.39217372   5.32171176   4.19000279\n",
      "   2.87798673  -5.5527673    0.           0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 9)  new position  (3, 9)\n",
      "140.0 4 38  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-25.70682346   1.91299764  -1.36504718   0.          -1.95783451\n",
      "  14.02281789  -5.13066179   3.75966355   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 9)  new position  (4, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -2.29869679  14.42553693 -15.05868978  10.77307644 -12.36526175\n",
      "  12.26308538  -0.76810966   0.90776151   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 0)  new position  (2, 0)\n",
      "144.0 4 39  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.88441266   7.71144117   5.0289837   -8.89780953 -18.12088537\n",
      "   0.28347754  -4.03483666  -9.64092712   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 0)  new position  (1, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [0.4       0.4       0.        1.5809022 0.        0.        0.\n",
      " 0.4       0.       ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 8)  new position  (3, 8)\n",
      "148.0 4 40  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-0.1539661  -4.01368844 14.68937914 -0.39756335  0.88630627 -8.34488437\n",
      " -1.05069876  4.2575045   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 0)  new position  (0, 1)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [0.        0.7968    0.        0.4       0.        1.1904256 0.\n",
      " 0.        0.       ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 8)  new position  (3, 9)\n",
      "152.0 4 41  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 1)  new position  (9, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-5.24250943 -8.83943477 14.53417328  2.37960037  4.95460706  3.5387214\n",
      " -8.16420887  6.67776672  0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 9)  new position  (3, 0)\n",
      "156.0 4 42  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.04744736  -5.64635074   3.66599274  12.73537913 -25.13640359\n",
      "   3.20590757  -7.84987033  -9.80192907   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 2)  new position  (9, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 31.284968    -4.82813909 -10.67881149  -1.93536291   4.62396222\n",
      "   6.19813269  -5.04263151  -6.92183961   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (3, 0)  new position  (3, 1)\n",
      "160.0 4 43  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 1)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 1)  new position  (2, 0)\n",
      "164.0 4 44  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 0)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.13376359   0.55244619   0.82926859   6.67461715  20.98817125\n",
      "  15.01263039  -1.1080356  -12.02177202   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 0)  new position  (3, 0)\n",
      "168.0 4 45  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 0)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 31.284968    -4.82813909 -10.67881149  -1.93536291   4.62396222\n",
      "   6.19813269  -5.04263151  -6.92183961   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 0)  new position  (2, 9)\n",
      "172.0 4 46  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.57651127   7.72629917 -10.55591285  -0.57326429   2.25690737\n",
      "  -0.97810523  15.54622826  29.02055789   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 9)  new position  (3, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 0)  new position  (7, 1)\n",
      "176.0 4 47  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 31.284968    -4.82813909 -10.67881149  -1.93536291   4.62396222\n",
      "   6.19813269  -5.04263151  -6.92183961   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 0)  new position  (4, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 1)  new position  (6, 1)\n",
      "180.0 4 48  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.24979463  12.08678496   0.25557425 -10.63410889  11.29279005\n",
      "  10.83118175  -1.55123701   5.62332611   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 0)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.04011725   0.          -7.11438054 -11.58565826  20.18954968\n",
      "  19.31392902  15.07359785   1.35892936   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 1)  new position  (6, 0)\n",
      "184.0 4 49  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-12.25222494  -8.19921652   5.52711087  -4.78895076   2.00981552\n",
      "   6.8103209   -4.00404314   0.           0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 0)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.51459694  10.59865878 -11.28184541  -3.30558264   3.59602772\n",
      "  -9.5795194   11.3501866  -33.0510489    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 1)  new position  (4, 2)\n",
      "188.0 4 50  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 2)  new position  (3, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 0)  new position  (8, 9)\n",
      "192.0 4 51  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 9)  new position  (7, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  4.73977065   2.81570023  -2.44980913 -17.11226232  -3.17969996\n",
      "   6.75493258  25.64150678  18.70155119   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 1)  new position  (4, 0)\n",
      "196.0 4 52  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.78690545  6.15444352  3.85395187 -0.15385486 -5.38010634 15.5076787\n",
      " 14.73589801 13.31404824  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 0)  new position  (5, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-15.29653486  -7.0701285  -13.19895465 -25.67167333  -8.16140589\n",
      "  -7.91044292   0.         -10.74968812   0.        ]\n",
      "max index list:  [6, 8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 8)  new position  (6, 8)\n",
      "200.0 4 53  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-6.38378495 -3.52922784  0.         -9.89015119 -8.52982122 -5.85182864\n",
      "  0.         -5.48190315  0.        ]\n",
      "max index list:  [2, 6, 8]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 8)  new position  (5, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.25758403   4.05025537   0.         -11.68004608   2.11981485\n",
      "   1.21092607  -0.83558768  -7.98354435   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 0)  new position  (4, 0)\n",
      "204.0 4 54  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-13.55729219   7.24091789   5.81102051   3.65289763   0.\n",
      "  -9.97424811   5.2988351   -6.05859556   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 0)  new position  (3, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 14.52115544 -21.11479377 -13.26334659  -1.23846101  -4.93964641\n",
      " -14.35253006   4.15380409   2.57992463   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 9)  new position  (4, 0)\n",
      "208.0 4 55  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-1.24751507  0.         -6.46145844 -2.3907674  -5.65378812  0.70226694\n",
      " 14.34786442 -4.27000616  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 0)  new position  (3, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -5.62794129   2.52266167   0.         -10.03406338   0.27046152\n",
      " -14.31596012  -2.14496636   7.45462791   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 0)  new position  (2, 0)\n",
      "212.0 4 56  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -2.88416868 -17.09181984 -17.2521028  -12.59152919 -12.4199031\n",
      "   6.46421043   8.54724162   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 9)  new position  (2, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.21354459   7.61544056   0.          -6.78333649 -11.97203873\n",
      "   5.63352184  -8.44136553  -1.91823583   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 0)  new position  (2, 1)\n",
      "216.0 4 57  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.87702609   2.30111925 -24.56044064  -5.61123044   2.26187946\n",
      "   4.53944584  -2.18567101  -8.18736591   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (2, 9)  new position  (3, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 1)  new position  (2, 2)\n",
      "220.0 4 58  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  9.80521462 -15.96309796   4.78409449  -5.13222364 -13.33415301\n",
      " -13.03593811   3.35982094  19.32893409   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 2)  new position  (1, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [0.        0.7968    0.        0.4       0.        1.1904256 0.\n",
      " 0.        0.       ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  3\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (3, 8)  new position  (4, 9)\n",
      "224.0 4 59  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-3.72664211 -1.97610938  0.4605181  13.24895524  7.5974926  -1.16086855\n",
      "  5.16931     0.4316775   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 9)  new position  (4, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.66282809  -4.26618692   4.0766115  -11.41362679  10.69348545\n",
      "  11.03964625   1.67209256  -9.43224976   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 2)  new position  (2, 3)\n",
      "228.0 4 60  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.68256275 -16.7174098    2.02334361 -19.31482151  10.02558202\n",
      "  10.69528709  12.2001919    2.2586472    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 3)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.78690545  6.15444352  3.85395187 -0.15385486 -5.38010634 15.5076787\n",
      " 14.73589801 13.31404824  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 0)  new position  (5, 0)\n",
      "232.0 4 61  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-49.93923146 -55.67069264 -64.68938459 -62.39260348 -59.05012011\n",
      " -53.71221622 -52.52042591   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 4)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 0)  new position  (4, 0)\n",
      "236.0 4 62  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "I havent seen this before, picking randomly\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 5)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.78690545  6.15444352  3.85395187 -0.15385486 -5.38010634 15.5076787\n",
      " 14.73589801 13.31404824  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 0)  new position  (4, 1)\n",
      "240.0 4 63  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.91631167  -9.40148994  -9.11867023 -16.11964308 -14.68625626\n",
      "  15.86562523 -10.67987709  -2.1308996    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 1)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-54.32805394   0.         -63.46530657   0.         -61.99524083\n",
      "   0.         -53.37528749   0.           0.        ]\n",
      "max index list:  [1, 3, 5, 7, 8]\n",
      "max_action:  6\n",
      "index max:  3\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 5)  new position  (1, 4)\n",
      "244.0 4 64  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-66.69323341 -52.90264387 -48.06335468   0.         -62.51970143\n",
      " -76.98520587 -59.1176826  -51.20689783   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 4)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 1)  new position  (6, 0)\n",
      "248.0 4 65  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.         -26.28149844 -13.90777681   1.26203274 -37.85780123\n",
      "   0.         -31.23643274 -33.57173093   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 5)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 0)  new position  (7, 9)\n",
      "252.0 4 66  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 9)  new position  (7, 8)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 4)  new position  (0, 4)\n",
      "256.0 4 67  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.86126497  -7.64853507 -24.73028016 -22.18007293 -20.5990663\n",
      " -15.40596975   0.         -25.58865073   0.        ]\n",
      "max index list:  [6, 8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 8)  new position  (8, 7)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-44.0662643  -29.22028431 -31.17339583 -27.16213945 -26.72905924\n",
      "   0.         -26.5790402  -27.89015804   0.        ]\n",
      "max index list:  [5, 8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 4)  new position  (0, 5)\n",
      "260.0 4 68  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.         -16.76923766 -11.75650747 -13.34022793 -13.88648884\n",
      "   0.          -3.7752554   -4.77743755   0.        ]\n",
      "max index list:  [0, 5, 8]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 5)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-10.07688183 -28.80657813  -8.45533959  -0.51490706  -5.02334143\n",
      "  27.05083553 -18.10199168  -4.02836738   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 7)  new position  (9, 8)\n",
      "264.0 4 69  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 4)  new position  (8, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 8)  new position  (9, 9)\n",
      "268.0 4 70  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-21.29233052  -0.90155169  -4.80159657  -5.44809757  -7.46637592\n",
      "   9.1555158   -1.30919474  -0.7117906    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 9)  new position  (9, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 3)  new position  (8, 2)\n",
      "272.0 4 71  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -3.02449339   3.54376325 -11.24533114  10.65252991  14.91381551\n",
      "  24.48958529   2.77216248 -19.26298893   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 8)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 2)  new position  (7, 3)\n",
      "276.0 4 72  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 9)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 3)  new position  (6, 3)\n",
      "280.0 4 73  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [11.24702584 -6.6158547  -2.69336094 25.34191443 -9.49797965  9.46057546\n",
      " 10.0731984  -9.20881943  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 0)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -3.35936419  12.14963837   1.84434718 -29.86547573   5.17617944\n",
      " -27.53489966  -1.87480419 -31.38996799   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 3)  new position  (7, 3)\n",
      "284.0 4 74  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -8.07118229   0.33000369  -2.16150902   2.86268671 -19.53716877\n",
      " -24.10111346  -8.92046959 -13.56492484   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 1)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 3)  new position  (6, 4)\n",
      "288.0 4 75  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 0)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-50.8465978  -59.44023139 -45.81310149   0.         -43.4949661\n",
      " -57.77484227 -49.15655625 -49.00790923   0.        ]\n",
      "max index list:  [3, 8]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 4)  new position  (7, 5)\n",
      "292.0 4 76  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 0)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-33.75832968   0.         -35.01569024 -27.83497248 -25.25622167\n",
      " -36.40610341 -27.59878094   0.           0.        ]\n",
      "max index list:  [1, 7, 8]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 5)  new position  (7, 6)\n",
      "296.0 4 77  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 9)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-23.55340442   0.         -22.71395271 -35.44510746 -11.99189123\n",
      " -32.07197023   0.           0.           0.        ]\n",
      "max index list:  [1, 6, 7, 8]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 6)  new position  (7, 5)\n",
      "300.0 4 78  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-33.75832968   0.         -35.01569024 -27.83497248 -25.25622167\n",
      " -36.40610341 -27.59878094   0.           0.        ]\n",
      "max index list:  [1, 7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 5)  new position  (8, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 0)  new position  (6, 0)\n",
      "304.0 4 79  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 0)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 5)  new position  (9, 5)\n",
      "308.0 4 80  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 5)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 1)  new position  (5, 2)\n",
      "312.0 4 81  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 2)  new position  (6, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 4)  new position  (0, 3)\n",
      "316.0 4 82  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 2)  new position  (5, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 12.57281752  -0.75624967   1.71869118 -13.51084326   2.20879022\n",
      " -12.15224776  11.07685238   2.84777      0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 3)  new position  (0, 2)\n",
      "320.0 4 83  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 1)  new position  (4, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 2)  new position  (1, 3)\n",
      "324.0 4 84  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (1, 3)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.91631167  -9.40148994  -9.11867023 -16.11964308 -14.68625626\n",
      "  15.86562523 -10.67987709  -2.1308996    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 1)  new position  (5, 1)\n",
      "328.0 4 85  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 1)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-49.93923146 -55.67069264 -64.68938459 -62.39260348 -59.05012011\n",
      " -53.71221622 -52.52042591   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 4)  new position  (3, 4)\n",
      "332.0 4 86  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "I havent seen this before, picking randomly\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 4)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-4.21739388e+00 -6.63685610e+00 -1.41003625e+01 -2.48022143e+00\n",
      " -1.90468451e-01 -2.24318194e+01  1.12579666e-02 -9.69685765e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 1)  new position  (5, 2)\n",
      "336.0 4 87  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 2)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-49.93923146 -55.67069264 -64.68938459 -62.39260348 -59.05012011\n",
      " -53.71221622 -52.52042591   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 4)  new position  (2, 5)\n",
      "340.0 4 88  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 3)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-54.32805394   0.         -63.46530657   0.         -61.99524083\n",
      "   0.         -53.37528749   0.           0.        ]\n",
      "max index list:  [1, 3, 5, 7, 8]\n",
      "max_action:  3\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 5)  new position  (2, 4)\n",
      "344.0 4 89  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 2)  new position  (4, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -6.84521587  -8.14393736  -4.6776784  -10.15850773  -7.6217744\n",
      "  -7.84700283  -3.69310848   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 4)  new position  (2, 5)\n",
      "348.0 4 90  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-54.32805394   0.         -63.46530657   0.         -61.99524083\n",
      "   0.         -53.37528749   0.           0.        ]\n",
      "max index list:  [1, 3, 5, 7, 8]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 5)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 2)  new position  (5, 1)\n",
      "352.0 4 91  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "I havent seen this before, picking randomly\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (3, 5)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 1)  new position  (5, 0)\n",
      "356.0 4 92  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-54.32805394   0.         -63.46530657   0.         -61.99524083\n",
      "   0.         -53.37528749   0.           0.        ]\n",
      "max index list:  [1, 3, 5, 7, 8]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 5)  new position  (3, 5)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 0)  new position  (5, 1)\n",
      "360.0 4 93  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "I havent seen this before, picking randomly\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (3, 5)  new position  (4, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 1)  new position  (6, 2)\n",
      "364.0 4 94  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 2)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "I havent seen this before, picking randomly\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (4, 5)  new position  (3, 4)\n",
      "368.0 4 95  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "I havent seen this before, picking randomly\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 4)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -3.98648298  -4.48844753 -15.60315641   4.81354864 -20.12075586\n",
      " -17.57203565  -1.47342059  -1.4156848    0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 2)  new position  (4, 3)\n",
      "372.0 4 96  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 2.70475348  0.         -3.8738046   6.0630691  -1.48127289 11.18783186\n",
      " 18.1542991   1.98371708  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (4, 3)  new position  (3, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [11.30839527 -4.37996715  8.9742515   0.         20.81981206 -9.43903642\n",
      " 11.23576606  1.89311581  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 3)  new position  (2, 4)\n",
      "376.0 4 97  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "I havent seen this before, picking randomly\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (3, 4)  new position  (3, 3)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -6.36096672  -2.30084431 -13.59864677  -7.72872905   0.\n",
      "  -6.12924901  -4.51306508   0.           0.        ]\n",
      "max index list:  [4, 7, 8]\n",
      "max_action:  4\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 4)  new position  (2, 5)\n",
      "380.0 4 98  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -6.11414771   0.          -7.40294342   0.         -10.41460475\n",
      "   0.          -2.6682903    0.           0.        ]\n",
      "max index list:  [1, 3, 5, 7, 8]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 5)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.69931406   2.8824375  -13.08688145  15.20064426   4.27143766\n",
      "  16.90044104   1.48640802   0.           0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 3)  new position  (4, 2)\n",
      "384.0 4 99  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -6.84521587  -8.14393736  -4.6776784  -10.15850773  -7.6217744\n",
      "  -7.84700283  -3.69310848   0.           0.        ]\n",
      "max index list:  [7, 8]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 4)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-23.39723461 -17.5425382  -20.25299658 -12.866008   -14.74471616\n",
      "  -2.26152501  -6.70812199 -22.3874861    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (4, 2)  new position  (5, 1)\n",
      "388.0 4 100  Episode:  8\n",
      "cows in goal:  4 , previous_cow_count:  4  reward:  -1.0\n",
      "all cows herded in model!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n",
      "creating cow agent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating trained mc agent with vision range  2\n",
      "creating trained mc agent with vision range  2\n",
      "Monte Carlo Episode  9\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "I havent seen this before, picking randomly\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 8)  new position  (7, 7)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "I havent seen this before, picking randomly\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 3)  new position  (7, 2)\n",
      "0.0 0 1  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0.0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 0.944       0.         -1.17782917  3.77798573 -0.11797038  3.00617183\n",
      "  0.          6.58589548  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 7)  new position  (6, 8)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-6.22035375 -1.06102235  6.70916659  5.81841035 10.81074495 10.14305558\n",
      "  2.00171301 -8.05309682  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 2)  new position  (7, 3)\n",
      "0.0 0 2  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 3)  new position  (7, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-0.25148262  0.02992389  0.          0.87296     0.08091443 -1.272\n",
      "  0.          2.12        0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 8)  new position  (6, 9)\n",
      "0.0 0 3  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 2)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -4.34381594  13.8109085    3.06951426 -14.88499384 -12.2170721\n",
      "  -1.46520377  16.20487626   1.12953766   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 9)  new position  (5, 0)\n",
      "0.0 0 4  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-14.63341451  16.14675734 -13.16855406  -3.30791631   6.88781269\n",
      "   0.          15.41594673 -18.46137712   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 0)  new position  (6, 9)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -4.7675306   -7.57465156  -7.1857799   10.70232049  -1.38283779\n",
      " -11.6813405    2.04732412  14.25547312   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 1)  new position  (6, 2)\n",
      "0.0 0 5  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 2)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -4.24702509 -14.13072768   0.9504665    0.83209999  -1.26152662\n",
      "  12.25639219  -7.01730338   9.02650008   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 9)  new position  (6, 0)\n",
      "0.0 0 6  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (6, 0)  new position  (7, 9)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 3)  new position  (8, 2)\n",
      "0.0 0 7  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 9)  new position  (8, 9)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 2)  new position  (9, 2)\n",
      "0.0 0 8  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.04744736  -5.64635074   3.66599274  12.73537913 -25.13640359\n",
      "   3.20590757  -7.84987033  -9.80192907   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 2)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  6.02697299  17.33008228 -14.70041933  -7.48229855   3.05321735\n",
      "  -1.97716282  -9.02170967   1.06219893   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 9)  new position  (7, 0)\n",
      "0.0 0 9  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (9, 3)  new position  (8, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 15.83390824  -2.48785087 -15.98979175   0.8179071   -6.65361042\n",
      "   1.63234043  -0.0524492    2.52930912   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 0)  new position  (7, 1)\n",
      "0.0 0 10  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -8.81668382 -12.81233178 -18.95391419  -0.87654351  -5.75805041\n",
      "  -4.78130517   0.         -15.86084123   0.        ]\n",
      "max index list:  [6, 8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 2)  new position  (8, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.          -5.21528567  -8.08746122  -1.16366632   1.35850676\n",
      " -11.57535783 -11.03847905  -0.30329203   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 1)  new position  (8, 2)\n",
      "0.0 0 11  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.98228202  -0.72337414   0.          -1.23664782  -5.70960093\n",
      " -10.08535962   6.14068796  23.26784354   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 2)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 8.54738602  0.         -5.66776232 -2.30735431  6.22688228 -3.81019007\n",
      " 23.84548315 -1.76671935  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 1)  new position  (8, 0)\n",
      "0.0 0 12  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.96611446  -7.65116002 -10.70929269   4.65757676   0.\n",
      "  19.21092325 -11.47948805  -1.68203827   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 1)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [ -1.83369542   0.         -15.40463455  -9.27413769   0.5308924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -5.30498368   8.97130456  -6.92731796   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 0)  new position  (8, 1)\n",
      "0.0 0 13  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -6.49522949   0.8273087  -12.90538512  -6.25954954  13.33983632\n",
      "  -3.45658394   0.          -2.2577168    0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 1)  new position  (7, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -9.82450512  -0.89403116  20.06955841 -10.17175174 -13.73004827\n",
      "   7.37264771   3.16739316 -11.592771     0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 0)  new position  (7, 9)\n",
      "0.0 0 14  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 2)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -7.7570729   -6.15643969   0.22763399 -17.74841309  14.38006568\n",
      "  -9.2499113    6.19395807 -11.46650165   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 9)  new position  (7, 0)\n",
      "0.0 0 15  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -0.2578905    7.86470381  17.8128067   -3.89376523   0.\n",
      " -19.64660345  -5.73078712  -8.92872093   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 1)  new position  (6, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  1.49194904   3.04511779  -9.16901272   1.61079245  -3.02148133\n",
      "  -1.86482157   5.50650278 -12.07827213   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 0)  new position  (8, 9)\n",
      "0.0 0 16  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 2)  new position  (6, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  3.32101517 -12.05667953 -12.61848405 -12.11990549  -7.57995017\n",
      "   2.67841053  -5.95040143  11.04296771   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 9)  new position  (9, 0)\n",
      "0.0 0 17  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-2.08893966 -1.01020402 -2.0033564  -2.27421814 -6.07671871 31.51405898\n",
      "  6.23139027 10.0654985   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 0)  new position  (8, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  3.20568849  -2.72101115  -8.06494224  10.08376345  -0.09041242\n",
      " -11.00498802   2.84877379 -21.88231562   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 1)  new position  (5, 1)\n",
      "0.0 0 18  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.28642936  -4.18684621  -6.32962871 -12.67237532 -11.66958614\n",
      "  -4.66417748  16.14641489  -6.73474145   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 1)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  7\n",
      "in max action with choice\n",
      "Q state is  [ 7.48264161  0.07669582 12.15570631 17.24552519 -2.69173814  3.32208797\n",
      "  3.74598148 -6.53482845  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 0)  new position  (7, 9)\n",
      "0.0 0 19  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -3.29221699  -2.68182964 -10.41318018   2.23850265   8.62641153\n",
      " -16.61742452  -7.44956499   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 9)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.           3.32011144 -10.19153667   1.42337179  -5.80587771\n",
      "  -7.8160771   17.03771639  13.96222444   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 0)  new position  (5, 0)\n",
      "0.0 0 20  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -8.92069032 -12.49157747  -3.59343327  -2.8755857    6.49624622\n",
      "  12.95373795  -5.71155081  -3.01972272   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 0)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [20.3356824  -0.52016835  0.49415037 -1.32175052 -0.22383446  0.93927446\n",
      " -2.13508927 -1.09035564  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 0)  new position  (4, 1)\n",
      "0.0 0 21  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 1)  new position  (7, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.91631167  -9.40148994  -9.11867023 -16.11964308 -14.68625626\n",
      "  15.86562523 -10.67987709  -2.1308996    0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 1)  new position  (3, 1)\n",
      "0.0 0 22  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.26135617  18.35143684  -4.65486268  -3.76495528  17.07947243\n",
      "   7.91380443 -14.23095281  -3.05438243   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 1)  new position  (2, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.11788638  17.07866694 -15.36015177  -5.17033109   9.82974036\n",
      " -10.74692081  -1.44904036   4.90031145   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (7, 0)  new position  (7, 9)\n",
      "0.0 0 23  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 15.72328214  -1.14707659  -2.5149729  -14.19497799   9.27101811\n",
      "  -8.91326301  -4.8053784    6.74686365   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (2, 0)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 12.10254678  12.86554237 -22.93306928   4.80977379  12.87658053\n",
      "  -0.89745615  -1.34023323  22.94841187   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 9)  new position  (6, 8)\n",
      "0.0 0 24  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 1)  new position  (1, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [13.82148163 25.37561696  0.          8.2209153  18.64746376  7.86536258\n",
      "  0.         39.5350921   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (6, 8)  new position  (7, 8)\n",
      "0.0 0 25  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-4.40641975  0.21190832  2.65277038  8.68285583  5.56110801 -3.90005816\n",
      "  1.45830992  1.23385754  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 2)  new position  (0, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 1.71163303  5.56985383  2.5412142  25.58548787  8.48147069  7.12962517\n",
      "  0.          1.7325241   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 8)  new position  (6, 9)\n",
      "0.0 0 26  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-0.92992896  3.79348877 -3.77232175  2.11266442  0.30819873 -1.06290284\n",
      "  3.52907184  1.6236739   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 3)  new position  (9, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -4.24702509 -14.13072768   0.9504665    0.83209999  -1.26152662\n",
      "  12.25639219  -7.01730338   9.02650008   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 9)  new position  (5, 0)\n",
      "0.0 0 27  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 12.52504499 -14.4579551    0.36718675  -1.46168502 -14.70346603\n",
      "   3.82240708  -7.11358421  12.05802251   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 0)  new position  (6, 0)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.04744736  -5.64635074   3.66599274  12.73537913 -25.13640359\n",
      "   3.20590757  -7.84987033  -9.80192907   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 2)  new position  (0, 1)\n",
      "0.0 0 28  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 1)  new position  (9, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 0)  new position  (7, 1)\n",
      "0.0 0 29  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-3.48296044e+00  4.06522677e+00  3.89749411e+00 -9.49628096e+00\n",
      "  1.20737634e-02 -1.85724191e+01  9.91282813e+00 -5.73983377e+00\n",
      "  0.00000000e+00]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 1)  new position  (8, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  3.27422331   0.         -10.483211   -10.18339044  15.70026709\n",
      "  -3.37429169 -16.31667894  -8.28682552   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 2)  new position  (0, 3)\n",
      "0.0 0 30  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-7.28544122 -7.58916248 -0.37792941 -1.36247262 -7.20246224 -7.3436945\n",
      " -9.01240103 -8.98012277  0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 3)  new position  (1, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 2)  new position  (8, 1)\n",
      "0.0 0 31  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.66282809  -4.26618692   4.0766115  -11.41362679  10.69348545\n",
      "  11.03964625   1.67209256  -9.43224976   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 2)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -0.8268303  -15.96017114 -28.4152045   13.72221819  -2.95364657\n",
      " -17.67057092  -7.33161461   0.30599302   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 1)  new position  (8, 2)\n",
      "0.0 0 32  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-21.39932316   0.45876666  -2.29564724 -23.08025763 -21.7612374\n",
      "  -7.85535023  15.10616159   3.62853592   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 2)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -2.48226555 -17.68605023  -5.10088468  -5.23706893  -6.45408913\n",
      "   0.          -9.79143552  -8.51353667   0.        ]\n",
      "max index list:  [5, 8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 2)  new position  (7, 1)\n",
      "0.0 0 33  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.76435009  -2.65709896   1.96572225 -19.68420885  -5.2717481\n",
      "  -8.16424701  -8.51188991 -18.14744611   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 3)  new position  (0, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 1)  new position  (8, 2)\n",
      "0.0 0 34  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-9.01071681 -4.41477086  8.96804753 19.98065796 -0.31706278  0.\n",
      "  8.42694419 13.0248104   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 4)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 2)  new position  (7, 1)\n",
      "0.0 0 35  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 1)  new position  (6, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 3)  new position  (2, 3)\n",
      "0.0 0 36  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.68256275 -16.7174098    2.02334361 -19.31482151  10.02558202\n",
      "  10.69528709  12.2001919    2.2586472    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 3)  new position  (1, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 2)  new position  (7, 3)\n",
      "0.0 0 37  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (7, 3)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 24.54441861   4.12392875 -10.911081    -7.08131788 -21.72653424\n",
      "  -7.23925356   7.48156502 -18.43571935   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 3)  new position  (0, 4)\n",
      "0.0 0 38  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 3)  new position  (6, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [12.05554415 -3.58481019  2.73739827 -7.78074968  2.25650446  0.\n",
      " -4.44998765 -6.58215103  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 4)  new position  (9, 3)\n",
      "0.0 0 39  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [16.8360679  26.22068986 24.15256772  0.         34.806638    1.29563796\n",
      " 38.83026388 29.00790848  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (6, 4)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 3)  new position  (8, 3)\n",
      "0.0 0 40  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.55407961   2.045442    -6.28507824  -8.05169327   2.47490484\n",
      "  -6.38457339 -15.12740913 -10.87993894   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (6, 3)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 3)  new position  (8, 4)\n",
      "0.0 0 41  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (5, 2)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-14.67429578 -12.24215203 -20.5801667  -17.51072018   9.35698949\n",
      "   6.0546727   -1.49622716 -14.22890975   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (8, 4)  new position  (8, 5)\n",
      "0.0 0 42  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  9.79361913 -18.70526073  -0.22616549 -10.55335532 -12.53772021\n",
      "  -1.19395446 -16.1051999   -2.94638343   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 3)  new position  (5, 3)\n",
      "0.0 0 43  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 6)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.00746759 -11.1412527  -19.62385086  -1.01596757  -1.60346104\n",
      "  15.51659352 -10.01204177  -5.37875616   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (5, 3)  new position  (5, 4)\n",
      "0.0 0 44  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 4)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (0, 7)  new position  (0, 6)\n",
      "0.0 0 45  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 4)  new position  (5, 5)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.          16.9698146    6.44973079 -21.96511259   0.\n",
      "   0.          14.43365271  -6.44731182   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 6)  new position  (9, 6)\n",
      "0.0 0 46  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 5)  new position  (4, 6)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (9, 6)  new position  (0, 6)\n",
      "0.0 0 47  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.          16.9698146    6.44973079 -21.96511259   0.\n",
      "   0.          14.43365271  -6.44731182   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  1\n",
      "index max:  0\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 6)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -4.84933223 -28.48809138 -58.16988683   0.         -45.67297795\n",
      "   0.         -56.72299117   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 6)  new position  (3, 6)\n",
      "0.0 0 48  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 6)  new position  (8, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.90260846 -56.34286608 -32.56947316   0.         -36.81589887\n",
      "   0.         -46.41929819   0.           0.        ]\n",
      "max index list:  [3, 5, 7, 8]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 6)  new position  (2, 5)\n",
      "0.0 0 49  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-19.25776765 -16.71693457   1.14421871  -7.78298141 -19.78017513\n",
      "   2.83826722 -12.45137718   6.02343751   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 6)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-11.5691717    0.          18.83737322  12.54358916   7.61177462\n",
      " -18.91666582  -3.0443907    0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 5)  new position  (3, 5)\n",
      "0.0 0 50  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 6)  new position  (9, 7)\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 5)  new position  (2, 6)\n",
      "0.0 0 51  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-0.4139219   0.          9.4992501   0.          7.46749503  0.\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (2, 6)  new position  (2, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 7)  new position  (9, 6)\n",
      "0.0 0 52  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-11.5691717    0.          18.83737322  12.54358916   7.61177462\n",
      " -18.91666582  -3.0443907    0.           0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 5)  new position  (1, 4)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 1.07186398 -2.01986086 -5.33188534  4.80219303 -4.0750914   7.38604701\n",
      " 23.89498087 -0.76254897  0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 6)  new position  (0, 5)\n",
      "0.0 0 53  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [21.06287035 -4.90829349 10.80797482  0.         -1.94073325 -1.28154311\n",
      " -4.34445544  0.          0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (1, 4)  new position  (2, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  0.          10.36230689 -12.45419567   3.72050516   9.89856079\n",
      "   0.          11.75897734  19.70112625   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  6\n",
      "index max:  4\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 5)  new position  (9, 4)\n",
      "0.0 0 54  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 4)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [ -2.24777823  -0.03440161  -9.69044051   6.45263108 -22.79709799\n",
      "   2.73545365 -14.58901177   0.           0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 4)  new position  (3, 5)\n",
      "0.0 0 55  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  0\n",
      "in max action with choice\n",
      "Q state is  [-37.35221081 -49.11348962 -40.46631761 -41.2108146  -44.81215695\n",
      " -40.89781953 -11.84017609 -28.69331761   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (3, 5)  new position  (4, 4)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 5)  new position  (9, 6)\n",
      "0.0 0 56  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 6)  new position  (0, 7)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (4, 4)  new position  (4, 5)\n",
      "0.0 0 57  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  0.          18.07094995   0.45536216 -11.52361381   0.\n",
      "   6.81912032  11.5985063    3.48537264   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  0\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 7)  new position  (9, 7)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-37.03956589 -30.43405104 -42.69836143 -55.6345795  -57.90337094\n",
      " -36.0298802  -37.69723262 -48.03552549   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (4, 5)  new position  (5, 6)\n",
      "0.0 0 58  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 13.13505886  -2.38559846   7.45239056  -1.76423344 -13.39075526\n",
      "  -6.77657508 -14.92202395  -8.00128338   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 7)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 0.         10.6286564  18.21014817  0.          0.          0.\n",
      " 10.65163663  0.          0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 6)  new position  (5, 5)\n",
      "0.0 0 59  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 6)  new position  (0, 5)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  0.           0.08310326 -24.27605142  -3.66703023  27.14543019\n",
      "   0.           5.60764425  -1.21709173   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  1\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (5, 5)  new position  (5, 4)\n",
      "0.0 0 60  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ 0.         -5.31693078  6.00305672 23.0319424  19.73782956  0.\n",
      " -1.75260929 -8.44580215  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  5\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (5, 4)  new position  (4, 4)\n",
      "0.0 0 61  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.98518543   7.42837574   4.29024218  -0.29544516  11.3112998\n",
      "   1.2173654  -17.62208328  -8.32983349   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 6)  new position  (9, 5)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-43.18743644 -49.55585018 -37.7332419  -48.36550077 -56.79621586\n",
      " -30.85965643 -32.97035704 -40.46279982   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (4, 4)  new position  (3, 4)\n",
      "0.0 0 62  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 5)  new position  (9, 4)\n",
      "trained monte carlo step\n",
      "distance is  1\n",
      "in max action with choice\n",
      "Q state is  [-42.82087061 -36.97603648 -40.37821211 -63.24831962 -35.21881822\n",
      " -46.65772308 -36.22761687 -27.25788312   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 4)  new position  (2, 3)\n",
      "0.0 0 63  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-28.60555665   7.69636681  -4.85987721  -6.9134368  -17.99775945\n",
      "  16.20698294  -0.96043716  -3.53919139   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 4)  new position  (0, 3)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ -7.82051979  -7.57493878 -11.50269581   0.3338312   -4.06070169\n",
      "  -0.42477908 -20.74094973 -23.96942793   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (2, 3)  new position  (1, 2)\n",
      "0.0 0 64  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [0.    0.    0.352 0.    0.    0.    0.    0.    0.   ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (1, 2)  new position  (2, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -4.18191896  -6.14338103  -1.46567588 -11.8716925   -6.16674821\n",
      " -19.81417783   1.07983101   5.55146947   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (0, 3)  new position  (9, 3)\n",
      "0.0 0 65  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (9, 3)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-12.55430324  -5.17118408   1.881834    -9.14387339  -6.0041502\n",
      " -13.57092609  -0.81722612   2.59700929   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (2, 1)  new position  (2, 2)\n",
      "0.0 0 66  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [  8.55630126 -31.16616451 -17.988619    -3.31225786  -2.66314031\n",
      "   7.34715083   2.00864979   5.98680392   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (2, 2)  new position  (3, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (0, 2)  new position  (9, 3)\n",
      "0.0 0 67  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (3, 2)  new position  (2, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 11.26293783   7.30542895  -8.25626547   1.84893155   8.34267888\n",
      "  -2.31374026 -20.58636131  18.29843565   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 3)  new position  (8, 4)\n",
      "0.0 0 68  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 4)  new position  (7, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  6.38614483   7.80653461  -4.61850534  12.5048482   -7.46066077\n",
      " -22.89559775 -11.53968882  -2.23522653   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (2, 1)  new position  (3, 2)\n",
      "0.0 0 69  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -5.32807638 -16.47751163  -1.27173509 -19.32888232  -2.02888655\n",
      " -11.69300175 -12.85758914   9.40124041   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (7, 3)  new position  (8, 3)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-20.99339125 -12.15126356   3.96940559   6.98706327   4.49319568\n",
      "  -7.4680197   -5.35092639 -10.26721623   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (3, 2)  new position  (2, 3)\n",
      "0.0 0 70  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.40365888 -18.40504434  13.27696989 -17.10380792  -0.48963375\n",
      "   4.16066226   5.65887427  19.82584869   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 3)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [-14.68256275 -16.7174098    2.02334361 -19.31482151  10.02558202\n",
      "  10.69528709  12.2001919    2.2586472    0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (2, 3)  new position  (1, 3)\n",
      "0.0 0 71  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -0.95006707  -4.38095309  -3.99171717 -22.57037976 -17.61884865\n",
      "  -1.14296873  -5.28443411  -7.7881217    0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 3)  new position  (9, 2)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [-11.36303429   7.2313707    0.21736773   1.33474685 -15.38095845\n",
      "   3.02441246 -10.71073496   0.88877142   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (1, 3)  new position  (1, 2)\n",
      "0.0 0 72  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -0.79761656  -8.69093511  -8.06619647   3.89452592 -12.82697499\n",
      "   5.01337831 -17.96587528  -2.92761492   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 2)  new position  (9, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-11.13656306  -2.34978131 -10.80859449  -1.07542513   3.75238168\n",
      "  -4.30916511  -9.32701915 -14.55468465   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 2)  new position  (0, 1)\n",
      "0.0 0 73  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  3.87748265   0.           3.16223242  -7.74589436  -4.62626122\n",
      "   4.71621636  -4.91488971 -17.70727612   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (0, 1)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -6.86170392  18.09462603 -11.67824019 -12.532844     5.26154295\n",
      "   0.           1.60618591   2.96476963   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  6\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 1)  new position  (8, 2)\n",
      "0.0 0 74  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -8.81249337   5.15481602   8.25299428 -13.40307439  -5.70830617\n",
      "   3.93854474   4.97008415  -9.05100994   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (8, 2)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 2)  new position  (1, 1)\n",
      "0.0 0 75  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 1)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (7, 1)  new position  (6, 0)\n",
      "0.0 0 76  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-9.55634643  4.94838291  3.73179354 -7.75392202 -0.16564707 12.77678407\n",
      "  7.31935332 -3.22380241  0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 0)  new position  (7, 1)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (0, 0)  new position  (1, 1)\n",
      "0.0 0 77  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (1, 1)  new position  (0, 0)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  2.19381093 -11.72255547  -8.2123566   12.60516641  -3.69874732\n",
      " -18.49010927 -10.38680141  -5.17768385   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 1)  new position  (6, 2)\n",
      "0.0 0 78  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-14.39105233  -7.18759408 -12.23963615  -0.6372302   -6.10389668\n",
      "  -8.48240544 -16.52187747  13.04479254   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (6, 2)  new position  (5, 2)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 5.1560954   2.01184216  1.20793494 -1.89864745 21.77349855  8.1135236\n",
      " -8.7425939   9.80998547  0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (0, 0)  new position  (1, 0)\n",
      "0.0 0 79  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.42920102  -9.49988162 -23.43851053 -14.5021906  -15.42000625\n",
      " -11.24705584 -17.70845536   7.41756812   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (5, 2)  new position  (4, 3)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [13.12601014 12.14844637 -1.10361793 14.52261584 -4.11906788 -0.46161736\n",
      "  7.60447512 -0.15495821  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (1, 0)  new position  (1, 1)\n",
      "0.0 0 80  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (1, 1)  new position  (0, 1)\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [  3.32512587  -2.93962206 -12.27288836 -12.59513904 -11.90291207\n",
      "  16.69008476  -3.84143457  -7.97859842   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (4, 3)  new position  (5, 3)\n",
      "0.0 0 81  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 13.20883432 -20.31916903  11.08377425  -8.94819927  -7.27189932\n",
      "  12.17274065 -11.97260925 -25.63266435   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 1)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -4.00746759 -11.1412527  -19.62385086  -1.01596757  -1.60346104\n",
      "  15.51659352 -10.01204177  -5.37875616   0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (5, 3)  new position  (6, 3)\n",
      "0.0 0 82  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  1.61196042  -0.91928092 -18.12651379  -0.56419219  23.96169147\n",
      "   2.96590431 -20.7289637   -0.39510389   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (6, 3)  new position  (5, 4)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 0)  new position  (8, 1)\n",
      "0.0 0 83  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  2\n",
      "in max action with choice\n",
      "Q state is  [ 11.77047888 -20.90405729 -13.58497487   9.73262676   3.03202156\n",
      "   0.         -18.75402057  16.1516039    0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (5, 4)  new position  (6, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [20.56400437 -8.09864703  9.49830351 -5.27706666 -4.71466343 -1.55248731\n",
      " -4.14364984  6.60595807  0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  0\n",
      "index max:  0\n",
      "action:  0\n",
      "The max action is  0\n",
      "action word  0\n",
      "The action I choose is  UP\n",
      "Old location  (8, 1)  new position  (9, 1)\n",
      "0.0 0 84  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 1)  new position  (8, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-13.34124356 -10.56197088   5.01052864 -14.97066239  -1.52677878\n",
      "  -9.28177426 -20.22745031   4.52818066   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (6, 3)  new position  (6, 4)\n",
      "0.0 0 85  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  3\n",
      "in max action with choice\n",
      "Q state is  [ -7.08047033 -18.47128856   3.14491113   0.          14.01800978\n",
      "  -3.64287972  -3.96293517  23.38636514   0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  5\n",
      "index max:  5\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (6, 4)  new position  (7, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-18.19288306  -3.95256161   7.32841249 -13.30402027   4.50786223\n",
      "   4.75941516  -6.9629264  -13.8109806    0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 2)  new position  (9, 1)\n",
      "0.0 0 86  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -6.06130236   0.           4.31946053  12.32959837  -0.93909887\n",
      " -11.20154232  -5.8628412    0.           0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  3\n",
      "index max:  2\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (7, 5)  new position  (7, 6)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-12.62015654  13.68392642  -3.32426534  -3.84299757  -8.05823338\n",
      "  -0.29320965  -4.22433034   9.54222873   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (9, 1)  new position  (9, 0)\n",
      "0.0 0 87  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 0.072       0.         -0.46278656  2.50346598 -1.01664     3.83072943\n",
      "  0.          0.          0.        ]\n",
      "max index list:  [5]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 6)  new position  (8, 5)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -9.1830557   -1.82818327  13.90641964  -7.95393825  13.33209394\n",
      "   7.71330786 -16.78133819   4.87864956   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  3\n",
      "index max:  3\n",
      "action:  3\n",
      "The max action is  3\n",
      "action word  3\n",
      "The action I choose is  RIGHT\n",
      "Old location  (9, 0)  new position  (9, 1)\n",
      "0.0 0 88  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.12865047  -3.30267292   8.87733413   7.0500259  -20.98256701\n",
      "  19.46923766  21.92106381   2.37623948   0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (9, 1)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ 2.11684532 -4.62687229  0.6044095   7.06761317 -2.52421908 -7.74736749\n",
      "  4.59232263  1.26629386  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 5)  new position  (8, 4)\n",
      "0.0 0 89  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-17.36112948  -7.00031114  -8.94310554 -20.19554192  -5.71550601\n",
      "  -1.54812084  -5.60400036  -4.99549304   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 2)  new position  (1, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  0.93622483  -4.38945879  28.04348516 -17.29739325  -7.15538291\n",
      "  -1.27843327 -25.66158797 -39.50138201   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 4)  new position  (9, 5)\n",
      "0.0 0 90  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 7.44368514  0.34671582  7.02371165  3.98884401 -0.82229189 -6.5916102\n",
      "  0.97308311 13.42734062  0.        ]\n",
      "max index list:  [7]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 1)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.83309208  -1.06409624 -10.22750922   5.95044358  -9.14767946\n",
      " -14.23324168 -25.66983415 -31.72239528   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 5)  new position  (8, 5)\n",
      "0.0 0 91  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (8, 5)  new position  (7, 5)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-1.00740248 -4.31518872  8.68418149 29.14482694 -6.50579345 -5.00365428\n",
      " -0.94404219 -2.63368406  0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (0, 2)  new position  (1, 1)\n",
      "0.0 0 92  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -1.43862343   9.60378544   4.05602223  -8.06012653  20.66443689\n",
      " -12.49181872  -5.53148764   1.87291594   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (1, 1)  new position  (0, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [  2.52939684   0.         -16.17025784 -20.99596012 -43.77061943\n",
      " -17.78290425  10.17861875   0.           0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  4\n",
      "index max:  3\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (7, 5)  new position  (8, 4)\n",
      "0.0 0 93  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-24.3871204   -9.00912739  -8.34658703 -17.85523438 -11.58901356\n",
      " -13.43365495  -9.77788848  -8.23297622   0.        ]\n",
      "max index list:  [8]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 4)  new position  (9, 3)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ -9.01096062 -20.96582763  -3.5121129  -12.39706514  21.16932747\n",
      "  -3.32441676 -22.58012571   0.           0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  6\n",
      "index max:  5\n",
      "action:  6\n",
      "The max action is  6\n",
      "action word  6\n",
      "The action I choose is  DOWN_LEFT\n",
      "Old location  (0, 2)  new position  (9, 1)\n",
      "0.0 0 94  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-10.14964387  -2.12349616 -16.88757813  -8.34221473   1.06572792\n",
      " -22.59790652 -13.12839782 -10.49090045   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  1\n",
      "index max:  1\n",
      "action:  1\n",
      "The max action is  1\n",
      "action word  1\n",
      "The action I choose is  DOWN\n",
      "Old location  (9, 3)  new position  (8, 3)\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [  0.14147627 -39.24433331   2.20610281 -12.15931246  -4.04120475\n",
      " -28.28387937  -1.20071051  -9.20205713   0.        ]\n",
      "max index list:  [2]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 1)  new position  (8, 2)\n",
      "0.0 0 95  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [-29.3418462    3.33750964   0.43959804   0.         -12.76231597\n",
      " -16.93540862  -2.3882438  -11.32994693   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  2\n",
      "index max:  2\n",
      "action:  2\n",
      "The max action is  2\n",
      "action word  2\n",
      "The action I choose is  LEFT\n",
      "Old location  (8, 2)  new position  (8, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [  4.99033685  -0.21369508 -17.78386442 -17.73534807  -0.73535487\n",
      "   3.25833488   4.01263436 -18.12220829   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 3)  new position  (7, 4)\n",
      "0.0 0 96  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 24.28279917  -3.2503315   -3.4938431    7.46755294 -10.64272157\n",
      "  -9.66022703   0.34289003   0.44213197   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 1)  new position  (9, 0)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [ 3.6359424   4.66838662  3.61232695  0.69975406 -6.82693662 -1.95821441\n",
      " 21.32860793  0.          0.        ]\n",
      "max index list:  [6]\n",
      "max_action:  7\n",
      "max action is not in possible actions, pick a random action from list\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (7, 4)  new position  (8, 5)\n",
      "0.0 0 97  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ -1.93317718  18.93175598  -9.64402293  14.1826854   -7.95226194\n",
      "  -4.3107717  -15.06446815 -16.39727951   0.        ]\n",
      "max index list:  [1]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 0)  new position  (8, 1)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  4\n",
      "index max:  4\n",
      "action:  4\n",
      "The max action is  4\n",
      "action word  4\n",
      "The action I choose is  UP_LEFT\n",
      "Old location  (8, 5)  new position  (9, 4)\n",
      "0.0 0 98  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  6\n",
      "in max action with choice\n",
      "Q state is  [ 24.28279917  -3.2503315   -3.4938431    7.46755294 -10.64272157\n",
      "  -9.66022703   0.34289003   0.44213197   0.        ]\n",
      "max index list:  [0]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (8, 1)  new position  (7, 2)\n",
      "trained monte carlo step\n",
      "distance is  4\n",
      "in max action with choice\n",
      "Q state is  [-20.73410821  -3.2286344  -17.67387294  11.1369929   -2.11261116\n",
      " -10.35721382   4.88551999  -9.6647841    0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (9, 4)  new position  (8, 5)\n",
      "0.0 0 99  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [-34.3586957  -33.91680988 -13.09937452  20.73548999   1.47321832\n",
      "   6.00786622  -5.23725631 -11.79157297   0.        ]\n",
      "max index list:  [3]\n",
      "max_action:  5\n",
      "index max:  6\n",
      "action:  5\n",
      "The max action is  5\n",
      "action word  5\n",
      "The action I choose is  UP_RIGHT\n",
      "Old location  (8, 5)  new position  (9, 6)\n",
      "trained monte carlo step\n",
      "distance is  5\n",
      "in max action with choice\n",
      "Q state is  [ -1.29113061  -7.00436763 -11.73276711  -6.20503394  11.95505271\n",
      "  -5.12276543  -9.02185003  11.17885419   0.        ]\n",
      "max index list:  [4]\n",
      "max_action:  7\n",
      "index max:  7\n",
      "action:  7\n",
      "The max action is  7\n",
      "action word  7\n",
      "The action I choose is  DOWN_RIGHT\n",
      "Old location  (7, 2)  new position  (6, 3)\n",
      "0.0 0 100  Episode:  9\n",
      "cows in goal:  0 , previous_cow_count:  0  reward:  -1.0\n"
     ]
    }
   ],
   "source": [
    "# Collect times for trained monte carlo agents\n",
    "final_mc_scores = []\n",
    "final_mc_times = []\n",
    "for episode in range(episodes):\n",
    "    model = CHModel(10, 10, cow_n = cow_agents, t_mc_n = trained_mc_agents, episode_number = episode)\n",
    "\n",
    "    print(\"Monte Carlo Episode \", episode)\n",
    "    for i in range(steps):\n",
    "        model.step()\n",
    "        # if the agents are able to herd the cows in the given number of timesteps, save the time finished\n",
    "        if(model.done):\n",
    "            final_mc_times.append(i)\n",
    "            #save the final score\n",
    "            final_mc_scores.append(model.score)\n",
    "    # if the agents were not able to herd the cows in the given number of timesteps, save the maximum time allowed\n",
    "    if (not(model.done)):\n",
    "        final_mc_times.append(steps)\n",
    "        final_mc_scores.append(model.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 100, 100, 100, 100, 100, 100, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    }
   ],
   "source": [
    "print(final_mc_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.74, 0.0, 0.0, 0.0, 0.18, 2.0, 2.8, 2.909090909090909, 3.0, 3.076923076923077, 3.142857142857143, 3.2, 3.25, 3.2941176470588234, 3.3333333333333335, 3.3684210526315788, 3.4, 3.4285714285714284, 3.4545454545454546, 3.4782608695652173, 3.5, 3.52, 3.5384615384615383, 3.5555555555555554, 3.5714285714285716, 3.586206896551724, 3.6, 3.6129032258064515, 3.625, 3.6363636363636362, 3.6470588235294117, 3.657142857142857, 3.6666666666666665, 3.675675675675676, 3.6842105263157894, 3.6923076923076925, 3.7, 3.707317073170732, 3.7142857142857144, 3.7209302325581395, 3.727272727272727, 3.7333333333333334, 3.739130434782609, 3.74468085106383, 3.75, 3.7551020408163267, 3.76, 3.764705882352941, 3.769230769230769, 3.7735849056603774, 3.7777777777777777, 3.7818181818181817, 3.7857142857142856, 3.789473684210526, 3.793103448275862, 3.7966101694915255, 3.8, 3.80327868852459, 3.806451612903226, 3.8095238095238093, 3.8125, 3.8153846153846156, 3.8181818181818183, 3.8208955223880596, 3.823529411764706, 3.8260869565217392, 3.8285714285714287, 3.8309859154929575, 3.8333333333333335, 3.835616438356164, 3.8378378378378377, 3.84, 3.8421052631578947, 3.844155844155844, 3.8461538461538463, 3.848101265822785, 3.85, 3.8518518518518516, 3.8536585365853657, 3.855421686746988, 3.857142857142857, 3.8588235294117648, 3.86046511627907, 3.8620689655172415, 3.8636363636363638, 3.865168539325843, 3.8666666666666667, 3.868131868131868, 3.869565217391304, 3.870967741935484, 3.872340425531915, 3.873684210526316, 3.875, 3.8762886597938144, 3.877551020408163, 3.878787878787879, 3.88, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(final_mc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
